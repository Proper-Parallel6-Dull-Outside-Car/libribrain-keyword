{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ed89f-ebb0-4a0d-8030-ba2443b4e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q mne_bids lightning torchmetrics scikit-learn plotly ipywidgets neptune\n",
    "\n",
    "# Set up base path for dataset and related files\n",
    "base_path = \"./libribrain\"\n",
    "\n",
    "# Install pnpl from local modified package\n",
    "%pip install ../modified-pnpl/pnpl\n",
    "\n",
    "# Remember to set the NEPTUNE_API_TOKEN and NEPTUNE_PROJECT environment variables\n",
    "# before running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d766e-9c77-4660-beef-1fd14785b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Watson keyword detection on LibriBrain (MEG)\n",
    "- Oversampling-only training (no class-weighting)\n",
    "- Focal loss + pairwise ranking aux loss\n",
    "- Temporal attention pooling backbone\n",
    "- Robust, PR-friendly validation/test diagnostics\n",
    "- Warmup + cosine LR; optional Neptune logging\n",
    "- Fast cached label indexing for balanced sampler\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, math, random, json, hashlib, csv\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Iterator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, BatchSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "\n",
    "# ----------------- Utilities -----------------\n",
    "def collate_label_only_xy(batch):  # picklable; used by index scan\n",
    "    return [int(y) for _, y in batch]\n",
    "\n",
    "def _cpu(t): return t.detach().float().cpu()\n",
    "\n",
    "# ---------------- Neptune (optional) ----------------\n",
    "def make_neptune_logger(run_name: str | None = None):\n",
    "    api_key, project = os.getenv(\"NEPTUNE_API_TOKEN\"), os.getenv(\"NEPTUNE_PROJECT\")\n",
    "    if not api_key or not project:\n",
    "        print(\"Neptune: env vars not found -> skipping Neptune logging.\")\n",
    "        return None\n",
    "    try:\n",
    "        from lightning.pytorch.loggers import NeptuneLogger as _BaseNeptuneLogger\n",
    "    except Exception:\n",
    "        from pytorch_lightning.loggers import NeptuneLogger as _BaseNeptuneLogger\n",
    "\n",
    "    class CleanNeptuneLogger(_BaseNeptuneLogger):\n",
    "        def log_metrics(self, metrics, step: int | None = None):\n",
    "            filt = {k: v for k, v in metrics.items() if k != \"epoch\" and not k.endswith(\"/epoch\")}\n",
    "            super().log_metrics(filt, step=None)\n",
    "\n",
    "    logger = CleanNeptuneLogger(\n",
    "        api_key=api_key, project=project, name=run_name,\n",
    "        tags=[\"libribrain\", \"watson\", \"meg\", \"keyword-detection\"],\n",
    "        prefix=\"training/\", log_model_checkpoints=False,\n",
    "    )\n",
    "    print(\"Neptune: ✅ enabled.\")\n",
    "    return logger\n",
    "\n",
    "# ---------------- Model ----------------\n",
    "class ResNetBlock1D(nn.Module):\n",
    "    def __init__(self, channels: int = 128):\n",
    "        super().__init__()\n",
    "        same_supported = 'same' in nn.Conv1d.__init__.__code__.co_varnames\n",
    "        pad3 = 'same' if same_supported else 1\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ELU(), nn.Conv1d(channels, channels, 3, 1, pad3),\n",
    "            nn.ELU(), nn.Conv1d(channels, channels, 1, 1, 0),\n",
    "        )\n",
    "    def forward(self, x): return x + self.net(x)\n",
    "\n",
    "class SpeechDetectionNet(nn.Module):\n",
    "    \"\"\"Conv trunk + temporal attention pooling.\"\"\"\n",
    "    def __init__(self, in_channels: int = 306, lse_temperature: float = 0.5):  # lse_temperature kept for API compat\n",
    "        super().__init__()\n",
    "        same_supported = 'same' in nn.Conv1d.__init__.__code__.co_varnames\n",
    "        pad7 = 'same' if same_supported else 3\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 128, 7, 1, pad7),\n",
    "            ResNetBlock1D(128),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(128, 128, 50, 25, 0),  # downsample time\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(128, 128, 7, 1, pad7),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Conv1d(128, 512, 4, 1, 0), nn.ReLU(), nn.Dropout(0.5))\n",
    "        self.logits_t = nn.Conv1d(512, 1, 1, 1, 0)\n",
    "        self.attn_t   = nn.Conv1d(512, 1, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.head(self.trunk(x))        # (N,512,T')\n",
    "        logit_t = self.logits_t(h)          # (N,1,T')\n",
    "        attn = torch.softmax(self.attn_t(h), dim=-1)\n",
    "        return (logit_t * attn).sum(dim=-1).squeeze(1)  # (N,)\n",
    "\n",
    "# ---------------- Losses + metrics helpers ----------------\n",
    "@dataclass\n",
    "class OptimConfig:\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    max_time_shift: int = 4\n",
    "    noise_std: float = 0.01\n",
    "    warmup_epochs: int = 1\n",
    "    cosine_after_warmup: bool = True\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: float = 0.95, gamma: float = 2.0):\n",
    "        super().__init__(); self.alpha = float(alpha); self.gamma = float(gamma)\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        ce = nn.functional.binary_cross_entropy_with_logits(logits, targets.float(), reduction='none')\n",
    "        p = torch.sigmoid(logits); pt = torch.where(targets == 1, p, 1 - p)\n",
    "        alpha_t = torch.where(targets == 1, logits.new_tensor(self.alpha), logits.new_tensor(1 - self.alpha))\n",
    "        return (alpha_t * (1 - pt).pow(self.gamma) * ce).mean()\n",
    "\n",
    "try:\n",
    "    from torchmetrics.classification import BinaryAccuracy, BinaryAveragePrecision, BinaryAUROC\n",
    "except Exception:\n",
    "    from torchmetrics import Accuracy as BinaryAccuracy                 # type: ignore\n",
    "    from torchmetrics import AveragePrecision as BinaryAveragePrecision # type: ignore\n",
    "    from torchmetrics import AUROC as BinaryAUROC                       # type: ignore\n",
    "\n",
    "# ---------------- LightningModule ----------------\n",
    "class WatsonKeywordPL(pl.LightningModule):\n",
    "    def __init__(self, in_channels: int = 306, pos_weight: float = 1.0,\n",
    "                 opt: OptimConfig = OptimConfig(), lse_temperature: float = 0.5,\n",
    "                 pairwise_lambda: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = SpeechDetectionNet(in_channels, lse_temperature=lse_temperature)\n",
    "        self.register_buffer(\"pos_weight_tensor\", torch.tensor([pos_weight], dtype=torch.float32))  # kept for API compat\n",
    "        self.criterion = FocalLoss(alpha=0.95, gamma=2.0)\n",
    "        self.pairwise_lambda = float(pairwise_lambda)\n",
    "\n",
    "        # epoch aggregates (ranking-friendly)\n",
    "        self.train_acc = BinaryAccuracy(); self.val_acc = BinaryAccuracy(); self.test_acc = BinaryAccuracy()\n",
    "        self.val_auprc = BinaryAveragePrecision(); self.test_auprc = BinaryAveragePrecision()\n",
    "        self.val_auroc = BinaryAUROC(); self.test_auroc = BinaryAUROC()\n",
    "\n",
    "        self._val_probs: List[torch.Tensor] = []; self._val_labels: List[torch.Tensor] = []\n",
    "        self._test_probs: List[torch.Tensor] = []; self._test_labels: List[torch.Tensor] = []\n",
    "        self._train_pos = 0; self._train_total = 0\n",
    "        self._val_pos = 0; self._val_total = 0\n",
    "\n",
    "    # ---- small helpers ----\n",
    "    @staticmethod\n",
    "    def _pairwise_logistic_loss(logits: torch.Tensor, labels: torch.Tensor, max_pairs: int = 4096) -> torch.Tensor:\n",
    "        pos_idx = (labels == 1).nonzero(as_tuple=False).view(-1)\n",
    "        neg_idx = (labels == 0).nonzero(as_tuple=False).view(-1)\n",
    "        if pos_idx.numel() == 0 or neg_idx.numel() == 0: return logits.new_zeros(())\n",
    "        num_pairs = min(max_pairs, int(pos_idx.numel()) * int(neg_idx.numel()))\n",
    "        pi = pos_idx[torch.randint(0, pos_idx.numel(), (num_pairs,), device=logits.device)]\n",
    "        ni = neg_idx[torch.randint(0, neg_idx.numel(), (num_pairs,), device=logits.device)]\n",
    "        return torch.nn.functional.softplus(-(logits[pi] - logits[ni])).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def _rprecision(probs: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "        m = int(labels.sum().item()); \n",
    "        if m <= 0: return 0.0\n",
    "        k = min(m, probs.numel())\n",
    "        prec_at_m = labels[torch.topk(probs, k=k, largest=True).indices].float().mean().item()\n",
    "        return float(prec_at_m)\n",
    "\n",
    "    @staticmethod\n",
    "    def _precision_recall_at_k(probs: torch.Tensor, labels: torch.Tensor, k: int) -> Tuple[float, float]:\n",
    "        k = max(1, min(k, probs.numel()))\n",
    "        topk = torch.topk(probs, k=k, largest=True).indices\n",
    "        tp = labels[topk].sum().item()\n",
    "        prec = tp / k; rec = tp / max(1, int(labels.sum().item()))\n",
    "        return float(prec), float(rec)\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_f1(probs: torch.Tensor, labels: torch.Tensor) -> Tuple[float, float, Tuple[int,int,int,int]]:\n",
    "        N = probs.numel()\n",
    "        if N == 0: return 0.0, 0.5, (0,0,0,0)\n",
    "        sort_idx = torch.argsort(probs, descending=True); y = labels[sort_idx].to(torch.int32)\n",
    "        cum_tp = torch.cumsum(y, dim=0); ks = torch.arange(1, N+1, device=probs.device)\n",
    "        precision = cum_tp / ks; total_pos = max(1, int(labels.sum().item()))\n",
    "        recall = cum_tp / total_pos\n",
    "        denom = precision + recall\n",
    "        f1 = torch.where(denom > 0, 2 * precision * recall / denom, torch.zeros_like(denom))\n",
    "        i = int(torch.argmax(f1).item()); best_f1 = float(f1[i].item()); thr = float(probs[sort_idx[i]].item())\n",
    "        k = i + 1; tp = int(cum_tp[i].item()); fp = int(k - tp); fn = int(total_pos - tp); tn = int(N - k - fn)\n",
    "        return best_f1, thr, (tp, fp, tn, fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _f1_macro_at_threshold(probs: torch.Tensor, labels: torch.Tensor, threshold: float = 0.5) -> float:\n",
    "        if probs.numel() == 0: return 0.0\n",
    "        preds, lab = (probs >= threshold).to(torch.int32), labels.to(torch.int32)\n",
    "        tp = int(((preds == 1) & (lab == 1)).sum().item())\n",
    "        fp = int(((preds == 1) & (lab == 0)).sum().item())\n",
    "        fn = int(((preds == 0) & (lab == 1)).sum().item())\n",
    "        tn = int(((preds == 0) & (lab == 0)).sum().item())\n",
    "        def _f1(p, r): return 0.0 if (p + r) == 0 else (2 * p * r) / (p + r)\n",
    "        prec_pos = tp / max(1, tp + fp); rec_pos = tp / max(1, tp + fn)\n",
    "        prec_neg = tn / max(1, tn + fn); rec_neg = tn / max(1, tn + fp)\n",
    "        return float((_f1(prec_pos, rec_pos) + _f1(prec_neg, rec_neg)) / 2.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _recall_at_precision(probs: torch.Tensor, labels: torch.Tensor, min_precision: float = 0.9) -> float:\n",
    "        N = probs.numel()\n",
    "        if N == 0: return 0.0\n",
    "        sort_idx = torch.argsort(probs, descending=True); y = labels[sort_idx].to(torch.int32)\n",
    "        cum_tp = torch.cumsum(y, dim=0); ks = torch.arange(1, N+1, device=probs.device)\n",
    "        precision = cum_tp / ks; total_pos = max(1, int(labels.sum().item()))\n",
    "        recall = cum_tp / total_pos\n",
    "        mask = precision >= min_precision\n",
    "        return float(recall[mask].max().item()) if mask.any() else 0.0\n",
    "\n",
    "    # ---- Lightning required ----\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "    def _augment(self, x):\n",
    "        if not self.training: return x\n",
    "        smax = self.hparams.opt.max_time_shift\n",
    "        if smax and smax > 0:\n",
    "            shifts = torch.randint(-smax, smax + 1, (x.size(0),), device=x.device)\n",
    "            for i, sh in enumerate(shifts):\n",
    "                if int(sh) != 0: x[i] = torch.roll(x[i], int(sh), dims=-1)\n",
    "        sigma = self.hparams.opt.noise_std\n",
    "        return x + torch.randn_like(x) * sigma if (sigma and sigma > 0) else x\n",
    "\n",
    "    def _bce_unweighted(self, logits, y):\n",
    "        return nn.functional.binary_cross_entropy_with_logits(logits.float(), y.float())\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        self._train_pos += int(y.sum()); self._train_total += int(y.numel())\n",
    "        logits = self(self._augment(x))\n",
    "        focal = self.criterion(logits.float(), y.float())\n",
    "        pairwise = self._pairwise_logistic_loss(logits.detach(), y)  # detached for stability\n",
    "        loss = focal + self.pairwise_lambda * pairwise\n",
    "        probs = torch.sigmoid(logits.float())\n",
    "        self.train_acc.update(probs, y)\n",
    "        self.log_dict({\n",
    "            \"train_loss\": loss, \"train_focal\": focal, \"train_pairwise\": pairwise,\n",
    "            \"train_pos_frac\": y.float().mean(),\n",
    "        }, on_step=True, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"train_acc\", self.train_acc.compute(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        if self._train_total > 0:\n",
    "            self.log(\"train_pos_fraction_epoch\", float(self._train_pos) / float(self._train_total),\n",
    "                     on_step=False, on_epoch=True)\n",
    "        self._train_pos = 0; self._train_total = 0; self.train_acc.reset()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self._val_pos = 0; self._val_total = 0; self._val_probs.clear(); self._val_labels.clear()\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logits = self(x); probs = torch.sigmoid(logits.float())\n",
    "        self.val_acc.update(probs, y); self.val_auprc.update(probs, y); self.val_auroc.update(probs, y)\n",
    "        self._val_pos += int(y.sum()); self._val_total += int(y.numel())\n",
    "        self._val_probs.append(_cpu(probs)); self._val_labels.append(_cpu(y).int())\n",
    "        self.log(\"val_loss\", self._bce_unweighted(logits, y), on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        base_rate = (self._val_pos / max(self._val_total, 1)) if self._val_total > 0 else 0.0\n",
    "        both_classes = (self._val_pos > 0) and (self._val_pos < self._val_total)\n",
    "        val_acc = self.val_acc.compute()\n",
    "        val_auprc = (self.val_auprc.compute() if self._val_pos > 0\n",
    "                     else torch.as_tensor(base_rate, device=self.device))\n",
    "        val_auroc = (self.val_auroc.compute() if both_classes\n",
    "                     else torch.as_tensor(0.5, device=self.device))\n",
    "\n",
    "        probs = torch.cat(self._val_probs, dim=0) if self._val_probs else torch.empty(0)\n",
    "        labels = torch.cat(self._val_labels, dim=0) if self._val_labels else torch.empty(0, dtype=torch.int64)\n",
    "        if probs.numel() != labels.numel(): probs = probs[:labels.numel()]\n",
    "\n",
    "        rprec = self._rprecision(probs, labels)\n",
    "        m = int(labels.sum().item()) if labels.numel() > 0 else 1\n",
    "        prec_m, rec_m = self._precision_recall_at_k(probs, labels, max(1, m))\n",
    "        prec_2m, rec_2m = self._precision_recall_at_k(probs, labels, max(1, 2*m))\n",
    "        prec_5m, rec_5m = self._precision_recall_at_k(probs, labels, max(1, 5*m))\n",
    "        best_f1, best_thr, (tp, fp, tn, fn) = self._best_f1(probs, labels)\n",
    "        rec_at_p90 = self._recall_at_precision(probs, labels, 0.90)\n",
    "        f1_macro_05 = self._f1_macro_at_threshold(probs, labels, 0.5)\n",
    "\n",
    "        print(\n",
    "            f\"[VAL] base_rate={base_rate:.6f}  AUPRC={float(val_auprc):.4f}  AUROC={float(val_auroc):.4f}  \"\n",
    "            f\"RPrec={rprec:.4f}  BestF1={best_f1:.4f} @thr={best_thr:.4f}  \"\n",
    "            f\"F1-macro@0.5={f1_macro_05:.4f}  Rec@P>=0.90={rec_at_p90:.4f}  \"\n",
    "            f\"Conf(TP/FP/TN/FN)={tp}/{fp}/{tn}/{fn}\"\n",
    "        )\n",
    "\n",
    "        self.log_dict({\n",
    "            \"val_acc\": val_acc, \"val_auprc\": val_auprc, \"val_auroc\": val_auroc,\n",
    "            \"val_pos_rate\": torch.as_tensor(base_rate, device=self.device),\n",
    "            \"val_random_auprc\": torch.as_tensor(base_rate, device=self.device),\n",
    "            \"val_rprecision\": torch.as_tensor(rprec, device=self.device),\n",
    "            \"val_precision_at_M\": torch.as_tensor(prec_m, device=self.device),\n",
    "            \"val_recall_at_M\": torch.as_tensor(rec_m, device=self.device),\n",
    "            \"val_precision_at_2M\": torch.as_tensor(prec_2m, device=self.device),\n",
    "            \"val_recall_at_2M\": torch.as_tensor(rec_2m, device=self.device),\n",
    "            \"val_precision_at_5M\": torch.as_tensor(prec_5m, device=self.device),\n",
    "            \"val_recall_at_5M\": torch.as_tensor(rec_5m, device=self.device),\n",
    "            \"val_best_f1\": torch.as_tensor(best_f1, device=self.device),\n",
    "            \"val_best_f1_threshold\": torch.as_tensor(best_thr, device=self.device),\n",
    "            \"val_macro_f1@0.5\": torch.as_tensor(f1_macro_05, device=self.device),\n",
    "            \"val_recall_at_precision_0.90\": torch.as_tensor(rec_at_p90, device=self.device),\n",
    "            \"val_tp_bestf1\": torch.as_tensor(tp, device=self.device),\n",
    "            \"val_fp_bestf1\": torch.as_tensor(fp, device=self.device),\n",
    "            \"val_tn_bestf1\": torch.as_tensor(tn, device=self.device),\n",
    "            \"val_fn_bestf1\": torch.as_tensor(fn, device=self.device),\n",
    "        }, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.val_acc.reset(); self.val_auprc.reset(); self.val_auroc.reset()\n",
    "        self._val_probs.clear(); self._val_labels.clear()\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self._test_probs.clear(); self._test_labels.clear()\n",
    "\n",
    "    def test_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logits = self(x); probs = torch.sigmoid(logits.float())\n",
    "        self.test_acc.update(probs, y); self.test_auprc.update(probs, y); self.test_auroc.update(probs, y)\n",
    "        self._test_probs.append(_cpu(probs)); self._test_labels.append(_cpu(y).int())\n",
    "        self.log(\"test_loss\", self._bce_unweighted(logits, y), on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        probs = torch.cat(self._test_probs, dim=0) if self._test_probs else torch.empty(0)\n",
    "        labels = torch.cat(self._test_labels, dim=0) if self._test_labels else torch.empty(0, dtype=torch.int64)\n",
    "        base_rate = float(labels.float().mean().item()) if labels.numel() > 0 else 0.0\n",
    "        both_classes = (labels.sum().item() > 0) and (labels.sum().item() < labels.numel())\n",
    "        try: test_auprc = self.test_auprc.compute()\n",
    "        except Exception: test_auprc = torch.as_tensor(base_rate, device=self.device)\n",
    "        try: test_auroc = self.test_auroc.compute() if both_classes else torch.as_tensor(0.5, device=self.device)\n",
    "        except Exception: test_auroc = torch.as_tensor(0.5, device=self.device)\n",
    "\n",
    "        rprec = self._rprecision(probs, labels)\n",
    "        m = int(labels.sum().item()) if labels.numel() > 0 else 1\n",
    "        prec_m, rec_m = self._precision_recall_at_k(probs, labels, max(1, m))\n",
    "        best_f1, best_thr, (tp, fp, tn, fn) = self._best_f1(probs, labels)\n",
    "        rec_at_p90 = self._recall_at_precision(probs, labels, 0.90)\n",
    "        f1_macro_05 = self._f1_macro_at_threshold(probs, labels, 0.5)\n",
    "\n",
    "        print(\n",
    "            f\"[TEST] base_rate={base_rate:.6f}  AUPRC={float(test_auprc):.4f}  AUROC={float(test_auroc):.4f}  \"\n",
    "            f\"RPrec={rprec:.4f}  BestF1={best_f1:.4f} @thr={best_thr:.4f}  \"\n",
    "            f\"F1-macro@0.5={f1_macro_05:.4f}  Rec@P>=0.90={rec_at_p90:.4f}  \"\n",
    "            f\"Conf(TP/FP/TN/FN)={tp}/{fp}/{tn}/{fn}\"\n",
    "        )\n",
    "\n",
    "        self.log_dict({\n",
    "            \"test_acc\": self.test_acc.compute(), \"test_auprc\": test_auprc, \"test_auroc\": test_auroc,\n",
    "            \"test_rprecision\": torch.as_tensor(rprec, device=self.device),\n",
    "            \"test_precision_at_M\": torch.as_tensor(prec_m, device=self.device),\n",
    "            \"test_recall_at_M\": torch.as_tensor(rec_m, device=self.device),\n",
    "            \"test_best_f1\": torch.as_tensor(best_f1, device=self.device),\n",
    "            \"test_best_f1_threshold\": torch.as_tensor(best_thr, device=self.device),\n",
    "            \"test_f1_macro@0.5\": torch.as_tensor(f1_macro_05, device=self.device),\n",
    "            \"test_recall_at_precision_0.90\": torch.as_tensor(rec_at_p90, device=self.device),\n",
    "        }, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Save predictions as CSV (index, label, probability)\n",
    "        try:\n",
    "            os.makedirs(\"playground\", exist_ok=True)\n",
    "            out_path = os.path.join(\"playground\", \"test_predictions.csv\")\n",
    "            with open(out_path, \"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f); w.writerow([\"index\", \"label\", \"probability\"])\n",
    "                for i, (p, y) in enumerate(zip(probs.tolist(), labels.tolist())):\n",
    "                    w.writerow([i, int(y), float(p)])\n",
    "            print(f\"Saved test predictions to {out_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save test predictions CSV: {e}\")\n",
    "\n",
    "        self.test_acc.reset(); self.test_auprc.reset(); self.test_auroc.reset()\n",
    "        self._test_probs.clear(); self._test_labels.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.opt.lr, weight_decay=self.hparams.opt.weight_decay)\n",
    "        total_epochs = getattr(self.trainer, \"max_epochs\", 30) or 30\n",
    "        warm = max(0, int(self.hparams.opt.warmup_epochs))\n",
    "        if self.hparams.opt.cosine_after_warmup:\n",
    "            def lr_lambda(epoch):\n",
    "                if epoch < warm: return (epoch + 1) / max(1, warm)\n",
    "                t = (epoch - warm) / max(1, total_epochs - warm)\n",
    "                return 0.5 * (1 + math.cos(math.pi * t))\n",
    "            return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)}}\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=2)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"monitor\": \"val_auprc\"}}\n",
    "\n",
    "# ---------------- Data ----------------\n",
    "from pnpl.datasets.libribrain2025.word_dataset import LibriBrainWord\n",
    "from pnpl.datasets.libribrain2025.constants import RUN_KEYS\n",
    "try:\n",
    "    from pnpl.datasets.libribrain2025.base import LibriBrainBase\n",
    "except Exception:\n",
    "    LibriBrainBase = None\n",
    "\n",
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"Oversample positives to reach target fraction per batch (with replacement).\"\"\"\n",
    "    def __init__(self, pos_idx: List[int], neg_idx: List[int], batch_size: int, pos_fraction: float = 0.1):\n",
    "        assert 0.0 < pos_fraction < 1.0 and len(pos_idx) > 0\n",
    "        self.p_idx, self.n_idx = pos_idx, neg_idx\n",
    "        self.batch_size = batch_size\n",
    "        self.n_pos = max(1, int(round(batch_size * pos_fraction)))\n",
    "        self.n_neg = batch_size - self.n_pos\n",
    "        total = len(pos_idx) + len(neg_idx)\n",
    "        self._epoch_len = max(1, total // batch_size)\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        p, n = self.p_idx[:], self.n_idx[:]; random.shuffle(p); random.shuffle(n); pi = ni = 0\n",
    "        while True:\n",
    "            if pi + self.n_pos > len(p): random.shuffle(p); pi = 0\n",
    "            if ni + self.n_neg > len(n): random.shuffle(n); ni = 0\n",
    "            batch = p[pi:pi+self.n_pos] + n[ni:ni+self.n_neg]; pi += self.n_pos; ni += self.n_neg\n",
    "            random.shuffle(batch); yield batch\n",
    "\n",
    "    def __len__(self) -> int: return self._epoch_len\n",
    "\n",
    "class LibriBrainWordDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path: str, tmin: float=-0.1, tmax: float=0.8, batch_size: int=256,\n",
    "                 num_workers: int=4, pin_memory: bool=True, standardize_train: bool=True,\n",
    "                 target_pos_fraction: float = 0.10,\n",
    "                 val_run_override: Optional[Tuple[str,str,str,str]] = None,\n",
    "                 test_run_override: Optional[Tuple[str,str,str,str]] = None):\n",
    "        super().__init__()\n",
    "        self.data_path, self.tmin, self.tmax = data_path, tmin, tmax\n",
    "        self.batch_size, self.num_workers, self.pin_memory = batch_size, num_workers, pin_memory\n",
    "        self.standardize_train = standardize_train\n",
    "        self.target_pos_fraction = target_pos_fraction\n",
    "        self.val_run_override = val_run_override; self.test_run_override = test_run_override\n",
    "        self._train_sampler: Optional[BalancedBatchSampler] = None\n",
    "\n",
    "    def _available_runs(self):\n",
    "        cands = [rk for rk in RUN_KEYS if rk[2].startswith('Sherlock')]\n",
    "        if LibriBrainBase is None: return cands\n",
    "        def _events_path(su, se, ta, ru):\n",
    "            return os.path.join(self.data_path, ta, \"derivatives\", \"events\",\n",
    "                                f\"sub-{su}_ses-{se}_task-{ta}_run-{ru}_events.tsv\")\n",
    "        def _h5_path(su, se, ta, ru):\n",
    "            return os.path.join(self.data_path, ta, \"derivatives\", \"serialised\",\n",
    "                                f\"sub-{su}_ses-{se}_task-{ta}_run-{ru}_proc-bads+headpos+sss+notch+bp+ds_meg.h5\")\n",
    "        avail=[]\n",
    "        for s,se,t,r in cands:\n",
    "            try:\n",
    "                LibriBrainBase.ensure_file_download(_events_path(s,se,t,r), data_path=self.data_path)\n",
    "                LibriBrainBase.ensure_file_download(_h5_path(s,se,t,r), data_path=self.data_path)\n",
    "                avail.append((s,se,t,r))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return avail or cands\n",
    "\n",
    "    def _hash_key(self, train_runs: List[Tuple[str,str,str,str]]) -> str:\n",
    "        m = hashlib.sha256()\n",
    "        m.update(json.dumps({\"tmin\": self.tmin, \"tmax\": self.tmax,\n",
    "                             \"runs\": sorted([\"_\".join(x) for x in train_runs])},\n",
    "                            sort_keys=True).encode(\"utf-8\"))\n",
    "        return m.hexdigest()[:16]\n",
    "\n",
    "    def _cache_paths(self, key: str):  # single file for pos/neg index cache\n",
    "        cache_dir = os.path.join(self.data_path, \"_indices\"); os.makedirs(cache_dir, exist_ok=True)\n",
    "        return os.path.join(cache_dir, f\"watson_{key}.pt\")\n",
    "\n",
    "    def _try_dataset_labels_fast(self, ds: Dataset) -> Optional[List[int]]:\n",
    "        for attr in (\"labels\", \"y\", \"targets\", \"_labels\", \"_y\", \"_targets\"):\n",
    "            if hasattr(ds, attr):\n",
    "                lab = getattr(ds, attr)\n",
    "                try:\n",
    "                    if torch.is_tensor(lab): return lab.view(-1).cpu().int().tolist()\n",
    "                    return list(map(int, list(lab)))\n",
    "                except Exception: continue\n",
    "        return None\n",
    "\n",
    "    def _build_pos_neg_indices(self, ds: Dataset, cache_file: str) -> Tuple[List[int], List[int]]:\n",
    "        if os.path.exists(cache_file):\n",
    "            obj = torch.load(cache_file, map_location=\"cpu\")\n",
    "            pos_idx = list(map(int, obj[\"pos_idx\"])); neg_idx = list(map(int, obj[\"neg_idx\"]))\n",
    "            print(f\"Index cache: loaded {len(pos_idx)} positives / {len(pos_idx)+len(neg_idx)} total.\")\n",
    "            return pos_idx, neg_idx\n",
    "\n",
    "        lbls = self._try_dataset_labels_fast(ds)\n",
    "        if lbls is not None:\n",
    "            pos_idx = [i for i, y in enumerate(lbls) if int(y) == 1]\n",
    "            neg_idx = [i for i, y in enumerate(lbls) if int(y) == 0]\n",
    "            print(f\"Index fast-path: found {len(pos_idx)} positives / {len(lbls)} total.\")\n",
    "            torch.save({\"pos_idx\": pos_idx, \"neg_idx\": neg_idx}, cache_file)\n",
    "            return pos_idx, neg_idx\n",
    "\n",
    "        def _scan(num_workers: int) -> Tuple[List[int], List[int]]:\n",
    "            print(f\"Scanning training labels to build balanced sampler (num_workers={num_workers})…\")\n",
    "            pos_idx, neg_idx, idx = [], [], 0\n",
    "            loader = DataLoader(ds, batch_size=2048, shuffle=False,\n",
    "                                num_workers=num_workers, pin_memory=False,\n",
    "                                persistent_workers=(num_workers > 0),\n",
    "                                prefetch_factor=2 if num_workers > 0 else None,\n",
    "                                collate_fn=collate_label_only_xy)\n",
    "            for ys in loader:\n",
    "                for y in ys:\n",
    "                    (pos_idx if y == 1 else neg_idx).append(idx); idx += 1\n",
    "                if idx % 50000 == 0: print(f\"… scanned {idx} samples\")\n",
    "            return pos_idx, neg_idx\n",
    "\n",
    "        try: pos_idx, neg_idx = _scan(self.num_workers)\n",
    "        except Exception as e:\n",
    "            print(f\"[Label scan] parallel scan failed ({type(e).__name__}: {e}). Falling back to single-process.\")\n",
    "            pos_idx, neg_idx = _scan(0)\n",
    "\n",
    "        total = len(pos_idx) + len(neg_idx); frac = len(pos_idx) / max(1, total)\n",
    "        print(f\"Found {len(pos_idx)} positives / {total} total ({frac:.6f}).\")\n",
    "        torch.save({\"pos_idx\": pos_idx, \"neg_idx\": neg_idx}, cache_file)\n",
    "        return pos_idx, neg_idx\n",
    "\n",
    "    def setup(self, stage: Optional[str]=None):\n",
    "        all_runs = [rk for rk in self._available_runs()]\n",
    "        self.val_run  = self.val_run_override  or ('0','12','Sherlock4','1')\n",
    "        self.test_run = self.test_run_override or ('0','12','Sherlock5','1')\n",
    "        train_runs = [rk for rk in all_runs if rk not in (self.val_run, self.test_run)]\n",
    "\n",
    "        self.train_ds = LibriBrainWord(self.data_path, partition=\"train\",\n",
    "                                       keyword_detection=\"watson\",\n",
    "                                       preload_files=False,\n",
    "                                       include_info=False,\n",
    "                                       positive_buffer=0.25,\n",
    "                                       standardize=self.standardize_train)\n",
    "        self.val_ds   = LibriBrainWord(self.data_path, partition=\"validation\",\n",
    "                                       keyword_detection=\"watson\",\n",
    "                                       preload_files=False,\n",
    "                                       include_info=False,\n",
    "                                       standardize=True,\n",
    "                                       positive_buffer=0.25,\n",
    "                                       channel_means=getattr(self.train_ds, \"channel_means\", None),\n",
    "                                       channel_stds=getattr(self.train_ds, \"channel_stds\", None)\n",
    "                                      )\n",
    "        self.test_ds  = LibriBrainWord(self.data_path, partition=\"test\",\n",
    "                                       keyword_detection=\"watson\",\n",
    "                                       preload_files=False,\n",
    "                                       include_info=False,\n",
    "                                       standardize=True,\n",
    "                                       positive_buffer=0.25,\n",
    "                                       channel_means=getattr(self.train_ds, \"channel_means\", None),\n",
    "                                       channel_stds=getattr(self.train_ds, \"channel_stds\", None)\n",
    "                                      )\n",
    "\n",
    "        key = self._hash_key(train_runs); cache_file = self._cache_paths(key)\n",
    "        pos_idx, neg_idx = self._build_pos_neg_indices(self.train_ds, cache_file)\n",
    "        if len(pos_idx) == 0:\n",
    "            raise RuntimeError(\"No positive samples found in training set; cannot build balanced sampler.\")\n",
    "        self._pos_idx, self._neg_idx = pos_idx, neg_idx\n",
    "        self._train_sampler = BalancedBatchSampler(pos_idx, neg_idx, batch_size=self.batch_size,\n",
    "                                                   pos_fraction=self.target_pos_fraction)\n",
    "\n",
    "    def estimate_label_stats(self, sample: int = 200_000) -> Tuple[int,int]:\n",
    "        if hasattr(self, \"_pos_idx\") and hasattr(self, \"_neg_idx\"):\n",
    "            pos, total = len(self._pos_idx), len(self._pos_idx) + len(self._neg_idx)\n",
    "            if sample < total and total > 0:\n",
    "                frac = sample / total; pos = max(1, int(round(pos * frac))); total = sample\n",
    "            return pos, total\n",
    "        n = len(self.train_ds); k = min(20_000, n)\n",
    "        idxs = random.sample(range(n), k=k)\n",
    "        pos = sum(int(self.train_ds[i][1]) for i in idxs)\n",
    "        return pos, k\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_sampler=self._train_sampler,\n",
    "                          num_workers=self.num_workers, pin_memory=self.pin_memory,\n",
    "                          persistent_workers=(self.num_workers > 0),\n",
    "                          prefetch_factor=2 if self.num_workers > 0 else None)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=self.pin_memory,\n",
    "                          persistent_workers=(self.num_workers > 0),\n",
    "                          prefetch_factor=2 if self.num_workers > 0 else None)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=self.pin_memory,\n",
    "                          persistent_workers=(self.num_workers > 0),\n",
    "                          prefetch_factor=2 if self.num_workers > 0 else None)\n",
    "\n",
    "# ---------------- Train ----------------\n",
    "def main():\n",
    "    data_path   = \"dataset\"\n",
    "    tmin, tmax  = 0, 0.85\n",
    "    epochs      = 30\n",
    "    batch_size  = 256\n",
    "    lr          = 1e-4\n",
    "    num_workers = 4\n",
    "    precision   = \"bf16-mixed\"\n",
    "    devices     = 1\n",
    "    target_pos_fraction = 0.05\n",
    "    lse_temperature     = 0.5\n",
    "\n",
    "    VAL_RUN  = ('0','12','Sherlock4','1')\n",
    "    TEST_RUN = ('0','12','Sherlock5','1')\n",
    "\n",
    "    pl.seed_everything(42, workers=True)\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    dm = LibriBrainWordDataModule(\n",
    "        data_path, tmin, tmax, batch_size, num_workers,\n",
    "        standardize_train=True, target_pos_fraction=target_pos_fraction\n",
    "    )\n",
    "    dm.setup()\n",
    "\n",
    "    pos, total = dm.estimate_label_stats(sample=200_000)\n",
    "    base_rate = pos / total\n",
    "    pos_weight = 1.0  # oversampling-only\n",
    "    print(f\"[Label stats] pos={pos} total={total}  π={base_rate:.6f}  \"\n",
    "          f\"target_p={target_pos_fraction:.2f}  pos_weight_eff={pos_weight:.1f}\")\n",
    "    print(f\"[Config] window=({tmin:.2f},{tmax:.2f})  sampler_pos={target_pos_fraction:.2f}  \"\n",
    "          f\"loss=focal(0.95,2.0)+pairwise(0.5)  pooling=attention\")\n",
    "\n",
    "    neptune_logger = make_neptune_logger(run_name=\"watson-meg\")\n",
    "    if neptune_logger:\n",
    "        neptune_logger.log_hyperparams({\n",
    "            \"data_path\": data_path, \"tmin\": tmin, \"tmax\": tmax,\n",
    "            \"batch_size\": batch_size, \"lr\": lr, \"precision\": precision,\n",
    "            \"base_rate\": base_rate, \"target_pos_fraction\": target_pos_fraction,\n",
    "            \"loss\": \"focal(alpha=0.95,gamma=2.0)+pairwise(lambda=0.5)\", \"pooling\": \"temporal_attention\",\n",
    "            \"pos_weight_eff\": pos_weight,\n",
    "            \"val_run\": \"_\".join(VAL_RUN), \"test_run\": \"_\".join(TEST_RUN),\n",
    "            \"schedule\": \"warmup+cosine\",\n",
    "        })\n",
    "\n",
    "    model = WatsonKeywordPL(\n",
    "        in_channels=306, pos_weight=pos_weight,\n",
    "        opt=OptimConfig(lr=lr, weight_decay=1e-4, max_time_shift=4, noise_std=0.01,\n",
    "                        warmup_epochs=1, cosine_after_warmup=True),\n",
    "        lse_temperature=lse_temperature\n",
    "    )\n",
    "\n",
    "    ckpt_cb = ModelCheckpoint(monitor=\"val_auprc\", mode=\"max\", save_top_k=1, filename=\"best-val-auprc\")\n",
    "    callbacks = [ckpt_cb, EarlyStopping(monitor=\"val_auprc\", mode=\"max\", patience=6, min_delta=5e-4),\n",
    "                 LearningRateMonitor(logging_interval=\"epoch\")]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs, precision=precision, devices=devices,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        callbacks=callbacks, logger=neptune_logger,\n",
    "        log_every_n_steps=25, gradient_clip_val=1.0,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    if neptune_logger and ckpt_cb.best_model_path:\n",
    "        try:\n",
    "            neptune_logger.experiment[\"artifacts/checkpoints/best\"].upload(ckpt_cb.best_model_path)\n",
    "            print(f\"Neptune: uploaded best checkpoint -> {ckpt_cb.best_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Neptune: failed to upload checkpoint: {e}\")\n",
    "\n",
    "    trainer.test(model, datamodule=dm)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e10d9-c14d-4751-95b8-ff7b2080320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# STRICT sessions + no partition/include mixing:\n",
    "# - We NEVER pass `partition` when `include_run_keys` is used.\n",
    "# - If the exact requested sessions don't exist, we raise with a helpful message.\n",
    "\n",
    "import os, json, shutil\n",
    "from typing import Tuple, List\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "\n",
    "class KeywordDataModule(LibriBrainWordDataModule):\n",
    "    def __init__(self, keyword: str,\n",
    "                 val_task: str, val_ses: str,\n",
    "                 test_task: str, test_ses: str,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.keyword = str(keyword)\n",
    "        self._val_task, self._val_ses = str(val_task), str(val_ses)\n",
    "        self._test_task, self._test_ses = str(test_task), str(test_ses)\n",
    "\n",
    "    def _pick_run_strict(self, task: str, ses: str) -> Tuple[str,str,str,str]:\n",
    "        avail = [rk for rk in self._available_runs()]\n",
    "        matches = [rk for rk in avail if rk[2] == task and rk[1] == ses]\n",
    "        if matches:\n",
    "            # Prefer the smallest run index if multiple exist\n",
    "            def _run_sort_key(rk):\n",
    "                rnum = int(rk[3]) if str(rk[3]).isdigit() else 10**9\n",
    "                return (rk[0], rk[1], rk[2], rnum)\n",
    "            chosen = sorted(matches, key=_run_sort_key)[0]\n",
    "            print(f\"[INFO] Using exact run for {task} ses-{ses}: {chosen}\")\n",
    "            return chosen\n",
    "\n",
    "        tasks_present = sorted({t for _,_,t,_ in avail})\n",
    "        sessions_for_task = sorted({s for _,s,t,_ in avail if t == task})\n",
    "        raise RuntimeError(\n",
    "            \"Requested validation/test run not found.\\n\"\n",
    "            f\"  requested: task={task}, ses={ses}\\n\"\n",
    "            f\"  tasks present: {tasks_present}\\n\"\n",
    "            f\"  sessions present for task={task}: {sessions_for_task}\\n\"\n",
    "            \"Please adjust the requested session or populate the dataset.\"\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        all_runs = [rk for rk in self._available_runs()]\n",
    "        self.val_run  = self._pick_run_strict(self._val_task,  self._val_ses)\n",
    "        self.test_run = self._pick_run_strict(self._test_task, self._test_ses)\n",
    "\n",
    "        # TRAIN = everything except the strict val/test run\n",
    "        train_runs: List[Tuple[str,str,str,str]] = [rk for rk in all_runs if rk not in (self.val_run, self.test_run)]\n",
    "        if not train_runs:\n",
    "            raise RuntimeError(\"No training runs left after excluding strict val/test.\")\n",
    "\n",
    "        # --- Datasets: partition=None; we use include_run_keys explicitly. ---\n",
    "        self.train_ds = LibriBrainWord(\n",
    "            self.data_path,\n",
    "            partition=None,\n",
    "            keyword_detection=self.keyword,\n",
    "            include_run_keys=train_runs,\n",
    "            preload_files=False, include_info=False,\n",
    "            standardize=self.standardize_train,\n",
    "            positive_buffer=0.25,\n",
    "        )\n",
    "        self.val_ds = LibriBrainWord(\n",
    "            self.data_path,\n",
    "            partition=None,\n",
    "            keyword_detection=self.keyword,\n",
    "            include_run_keys=[self.val_run],\n",
    "            preload_files=False, include_info=False,\n",
    "            standardize=True, positive_buffer=0.25,\n",
    "            channel_means=getattr(self.train_ds, \"channel_means\", None),\n",
    "            channel_stds=getattr(self.train_ds, \"channel_stds\", None),\n",
    "        )\n",
    "        self.test_ds = LibriBrainWord(\n",
    "            self.data_path,\n",
    "            partition=None,\n",
    "            keyword_detection=self.keyword,\n",
    "            include_run_keys=[self.test_run],\n",
    "            preload_files=False, include_info=False,\n",
    "            standardize=True, positive_buffer=0.25,\n",
    "            channel_means=getattr(self.train_ds, \"channel_means\", None),\n",
    "            channel_stds=getattr(self.train_ds, \"channel_stds\", None),\n",
    "        )\n",
    "\n",
    "        # Balanced sampler\n",
    "        key = self._hash_key(train_runs); cache_file = self._cache_paths(key)\n",
    "        pos_idx, neg_idx = self._build_pos_neg_indices(self.train_ds, cache_file)\n",
    "        if len(pos_idx) == 0:\n",
    "            raise RuntimeError(\"No positive samples found in training set; cannot build balanced sampler.\")\n",
    "        self._pos_idx, self._neg_idx = pos_idx, neg_idx\n",
    "        self._train_sampler = BalancedBatchSampler(pos_idx, neg_idx, batch_size=self.batch_size,\n",
    "                                                   pos_fraction=self.target_pos_fraction)\n",
    "\n",
    "def run_keyword(keyword: str,\n",
    "                val_task=\"Sherlock4\", val_ses=\"1\",\n",
    "                test_task=\"Sherlock6\", test_ses=\"11\",\n",
    "                seeds=(1,2,3),\n",
    "                out_root=\"playground/keyword-length\",\n",
    "                data_path=\"dataset\"):\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    results_summary = []\n",
    "\n",
    "    tmin, tmax  = 0, 0.85\n",
    "    epochs      = 30\n",
    "    batch_size  = 256\n",
    "    lr          = 1e-4\n",
    "    num_workers = 4\n",
    "    precision   = \"bf16-mixed\"\n",
    "    devices     = 1\n",
    "    target_pos_fraction = 0.05\n",
    "    lse_temperature     = 0.5\n",
    "\n",
    "    for seed in seeds:\n",
    "        out_dir = os.path.join(out_root, f\"{keyword}\", f\"seed-{seed}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        pl.seed_everything(seed, workers=True)\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "        dm = KeywordDataModule(\n",
    "            keyword=keyword,\n",
    "            val_task=val_task, val_ses=val_ses,\n",
    "            test_task=test_task, test_ses=test_ses,\n",
    "            data_path=data_path, tmin=tmin, tmax=tmax,\n",
    "            batch_size=batch_size, num_workers=num_workers,\n",
    "            standardize_train=True, target_pos_fraction=target_pos_fraction\n",
    "        )\n",
    "        dm.setup()\n",
    "\n",
    "        pos, total = dm.estimate_label_stats(sample=200_000)\n",
    "        base_rate = pos / total if total > 0 else 0.0\n",
    "        print(f\"\\n=== {keyword} | seed {seed} ===\")\n",
    "        print(f\"[Label stats] pos={pos} total={total}  π={base_rate:.6f}\")\n",
    "        print(f\"[VAL]  {dm.val_run}  | [TEST] {dm.test_run}\")\n",
    "\n",
    "        neptune_logger = make_neptune_logger(run_name=f\"kws-{keyword}-seed{seed}\")\n",
    "\n",
    "        model = WatsonKeywordPL(\n",
    "            in_channels=306, pos_weight=1.0,\n",
    "            opt=OptimConfig(lr=lr, weight_decay=1e-4, max_time_shift=4, noise_std=0.01,\n",
    "                            warmup_epochs=1, cosine_after_warmup=True),\n",
    "            lse_temperature=lse_temperature\n",
    "        )\n",
    "\n",
    "        ckpt_cb = ModelCheckpoint(monitor=\"val_auprc\", mode=\"max\", save_top_k=1, filename=\"best-val-auprc\")\n",
    "        callbacks = [\n",
    "            ckpt_cb,\n",
    "            EarlyStopping(monitor=\"val_auprc\", mode=\"max\", patience=6, min_delta=5e-4),\n",
    "            LearningRateMonitor(logging_interval=\"epoch\")\n",
    "        ]\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=epochs, precision=precision, devices=devices,\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            callbacks=callbacks, logger=neptune_logger,\n",
    "            log_every_n_steps=25, gradient_clip_val=1.0,\n",
    "            default_root_dir=out_dir,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, datamodule=dm)\n",
    "\n",
    "        if neptune_logger and ckpt_cb.best_model_path:\n",
    "            try:\n",
    "                neptune_logger.experiment[\"artifacts/checkpoints/best\"].upload(ckpt_cb.best_model_path)\n",
    "                print(f\"Neptune: uploaded best checkpoint -> {ckpt_cb.best_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Neptune: failed to upload checkpoint: {e}\")\n",
    "\n",
    "        test_metrics = trainer.test(model, datamodule=dm)\n",
    "        print(f\"[Test metrics] {test_metrics}\")\n",
    "\n",
    "        # Move predictions CSV into run folder (if produced by on_test_epoch_end)\n",
    "        src_pred = os.path.join(\"playground\", \"test_predictions.csv\")\n",
    "        dst_pred = os.path.join(out_dir, \"test_predictions.csv\")\n",
    "        try:\n",
    "            if os.path.exists(src_pred):\n",
    "                os.makedirs(os.path.dirname(dst_pred), exist_ok=True)\n",
    "                shutil.move(src_pred, dst_pred)\n",
    "                print(f\"Moved predictions -> {dst_pred}\")\n",
    "            else:\n",
    "                print(\"No test_predictions.csv found to move.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to move predictions CSV: {e}\")\n",
    "\n",
    "        with open(os.path.join(out_dir, \"test_metrics.json\"), \"w\") as f:\n",
    "            json.dump(test_metrics, f, indent=2)\n",
    "        with open(os.path.join(out_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"keyword\": keyword,\n",
    "                \"seed\": seed,\n",
    "                \"val_run\": \"_\".join(dm.val_run),\n",
    "                \"test_run\": \"_\".join(dm.test_run),\n",
    "                \"base_rate_estimate\": base_rate,\n",
    "                \"tmin\": tmin, \"tmax\": tmax,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"lr\": lr,\n",
    "                \"target_pos_fraction\": target_pos_fraction,\n",
    "                \"precision\": precision,\n",
    "            }, f, indent=2)\n",
    "\n",
    "        row = {\"keyword\": keyword, \"seed\": seed, \"val_run\": dm.val_run, \"test_run\": dm.test_run}\n",
    "        if isinstance(test_metrics, list) and test_metrics:\n",
    "            row.update({k: float(v) if isinstance(v, (int, float)) else v for k, v in test_metrics[0].items()})\n",
    "        results_summary.append(row)\n",
    "\n",
    "    with open(os.path.join(out_root, f\"{keyword}_summary.json\"), \"w\") as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    return results_summary\n",
    "\n",
    "# Kick off the three keywords with strict sessions.\n",
    "all_summaries = {}\n",
    "for kw in [\"walk\", \"surely\", \"excellent\"]:\n",
    "    all_summaries[kw] = run_keyword(\n",
    "        kw, val_task=\"Sherlock4\", val_ses=\"1\", test_task=\"Sherlock6\", test_ses=\"11\",\n",
    "        seeds=(1,2,3), out_root=\"playground/keyword-length\", data_path=\"dataset\"\n",
    "    )\n",
    "\n",
    "print(\"\\nDone. Summaries written under playground/keyword-length/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b69c9-2216-4300-99f4-9e4cce52f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Export test-set labels as CSV for a given keyword (strict sessions).\n",
    "import os, csv, torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def export_test_labels(keyword: str,\n",
    "                       val_task=\"Sherlock4\", val_ses=\"1\",\n",
    "                       test_task=\"Sherlock6\", test_ses=\"11\",\n",
    "                       data_path=\"dataset\",\n",
    "                       tmin=0.0, tmax=0.85,\n",
    "                       batch_size=512, num_workers=4,\n",
    "                       out_root=\"playground/keyword-length\"):\n",
    "    # Build the same strict DM as training used (partition=None + include_run_keys)\n",
    "    dm = KeywordDataModule(\n",
    "        keyword=keyword,\n",
    "        val_task=val_task, val_ses=val_ses,\n",
    "        test_task=test_task, test_ses=test_ses,\n",
    "        data_path=data_path, tmin=tmin, tmax=tmax,\n",
    "        batch_size=batch_size, num_workers=num_workers,\n",
    "        standardize_train=True, target_pos_fraction=0.05\n",
    "    )\n",
    "    dm.setup()\n",
    "\n",
    "    # Sequential order to match prediction CSV indexing\n",
    "    loader = DataLoader(\n",
    "        dm.test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True,\n",
    "        persistent_workers=(num_workers > 0),\n",
    "        prefetch_factor=2 if num_workers > 0 else None,\n",
    "        collate_fn=collate_label_only_xy,  # returns just labels\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for ys in loader:\n",
    "        labels.extend(int(y) for y in ys)\n",
    "\n",
    "    out_dir = os.path.join(out_root, keyword)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, \"test_labels.csv\")\n",
    "    with open(out_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"index\", \"label\"])\n",
    "        for i, y in enumerate(labels):\n",
    "            w.writerow([i, y])\n",
    "\n",
    "    print(f\"Wrote {len(labels)} test labels -> {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# Example: export for the three keywords used above\n",
    "for kw in [\"walk\", \"surely\", \"excellent\"]:\n",
    "    export_test_labels(kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffb06d-8b0d-4f01-a65e-bb6345aed202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
