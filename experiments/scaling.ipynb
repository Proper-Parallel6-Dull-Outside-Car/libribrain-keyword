{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab43f6-bfba-4226-aa71-99348d19a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q mne_bids lightning torchmetrics scikit-learn plotly ipywidgets neptune\n",
    "\n",
    "# Set up base path for dataset and related files\n",
    "base_path = \"./libribrain\"\n",
    "\n",
    "# Install pnpl from local modified package\n",
    "%pip install ../modified-pnpl/pnpl\n",
    "\n",
    "# Remember to set the NEPTUNE_API_TOKEN and NEPTUNE_PROJECT environment variables\n",
    "# before running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5229d68-94d8-4963-82cf-308662975eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Keyword spotting on LibriBrain MEG — operationalized runner (scaling sweep)\n",
    "# - Adds train_fraction to use a subset of the train set\n",
    "# - Sweeps fractions: 0.05, 0.10, 0.20, 0.40, 0.60, 0.80, 1.00\n",
    "# - Logs hours_per_epoch and hours_total (samples-consumed * window_seconds)\n",
    "# - Leaves validation/test sets unchanged\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, math, random, json, hashlib, time, platform, subprocess\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Iterator\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, BatchSampler\n",
    "\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# ============================== Sensor mask (optional) ==============================\n",
    "\n",
    "SENSORS_SPEECH_MASK = [18, 20, 22, 23, 45, 120, 138, 140, 142, 143, 145, 146, 147, 149, 175, 176, 177, 179, 180, 198, 271, 272, 275]\n",
    "\n",
    "class ChannelMaskedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, channel_indices=None):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.channel_indices = channel_indices if channel_indices is not None else SENSORS_SPEECH_MASK\n",
    "    def __len__(self): return len(self.base_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base_dataset[idx]\n",
    "        x_sel = x[self.channel_indices]\n",
    "        if torch.is_tensor(y) and y.ndim > 0:\n",
    "            y = y[y.shape[0] // 2]\n",
    "        return x_sel, y\n",
    "\n",
    "# ----------------- small helper for fast label scans -----------------\n",
    "\n",
    "def collate_label_only_xy(batch):  # list[(x,y)] -> list[int]\n",
    "    return [int(y) for _, y in batch]\n",
    "\n",
    "# ============================== Neptune (optional) ==============================\n",
    "\n",
    "def make_neptune_logger(run_name: str | None = None):\n",
    "    api_key = os.getenv(\"NEPTUNE_API_TOKEN\")\n",
    "    project = os.getenv(\"NEPTUNE_PROJECT\")\n",
    "    if not api_key or not project:\n",
    "        print(\"Neptune: env vars not found -> skipping Neptune logging.\")\n",
    "        return None\n",
    "    try:\n",
    "        from lightning.pytorch.loggers import NeptuneLogger as _BaseNeptuneLogger\n",
    "    except Exception:\n",
    "        from pytorch_lightning.loggers import NeptuneLogger as _BaseNeptuneLogger\n",
    "\n",
    "    class CleanNeptuneLogger(_BaseNeptuneLogger):\n",
    "        def log_metrics(self, metrics, step: int | None = None):\n",
    "            filt = {k: v for k, v in metrics.items() if k != \"epoch\" and not k.endswith(\"/epoch\")}\n",
    "            super().log_metrics(filt, step=None)\n",
    "\n",
    "    logger = CleanNeptuneLogger(\n",
    "        api_key=api_key, project=project, name=run_name,\n",
    "        tags=[\"libribrain\", \"watson\", \"meg\", \"keyword-detection\", \"generalization\", \"scaling\"],\n",
    "        prefix=\"training/\", log_model_checkpoints=False,\n",
    "    )\n",
    "    print(\"Neptune: ✅ enabled.\")\n",
    "    return logger\n",
    "\n",
    "# ============================== Model bits ==============================\n",
    "\n",
    "@dataclass\n",
    "class OptimConfig:\n",
    "    lr: float = 2.5e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    warmup_epochs: int = 2\n",
    "    cosine_after_warmup: bool = True\n",
    "    noise_std: float = 0.01\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: float = 0.95, gamma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = float(alpha); self.gamma = float(gamma)\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        ce = nn.functional.binary_cross_entropy_with_logits(logits, targets.float(), reduction='none')\n",
    "        p = torch.sigmoid(logits)\n",
    "        pt = torch.where(targets == 1, p, 1 - p)\n",
    "        alpha_t = torch.where(\n",
    "            targets == 1,\n",
    "            torch.as_tensor(self.alpha, device=logits.device, dtype=logits.dtype),\n",
    "            torch.as_tensor(1 - self.alpha, device=logits.device, dtype=logits.dtype),\n",
    "        )\n",
    "        loss = alpha_t * (1 - pt).pow(self.gamma) * ce\n",
    "        return loss.mean()\n",
    "\n",
    "class ChannelSE(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 4):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc  = nn.Sequential(\n",
    "            nn.Linear(channels, max(1, channels // reduction), bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(max(1, channels // reduction), channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.avg(x).squeeze(-1)      # (B,C)\n",
    "        w = self.fc(w).unsqueeze(-1)     # (B,C,1)\n",
    "        return x * w\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(1e4) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1,T,d)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "def _gn(c: int, groups: int = 16):\n",
    "    g = max(1, min(groups, c))\n",
    "    return nn.GroupNorm(num_groups=g, num_channels=c)\n",
    "\n",
    "class EnhancedKeywordNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SE-gated 1-D Conv → 2-layer Transformer → temporal attention pool.\n",
    "    GroupNorm for domain robustness (vs BN).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, d_model: int = 128, n_heads: int = 4, n_layers: int = 2, dropout: float = 0.15):\n",
    "        super().__init__()\n",
    "        self.se = ChannelSE(in_channels)\n",
    "\n",
    "        same = 'same' in nn.Conv1d.__init__.__code__.co_varnames\n",
    "        k7pad = 'same' if same else 3\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, d_model, 7, 2, k7pad, bias=False),\n",
    "            _gn(d_model), nn.ELU(),\n",
    "            nn.Conv1d(d_model, d_model, 7, 2, k7pad, bias=False),\n",
    "            _gn(d_model), nn.ELU()\n",
    "        )\n",
    "\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.pe          = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.dropout_h   = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_logit_t  = nn.Linear(d_model, 1)\n",
    "        self.to_attn_t   = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,C,T) -> logits: (B,)\n",
    "        x = self.se(x)\n",
    "        h = self.stem(x)              # (B,d,T')\n",
    "        h = h.permute(0, 2, 1)        # (B,T',d)\n",
    "        h = self.transformer(self.pe(h))\n",
    "        h = self.dropout_h(h)\n",
    "        logit_t = self.to_logit_t(h).transpose(1, 2)       # (B,1,T')\n",
    "        attn_t  = torch.softmax(self.to_attn_t(h).transpose(1, 2), dim=-1)\n",
    "        return (logit_t * attn_t).sum(-1).squeeze(1)\n",
    "\n",
    "# ============================== EMA helper ==============================\n",
    "\n",
    "class EMAHelper:\n",
    "    def __init__(self, module: nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {n: p.detach().clone()\n",
    "                       for n, p in module.named_parameters() if p.requires_grad}\n",
    "        self.backup = None\n",
    "    @torch.no_grad()\n",
    "    def update(self, module: nn.Module):\n",
    "        for n, p in module.named_parameters():\n",
    "            if not p.requires_grad: continue\n",
    "            self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
    "    @torch.no_grad()\n",
    "    def apply_to(self, module: nn.Module):\n",
    "        self.backup = {n: p.detach().clone()\n",
    "                       for n, p in module.named_parameters() if p.requires_grad}\n",
    "        for n, p in module.named_parameters():\n",
    "            if not p.requires_grad: continue\n",
    "            p.copy_(self.shadow[n])\n",
    "    @torch.no_grad()\n",
    "    def restore(self, module: nn.Module):\n",
    "        if self.backup is None: return\n",
    "        for n, p in module.named_parameters():\n",
    "            if not p.requires_grad: continue\n",
    "            p.copy_(self.backup[n])\n",
    "        self.backup = None\n",
    "\n",
    "# ============================== Metrics + module ==============================\n",
    "\n",
    "try:\n",
    "    from torchmetrics.classification import BinaryAccuracy, BinaryAveragePrecision, BinaryAUROC\n",
    "except Exception:\n",
    "    from torchmetrics import Accuracy as BinaryAccuracy                 # type: ignore\n",
    "    from torchmetrics import AveragePrecision as BinaryAveragePrecision # type: ignore\n",
    "    from torchmetrics import AUROC as BinaryAUROC                       # type: ignore\n",
    "\n",
    "class WatsonKeywordPL(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        opt: OptimConfig,\n",
    "        loss_mode: str = \"focal_pairwise\",\n",
    "        pairwise_lambda: float = 1.0,\n",
    "        # priors for correction at inference:\n",
    "        pi_train: float = 0.10,      # effective positive rate in training batches (sampler)\n",
    "        pi_target: float = 0.003,    # true base rate in the wild / dataset\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = EnhancedKeywordNet(in_channels)\n",
    "\n",
    "        self.criterion = FocalLoss(alpha=0.95, gamma=2.0)\n",
    "        self.loss_mode = loss_mode\n",
    "        self.pairwise_lambda = float(pairwise_lambda)\n",
    "\n",
    "        # torchmetrics\n",
    "        self.train_acc = BinaryAccuracy()\n",
    "        self.val_acc   = BinaryAccuracy()\n",
    "        self.test_acc  = BinaryAccuracy()\n",
    "        self.val_auprc = BinaryAveragePrecision()\n",
    "        self.test_auprc= BinaryAveragePrecision()\n",
    "        self.val_auroc = BinaryAUROC()\n",
    "        self.test_auroc= BinaryAUROC()\n",
    "\n",
    "        # buffers for diagnostics & calibration\n",
    "        self._val_probs: List[torch.Tensor] = []; self._val_labels: List[torch.Tensor] = []\n",
    "        self._val_logits: List[torch.Tensor] = []\n",
    "        self._test_probs: List[torch.Tensor] = []; self._test_labels: List[torch.Tensor] = []\n",
    "        self.register_buffer(\"temp_scale\", torch.tensor(1.0))  # temperature for logits at inference\n",
    "        self.prior_bias = math.log((pi_target / (1 - pi_target)) / (pi_train / (1 - pi_train)))\n",
    "        self._ema: Optional[EMAHelper] = None\n",
    "\n",
    "        # exported at test end (for CSV)\n",
    "        self.last_test_probs: Optional[torch.Tensor] = None\n",
    "        self.last_test_labels: Optional[torch.Tensor] = None\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @staticmethod\n",
    "    def _pairwise_logistic_loss(logits: torch.Tensor, labels: torch.Tensor, max_pairs: int = 4096) -> torch.Tensor:\n",
    "        pos_idx = (labels == 1).nonzero(as_tuple=False).view(-1)\n",
    "        neg_idx = (labels == 0).nonzero(as_tuple=False).view(-1)\n",
    "        if pos_idx.numel() == 0 or neg_idx.numel() == 0: return logits.new_zeros(())\n",
    "        num_pairs = min(max_pairs, int(pos_idx.numel()) * int(neg_idx.numel()))\n",
    "        pi = pos_idx[torch.randint(0, pos_idx.numel(), (num_pairs,), device=logits.device)]\n",
    "        ni = neg_idx[torch.randint(0, neg_idx.numel(), (num_pairs,), device=logits.device)]\n",
    "        diff = logits[pi] - logits[ni]\n",
    "        return torch.nn.functional.softplus(-diff).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def _rprecision(probs: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "        m = int(labels.sum().item())\n",
    "        if m <= 0: return 0.0\n",
    "        k = min(m, probs.numel())\n",
    "        topk = torch.topk(probs, k=k, largest=True).indices\n",
    "        return float(labels[topk].float().mean().item())\n",
    "\n",
    "    @staticmethod\n",
    "    def _precision_recall_at_k(probs: torch.Tensor, labels: torch.Tensor, k: int) -> Tuple[float,float]:\n",
    "        k = max(1, min(k, probs.numel()))\n",
    "        topk = torch.topk(probs, k=k, largest=True).indices\n",
    "        tp = labels[topk].sum().item()\n",
    "        prec = tp / k\n",
    "        total_pos = max(1, int(labels.sum().item()))\n",
    "        rec = tp / total_pos\n",
    "        return float(prec), float(rec)\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_f1(probs: torch.Tensor, labels: torch.Tensor) -> Tuple[float, float, Tuple[int,int,int,int]]:\n",
    "        N = probs.numel()\n",
    "        if N == 0: return 0.0, 0.5, (0,0,0,0)\n",
    "        sort_idx = torch.argsort(probs, descending=True)\n",
    "        sorted_labels = labels[sort_idx].to(torch.int32)\n",
    "        cum_tp = torch.cumsum(sorted_labels, dim=0)\n",
    "        ks = torch.arange(1, N+1, device=probs.device)\n",
    "        precision = cum_tp / ks\n",
    "        total_pos = max(1, int(labels.sum().item()))\n",
    "        recall = cum_tp / total_pos\n",
    "        denom = precision + recall\n",
    "        f1 = torch.where(denom > 0, 2 * precision * recall / denom, torch.zeros_like(denom))\n",
    "        best_idx = int(torch.argmax(f1).item())\n",
    "        best_f1 = float(f1[best_idx].item())\n",
    "        thr = float(probs[sort_idx[best_idx]].item())\n",
    "        k = best_idx + 1\n",
    "        tp = int(cum_tp[best_idx].item()); fp = int(k - tp); fn = int(total_pos - tp); tn = int(N - k - fn)\n",
    "        return best_f1, thr, (tp, fp, tn, fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _recall_at_precision(probs: torch.Tensor, labels: torch.Tensor, min_precision: float = 0.90) -> float:\n",
    "        N = probs.numel()\n",
    "        if N == 0: return 0.0\n",
    "        sort_idx = torch.argsort(probs, descending=True)\n",
    "        sorted_labels = labels[sort_idx].to(torch.int32)\n",
    "        cum_tp = torch.cumsum(sorted_labels, dim=0)\n",
    "        ks = torch.arange(1, N+1, device=probs.device)\n",
    "        precision = cum_tp / ks\n",
    "        total_pos = max(1, int(labels.sum().item()))\n",
    "        recall = cum_tp / total_pos\n",
    "        mask = precision >= min_precision\n",
    "        return float(recall[mask].max().item()) if mask.any() else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _macro_f1_balanced_acc(probs: torch.Tensor, labels: torch.Tensor, thr: float = 0.5):\n",
    "        preds = (probs >= thr).to(torch.int32)\n",
    "        labels_i = labels.to(torch.int32)\n",
    "        tp = int(((preds == 1) & (labels_i == 1)).sum().item())\n",
    "        fp = int(((preds == 1) & (labels_i == 0)).sum().item())\n",
    "        fn = int(((preds == 0) & (labels_i == 1)).sum().item())\n",
    "        tn = int(((preds == 0) & (labels_i == 0)).sum().item())\n",
    "        # F1 for pos and neg\n",
    "        def _f1(tp_, fp_, fn_):\n",
    "            p = tp_ / max(1, tp_ + fp_)\n",
    "            r = tp_ / max(1, tp_ + fn_)\n",
    "            d = p + r\n",
    "            return float((2 * p * r / d) if d > 0 else 0.0)\n",
    "        f1_pos = _f1(tp, fp, fn)\n",
    "        f1_neg = _f1(tn, fn, fp)\n",
    "        f1_macro = 0.5 * (f1_pos + f1_neg)\n",
    "        # Balanced accuracy\n",
    "        tpr = tp / max(1, tp + fn)\n",
    "        tnr = tn / max(1, tn + fp)\n",
    "        bal_acc = 0.5 * (tpr + tnr)\n",
    "        return float(f1_macro), float(bal_acc), (tp, fp, tn, fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _brier(probs: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "        diff = probs.float() - labels.float()\n",
    "        return float((diff * diff).mean().item())\n",
    "\n",
    "    def _save_pr_curve(self, probs: torch.Tensor, labels: torch.Tensor, out_path: Path, title: str = \"Validation\"):\n",
    "        try:\n",
    "            y_true = labels.cpu().numpy()\n",
    "            y_score = probs.cpu().numpy()\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "            ap = average_precision_score(y_true, y_score)\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.step(recall, precision, where=\"post\")\n",
    "            plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "            plt.ylim(0.0, 1.05); plt.xlim(0.0, 1.0)\n",
    "            plt.title(f\"{title} PR curve (AP={ap:.4f})\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(str(out_path), bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"[PR] Failed to save PR curve to {out_path}: {e}\")\n",
    "\n",
    "    # ---------- Lightning lifecycle ----------\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        # EMA over backbone for stabler eval\n",
    "        self._ema = EMAHelper(self.model, decay=0.999)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        x = self._augment(x)\n",
    "        logits = self(x)\n",
    "\n",
    "        focal = self.criterion(logits.float(), y.float())\n",
    "        pairwise = self._pairwise_logistic_loss(logits, y)\n",
    "        loss = focal + self.pairwise_lambda * pairwise\n",
    "\n",
    "        probs = torch.sigmoid(logits.float())\n",
    "        self.train_acc.update(probs, y)\n",
    "\n",
    "        # metrics/logs\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train_focal\", focal, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_pairwise\", pairwise, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_pos_frac_batch\", y.float().mean(), on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        if self._ema is not None:\n",
    "            self._ema.update(self.model)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"train_acc\", self.train_acc.compute(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.train_acc.reset()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        if self._ema is not None:\n",
    "            self._ema.apply_to(self.model)\n",
    "        self._val_probs.clear(); self._val_labels.clear(); self._val_logits.clear()\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        probs = torch.sigmoid(logits.float())\n",
    "\n",
    "        # accumulate\n",
    "        self.val_acc.update(probs, y); self.val_auprc.update(probs, y); self.val_auroc.update(probs, y)\n",
    "        self._val_probs.append(probs.detach().float().cpu())\n",
    "        self._val_labels.append(y.detach().int().cpu())\n",
    "        self._val_logits.append(logits.detach().float().cpu())\n",
    "\n",
    "    def _fit_temperature_on_val(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        # Prior-correct logits then fit a scalar temperature to minimize NLL on val\n",
    "        device = logits.device\n",
    "        prior_bias = torch.tensor(self.prior_bias, device=device, dtype=logits.dtype)\n",
    "        z = logits + prior_bias\n",
    "        t_raw = torch.tensor(0.0, device=device, requires_grad=True)  # T = softplus(t_raw) + eps\n",
    "        opt = torch.optim.LBFGS([t_raw], lr=0.5, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            T = torch.nn.functional.softplus(t_raw) + 1e-4\n",
    "            loss = nn.functional.binary_cross_entropy_with_logits(z / T, labels.float())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        try:\n",
    "            opt.step(closure)\n",
    "            with torch.no_grad():\n",
    "                T = torch.nn.functional.softplus(t_raw) + 1e-4\n",
    "                self.temp_scale.copy_(T.clamp(1e-3, 100.0))\n",
    "        except Exception as e:\n",
    "            print(f\"[Calib] Temperature fit failed: {e}. Keeping T=1.0.\")\n",
    "            self.temp_scale.copy_(torch.tensor(1.0, device=self.device))\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        probs = torch.cat(self._val_probs, dim=0) if self._val_probs else torch.empty(0)\n",
    "        labels= torch.cat(self._val_labels, dim=0) if self._val_labels else torch.empty(0, dtype=torch.int64)\n",
    "        logits= torch.cat(self._val_logits, dim=0) if self._val_logits else torch.empty(0)\n",
    "\n",
    "        base_rate = float(labels.float().mean().item()) if labels.numel() > 0 else 0.0\n",
    "        both_classes = (labels.sum().item() > 0) and (labels.sum().item() < labels.numel())\n",
    "\n",
    "        try:  val_auprc = self.val_auprc.compute()\n",
    "        except Exception: val_auprc = torch.as_tensor(base_rate, device=self.device)\n",
    "        try:  val_auroc = self.val_auroc.compute() if both_classes else torch.as_tensor(0.5, device=self.device)\n",
    "        except Exception: val_auroc = torch.as_tensor(0.5, device=self.device)\n",
    "        val_acc = self.val_acc.compute()\n",
    "\n",
    "        rprec = self._rprecision(probs, labels)\n",
    "        m = int(labels.sum().item()) if labels.numel() > 0 else 0\n",
    "        prec_m, rec_m = self._precision_recall_at_k(probs, labels, max(1, m))\n",
    "        prec_2m, rec_2m = self._precision_recall_at_k(probs, labels, max(1, 2*m))\n",
    "        prec_5m, rec_5m = self._precision_recall_at_k(probs, labels, max(1, 5*m))\n",
    "        best_f1, best_thr, (tp, fp, tn, fn) = self._best_f1(probs, labels)\n",
    "        rec_at_p90 = self._recall_at_precision(probs, labels, 0.90)\n",
    "\n",
    "        print(f\"[VAL] base_rate={base_rate:.6f}  AUPRC={float(val_auprc):.4f}  AUROC={float(val_auroc):.4f}  \"\n",
    "              f\"RPrec={rprec:.4f}  BestF1={best_f1:.4f} @thr={best_thr:.4f}  \"\n",
    "              f\"Rec@P>=0.90={rec_at_p90:.4f}  Conf(TP/FP/TN/FN)={tp}/{fp}/{tn}/{fn}\")\n",
    "\n",
    "        # log\n",
    "        self.log(\"val_acc\", val_acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_auprc\", val_auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_auroc\", val_auroc, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_pos_rate\", torch.as_tensor(base_rate, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_random_auprc\", torch.as_tensor(base_rate, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_rprecision\", torch.as_tensor(rprec, device=self.device), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_precision_at_M\", torch.as_tensor(prec_m, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_at_M\", torch.as_tensor(rec_m, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_precision_at_2M\", torch.as_tensor(prec_2m, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_at_2M\", torch.as_tensor(rec_2m, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_precision_at_5M\", torch.as_tensor(prec_5m, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_at_5M\", torch.as_tensor(rec_5m, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_best_f1\", torch.as_tensor(best_f1, device=self.device), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_best_f1_threshold\", torch.as_tensor(best_thr, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall_at_precision_0.90\", torch.as_tensor(rec_at_p90, device=self.device), on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # extra metrics\n",
    "        f1_macro, bal_acc, _ = self._macro_f1_balanced_acc(probs, labels, thr=best_thr)\n",
    "        brier = self._brier(probs, labels)\n",
    "        self.log(\"val_f1_macro\", torch.as_tensor(f1_macro, device=self.device), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_bal_acc\", torch.as_tensor(bal_acc, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"val_brier\", torch.as_tensor(brier, device=self.device), on_step=False, on_epoch=True)\n",
    "\n",
    "        # PR curve figure\n",
    "        try:\n",
    "            out_path = Path(self.trainer.default_root_dir) / \"artifacts\" / \"val_pr_curve.png\"\n",
    "            self._save_pr_curve(probs, labels, out_path, title=\"Validation\")\n",
    "            print(f\"Saved validation PR curve -> {out_path}\")\n",
    "            # optional Neptune upload\n",
    "            try:\n",
    "                nexp = None\n",
    "                for lg in (getattr(self.trainer, \"loggers\", []) or []):\n",
    "                    h = getattr(lg, \"experiment\", None) or getattr(lg, \"run\", None)\n",
    "                    if h is not None:\n",
    "                        nexp = h; break\n",
    "                if nexp is not None:\n",
    "                    nexp[\"artifacts/plots/val_pr_curve\"].upload(str(out_path))\n",
    "            except Exception as e:\n",
    "                print(f\"[PR] Neptune upload skipped: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[PR] Failed to generate validation PR curve: {e}\")\n",
    "\n",
    "        # temperature scaling on validation (after logs)\n",
    "        if logits.numel() > 0 and labels.sum().item() > 0:\n",
    "            self._fit_temperature_on_val(logits.to(self.device), labels.to(self.device))\n",
    "\n",
    "        # resets\n",
    "        self.val_acc.reset(); self.val_auprc.reset(); self.val_auroc.reset()\n",
    "        if self._ema is not None:\n",
    "            self._ema.restore(self.model)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        if self._ema is not None:\n",
    "            self._ema.apply_to(self.model)\n",
    "        self._test_probs.clear(); self._test_labels.clear()\n",
    "\n",
    "    def test_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        # prior correction + temperature scaling for calibrated inference\n",
    "        z = logits + self.prior_bias\n",
    "        z = z / self.temp_scale.clamp(1e-3, 100.0)\n",
    "        probs = torch.sigmoid(z.float())\n",
    "\n",
    "        self.test_acc.update(probs, y); self.test_auprc.update(probs, y); self.test_auroc.update(probs, y)\n",
    "        self._test_probs.append(probs.detach().float().cpu())\n",
    "        self._test_labels.append(y.detach().int().cpu())\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        probs = torch.cat(self._test_probs, dim=0) if self._test_probs else torch.empty(0)\n",
    "        labels= torch.cat(self._test_labels, dim=0) if self._test_labels else torch.empty(0, dtype=torch.int64)\n",
    "        base_rate = float(labels.float().mean().item()) if labels.numel() > 0 else 0.0\n",
    "        both_classes = (labels.sum().item() > 0) and (labels.sum().item() < labels.numel())\n",
    "\n",
    "        try:  test_auprc = self.test_auprc.compute()\n",
    "        except Exception: test_auprc = torch.as_tensor(base_rate, device=self.device)\n",
    "        try:  test_auroc = self.test_auroc.compute() if both_classes else torch.as_tensor(0.5, device=self.device)\n",
    "        except Exception: test_auroc = torch.as_tensor(0.5, device=self.device)\n",
    "\n",
    "        rprec = self._rprecision(probs, labels)\n",
    "        m = int(labels.sum().item()) if labels.numel() > 0 else 0\n",
    "        prec_m, rec_m = self._precision_recall_at_k(probs, labels, max(1, m))\n",
    "        best_f1, best_thr, (tp, fp, tn, fn) = self._best_f1(probs, labels)\n",
    "        rec_at_p90 = self._recall_at_precision(probs, labels, 0.90)\n",
    "\n",
    "        print(f\"[TEST] base_rate={base_rate:.6f}  AUPRC={float(test_auprc):.4f}  AUROC={float(test_auroc):.4f}  \"\n",
    "              f\"RPrec={rprec:.4f}  BestF1={best_f1:.4f} @thr={best_thr:.4f}  \"\n",
    "              f\"Rec@P>=0.90={rec_at_p90:.4f}  Conf=~(TP/FP/TN/FN)={tp}/{fp}/{tn}/{fn}\")\n",
    "\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), on_step=False, on_epoch=True)\n",
    "        self.log(\"test_auprc\", test_auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_auroc\", test_auroc, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_rprecision\", torch.as_tensor(rprec, device=self.device), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_precision_at_M\", torch.as_tensor(prec_m, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"test_recall_at_M\", torch.as_tensor(rec_m, device=self.device), on_step=False, on_epoch=True)\n",
    "\n",
    "        # extra metrics (test)\n",
    "        f1_macro, bal_acc, _ = self._macro_f1_balanced_acc(probs, labels, thr=best_thr)\n",
    "        brier = self._brier(probs, labels)\n",
    "        self.log(\"test_f1_macro\", torch.as_tensor(f1_macro, device=self.device), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_bal_acc\", torch.as_tensor(bal_acc, device=self.device), on_step=False, on_epoch=True)\n",
    "        self.log(\"test_brier\", torch.as_tensor(brier, device=self.device), on_step=False, on_epoch=True)\n",
    "\n",
    "        # PR curve figure (test)\n",
    "        try:\n",
    "            out_path = Path(self.trainer.default_root_dir) / \"artifacts\" / \"test_pr_curve.png\"\n",
    "            self._save_pr_curve(probs, labels, out_path, title=\"Test\")\n",
    "            print(f\"Saved test PR curve -> {out_path}\")\n",
    "            # optional Neptune upload\n",
    "            try:\n",
    "                nexp = None\n",
    "                for lg in (getattr(self.trainer, \"loggers\", []) or []):\n",
    "                    h = getattr(lg, \"experiment\", None) or getattr(lg, \"run\", None)\n",
    "                    if h is not None:\n",
    "                        nexp = h; break\n",
    "                if nexp is not None:\n",
    "                    nexp[\"artifacts/plots/test_pr_curve\"].upload(str(out_path))\n",
    "            except Exception as e:\n",
    "                print(f\"[PR] Neptune upload skipped: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[PR] Failed to generate test PR curve: {e}\")\n",
    "\n",
    "        # export for CSV in main()\n",
    "        self.last_test_probs = probs.cpu()\n",
    "        self.last_test_labels = labels.cpu()\n",
    "\n",
    "        self.test_acc.reset(); self.test_auprc.reset(); self.test_auroc.reset()\n",
    "        if self._ema is not None:\n",
    "            self._ema.restore(self.model)\n",
    "\n",
    "    # ---------- aug ----------\n",
    "    def _augment(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training: return x\n",
    "        B, C, T = x.shape\n",
    "\n",
    "        # ±5% circular time-shift\n",
    "        max_s = int(0.05 * T)\n",
    "        if max_s > 0:\n",
    "            shifts = torch.randint(-max_s, max_s + 1, (B,), device=x.device)\n",
    "            for i, s in enumerate(shifts):\n",
    "                if int(s) != 0:\n",
    "                    x[i] = torch.roll(x[i], int(s), dims=-1)\n",
    "\n",
    "        # Channel dropout (simulate dead sensors) 20%\n",
    "        drop_mask = (torch.rand(B, C, 1, device=x.device) > 0.20).float()\n",
    "        x = x * drop_mask\n",
    "\n",
    "        # Gain jitter\n",
    "        gains = torch.empty(B, 1, 1, device=x.device).uniform_(0.9, 1.1)\n",
    "        x = x * gains\n",
    "\n",
    "        # Gaussian noise\n",
    "        sigma = self.hparams.opt.noise_std\n",
    "        if sigma and sigma > 0: x = x + torch.randn_like(x) * sigma\n",
    "        return x\n",
    "\n",
    "    # ---------- optim ----------\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.opt.lr, weight_decay=self.hparams.opt.weight_decay)\n",
    "        total_epochs = getattr(self.trainer, \"max_epochs\", 50) or 50\n",
    "        warm = max(0, int(self.hparams.opt.warmup_epochs))\n",
    "        if self.hparams.opt.cosine_after_warmup:\n",
    "            def lr_lambda(epoch):\n",
    "                if epoch < warm: return (epoch + 1) / max(1, warm)\n",
    "                t = (epoch - warm) / max(1, total_epochs - warm)\n",
    "                return 0.5 * (1 + math.cos(math.pi * t))\n",
    "            sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "            return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch}}\n",
    "        else:\n",
    "            sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=2)\n",
    "            return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"monitor\": \"val_auprc\"}}\n",
    "\n",
    "# ============================== Data ==============================\n",
    "\n",
    "from pnpl.datasets.libribrain2025.word_dataset import LibriBrainWord\n",
    "from pnpl.datasets.libribrain2025.constants import RUN_KEYS\n",
    "try:\n",
    "    from pnpl.datasets.libribrain2025.base import LibriBrainBase\n",
    "except Exception:\n",
    "    LibriBrainBase = None\n",
    "\n",
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"Oversample positives to reach target fraction per batch (sampling with replacement).\"\"\"\n",
    "    def __init__(self, pos_idx: List[int], neg_idx: List[int], batch_size: int, pos_fraction: float = 0.1):\n",
    "        assert 0.0 < pos_fraction < 1.0\n",
    "        assert len(pos_idx) > 0 and len(neg_idx) > 0, \"BalancedBatchSampler requires at least one pos and neg.\"\n",
    "        self.p_idx, self.n_idx = pos_idx, neg_idx\n",
    "        self.batch_size = batch_size\n",
    "        self.n_pos = max(1, int(round(batch_size * pos_fraction)))\n",
    "        self.n_neg = self.batch_size - self.n_pos\n",
    "        total = len(pos_idx) + len(neg_idx)\n",
    "        self._epoch_len = max(1, total // batch_size)\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        p_shuf = self.p_idx[:]; n_shuf = self.n_idx[:]\n",
    "        random.shuffle(p_shuf); random.shuffle(n_shuf)\n",
    "        pi = ni = 0\n",
    "        while True:\n",
    "            if pi + self.n_pos > len(p_shuf): random.shuffle(p_shuf); pi = 0\n",
    "            if ni + self.n_neg > len(n_shuf): random.shuffle(n_shuf); ni = 0\n",
    "            batch = p_shuf[pi:pi+self.n_pos] + n_shuf[ni:ni+self.n_neg]\n",
    "            pi += self.n_pos; ni += self.n_neg\n",
    "            random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self) -> int: return self._epoch_len\n",
    "\n",
    "class LibriBrainWordDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, data_path: str, tmin: float=-0.1, tmax: float=0.8, batch_size: int=256,\n",
    "        num_workers: int=4, pin_memory: bool=True, standardize_train: bool=True,\n",
    "        target_pos_fraction: float = 0.10,\n",
    "        val_run_override: Optional[Tuple[str,str,str,str]] = None,\n",
    "        test_run_override: Optional[Tuple[str,str,str,str]] = None,\n",
    "        apply_channel_mask: bool = False,\n",
    "        channel_indices: Optional[List[int]] = None,\n",
    "        use_balanced_sampling: bool = True,\n",
    "        train_fraction: float = 1.0,\n",
    "        fraction_seed: int = 1234,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path, self.tmin, self.tmax = data_path, tmin, tmax\n",
    "        self.batch_size, self.num_workers, self.pin_memory = batch_size, num_workers, pin_memory\n",
    "        self.standardize_train = standardize_train\n",
    "        self.target_pos_fraction = target_pos_fraction\n",
    "        self.val_run_override = val_run_override\n",
    "        self.test_run_override = test_run_override\n",
    "        self.apply_channel_mask = apply_channel_mask\n",
    "        self.channel_indices = channel_indices\n",
    "        self.use_balanced_sampling = use_balanced_sampling\n",
    "        self.train_fraction = float(train_fraction)\n",
    "        self.fraction_seed = int(fraction_seed)\n",
    "\n",
    "        self.val_run: Optional[Tuple[str,str,str,str]] = None\n",
    "        self.test_run: Optional[Tuple[str,str,str,str]] = None\n",
    "        self.train_ds: Optional[Dataset] = None\n",
    "        self.val_ds: Optional[Dataset] = None\n",
    "        self.test_ds: Optional[Dataset] = None\n",
    "        self._train_sampler: Optional[BalancedBatchSampler] = None\n",
    "\n",
    "        # effective (possibly sub-sampled) index lists\n",
    "        self._pos_idx: List[int] = []\n",
    "        self._neg_idx: List[int] = []\n",
    "        self._eff_pos_idx: List[int] = []\n",
    "        self._eff_neg_idx: List[int] = []\n",
    "        self._eff_total: int = 0\n",
    "\n",
    "    def _available_runs(self):\n",
    "        cands = [rk for rk in RUN_KEYS if rk[2].startswith('Sherlock')]\n",
    "        if LibriBrainBase is None: return cands\n",
    "        def _events_path(su, se, ta, ru):\n",
    "            return os.path.join(self.data_path, ta, \"derivatives\", \"events\",\n",
    "                                f\"sub-{su}_ses-{se}_task-{ta}_run-{ru}_events.tsv\")\n",
    "        def _h5_path(su, se, ta, ru):\n",
    "            return os.path.join(self.data_path, ta, \"derivatives\", \"serialised\",\n",
    "                                f\"sub-{su}_ses-{se}_task-{ta}_run-{ru}_proc-bads+headpos+sss+notch+bp+ds_meg.h5\")\n",
    "        avail=[]\n",
    "        for s,se,t,r in cands:\n",
    "            try:\n",
    "                LibriBrainBase.ensure_file_download(_events_path(s,se,t,r), data_path=self.data_path)\n",
    "                LibriBrainBase.ensure_file_download(_h5_path(s,se,t,r), data_path=self.data_path)\n",
    "                avail.append((s,se,t,r))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return avail or cands\n",
    "\n",
    "    def _hash_key(self, train_runs: List[Tuple[str,str,str,str]]) -> str:\n",
    "        m = hashlib.sha256()\n",
    "        payload = {\"tmin\": self.tmin, \"tmax\": self.tmax, \"runs\": sorted(list(map(lambda x: \"_\".join(x), train_runs)))}\n",
    "        m.update(json.dumps(payload, sort_keys=True).encode(\"utf-8\"))\n",
    "        return m.hexdigest()[:16]\n",
    "\n",
    "    def _cache_paths(self, key: str):\n",
    "        cache_dir = os.path.join(self.data_path, \"_indices\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        return os.path.join(cache_dir, f\"watson_{key}.pt\")\n",
    "\n",
    "    def _try_dataset_labels_fast(self, ds: Dataset) -> Optional[List[int]]:\n",
    "        for attr in (\"labels\", \"y\", \"targets\", \"_labels\", \"_y\", \"_targets\"):\n",
    "            if hasattr(ds, attr):\n",
    "                lab = getattr(ds, attr)\n",
    "                try:\n",
    "                    if torch.is_tensor(lab):\n",
    "                        return lab.view(-1).cpu().int().tolist()\n",
    "                    return list(map(int, list(lab)))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    def _build_pos_neg_indices(self, ds: Dataset, cache_file: str) -> Tuple[List[int], List[int]]:\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                obj = torch.load(cache_file, map_location=\"cpu\")\n",
    "                pos_idx = list(map(int, obj[\"pos_idx\"]))\n",
    "                neg_idx = list(map(int, obj[\"neg_idx\"]))\n",
    "                # validate against current dataset length\n",
    "                n = len(ds)\n",
    "                pos_idx = [i for i in pos_idx if 0 <= i < n]\n",
    "                neg_idx = [i for i in neg_idx if 0 <= i < n]\n",
    "                if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "                    raise ValueError(\"empty index lists after validation\")\n",
    "                print(f\"Index cache: loaded {len(pos_idx)} positives / {len(pos_idx)+len(neg_idx)} total.\")\n",
    "                return pos_idx, neg_idx\n",
    "            except Exception as e:\n",
    "                print(f\"Index cache invalid ({type(e).__name__}: {e}); rebuilding indices…\")\n",
    "\n",
    "        lbls = self._try_dataset_labels_fast(ds)\n",
    "        if lbls is not None:\n",
    "            pos_idx = [i for i, y in enumerate(lbls) if int(y) == 1]\n",
    "            neg_idx = [i for i, y in enumerate(lbls) if int(y) == 0]\n",
    "            print(f\"Index fast-path: found {len(pos_idx)} positives / {len(lbls)} total.\")\n",
    "            torch.save({\"pos_idx\": pos_idx, \"neg_idx\": neg_idx}, cache_file)\n",
    "            return pos_idx, neg_idx\n",
    "\n",
    "        def _scan(num_workers: int) -> Tuple[List[int], List[int]]:\n",
    "            print(f\"Scanning training labels (num_workers={num_workers})…\")\n",
    "            pos_idx, neg_idx = [], []\n",
    "            loader = DataLoader(\n",
    "                ds, batch_size=2048, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=False,\n",
    "                persistent_workers=(num_workers > 0),\n",
    "                prefetch_factor=2 if num_workers > 0 else None,\n",
    "                collate_fn=collate_label_only_xy,\n",
    "            )\n",
    "            idx = 0\n",
    "            for ys in loader:\n",
    "                for y in ys:\n",
    "                    (pos_idx if y == 1 else neg_idx).append(idx)\n",
    "                    idx += 1\n",
    "                if idx % 50000 == 0:\n",
    "                    print(f\"… scanned {idx} samples\")\n",
    "            return pos_idx, neg_idx\n",
    "\n",
    "        try: pos_idx, neg_idx = _scan(self.num_workers)\n",
    "        except Exception as e:\n",
    "            print(f\"[Label scan] parallel failed ({type(e).__name__}: {e}). Falling back to single-process.\")\n",
    "            pos_idx, neg_idx = _scan(0)\n",
    "\n",
    "        total = len(pos_idx) + len(neg_idx)\n",
    "        print(f\"Found {len(pos_idx)} positives / {total} total ({len(pos_idx)/max(1,total):.6f}).\")\n",
    "        torch.save({\"pos_idx\": pos_idx, \"neg_idx\": neg_idx}, cache_file)\n",
    "        return pos_idx, neg_idx\n",
    "\n",
    "    def _subsample_indices(self, pos_idx: List[int], neg_idx: List[int], frac: float, seed: int) -> Tuple[List[int], List[int]]:\n",
    "        frac = float(max(0.0001, min(1.0, frac)))\n",
    "        if frac >= 0.9999:  # full\n",
    "            return pos_idx[:], neg_idx[:]\n",
    "        rng = random.Random(seed)\n",
    "        k_pos = max(1, int(round(len(pos_idx) * frac)))\n",
    "        k_neg = max(1, int(round(len(neg_idx) * frac)))\n",
    "        pos_sub = rng.sample(pos_idx, k_pos) if k_pos < len(pos_idx) else pos_idx[:]\n",
    "        neg_sub = rng.sample(neg_idx, k_neg) if k_neg < len(neg_idx) else neg_idx[:]\n",
    "        return pos_sub, neg_sub\n",
    "\n",
    "    def _infer_window_seconds(self) -> float:\n",
    "        \"\"\"\n",
    "        Try to infer per-sample window seconds from dataset metadata;\n",
    "        fall back to (tmax - tmin).\n",
    "        \"\"\"\n",
    "        # Attempt via dataset attributes\n",
    "        for ds in (self.train_ds, self.val_ds, self.test_ds):\n",
    "            if ds is None: continue\n",
    "            for attr_pair in ((\"tmin\", \"tmax\"),):\n",
    "                if hasattr(ds, attr_pair[0]) and hasattr(ds, attr_pair[1]):\n",
    "                    try:\n",
    "                        return float(getattr(ds, \"tmax\") - getattr(ds, \"tmin\"))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            for attr in (\"window_seconds\", \"win_seconds\", \"segment_seconds\"):\n",
    "                if hasattr(ds, attr):\n",
    "                    try:\n",
    "                        return float(getattr(ds, attr))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            # sfreq heuristic\n",
    "            if hasattr(ds, \"sfreq\") or hasattr(ds, \"sample_rate\"):\n",
    "                try:\n",
    "                    x0, _ = ds[0]\n",
    "                    sr = float(getattr(ds, \"sfreq\", getattr(ds, \"sample_rate\", 250.0)))\n",
    "                    return float(x0.shape[-1]) / sr\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # fallback to config window\n",
    "        return float(self.tmax - self.tmin)\n",
    "\n",
    "    def setup(self, stage: Optional[str]=None):\n",
    "        all_runs = [rk for rk in self._available_runs()]\n",
    "\n",
    "        # Fixed splits\n",
    "        self.val_run  = self.val_run_override  or ('0','12','Sherlock4','1')\n",
    "        self.test_run = self.test_run_override or ('0','12','Sherlock5','1')\n",
    "\n",
    "        # exclude val/test run from training\n",
    "        train_runs = [rk for rk in all_runs if rk not in (self.val_run, self.test_run)]\n",
    "\n",
    "        # datasets\n",
    "        self.train_ds = LibriBrainWord(\n",
    "            self.data_path,\n",
    "            partition=\"train\",\n",
    "            # include_run_keys=train_runs,\n",
    "            # tmin=self.tmin,\n",
    "            # tmax=self.tmax,\n",
    "            keyword_detection=\"watson\",\n",
    "            preload_files=False, include_info=False,\n",
    "            standardize=self.standardize_train\n",
    "        )\n",
    "        self.val_ds = LibriBrainWord(\n",
    "            self.data_path,\n",
    "            partition=\"validation\",\n",
    "            # include_run_keys=[self.val_run],\n",
    "            # tmin=self.tmin,\n",
    "            # tmax=self.tmax,\n",
    "            keyword_detection=\"watson\",\n",
    "            preload_files=False,\n",
    "            include_info=False,\n",
    "            standardize=True,\n",
    "            channel_means=getattr(self.train_ds, \"channel_means\", None),\n",
    "            channel_stds=getattr(self.train_ds, \"channel_stds\", None),\n",
    "        )\n",
    "        self.test_ds = LibriBrainWord(\n",
    "            self.data_path,\n",
    "            partition=\"test\",\n",
    "            # include_run_keys=[self.test_run],\n",
    "            # tmin=self.tmin,\n",
    "            # tmax=self.tmax,\n",
    "            keyword_detection=\"watson\",\n",
    "            preload_files=False, include_info=False,\n",
    "            standardize=True,\n",
    "            channel_means=getattr(self.train_ds, \"channel_means\", None),\n",
    "            channel_stds=getattr(self.train_ds, \"channel_stds\", None),\n",
    "        )\n",
    "\n",
    "        # optional channel mask wrapping\n",
    "        if self.apply_channel_mask:\n",
    "            ch_idx = self.channel_indices if self.channel_indices is not None else SENSORS_SPEECH_MASK\n",
    "            self.train_ds = ChannelMaskedDataset(self.train_ds, channel_indices=ch_idx)\n",
    "            self.val_ds   = ChannelMaskedDataset(self.val_ds,   channel_indices=ch_idx)\n",
    "            self.test_ds  = ChannelMaskedDataset(self.test_ds,  channel_indices=ch_idx)\n",
    "\n",
    "        # derive in_channels\n",
    "        try:\n",
    "            x0, _ = self.train_ds[0]\n",
    "            self.num_channels = int(x0.shape[0])\n",
    "        except Exception:\n",
    "            self.num_channels = 306\n",
    "\n",
    "        key = self._hash_key(train_runs)\n",
    "        cache_file = self._cache_paths(key)\n",
    "        pos_idx, neg_idx = self._build_pos_neg_indices(self.train_ds, cache_file)\n",
    "        if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "            raise RuntimeError(\"Need both positive and negative samples in training set.\")\n",
    "        self._pos_idx = pos_idx; self._neg_idx = neg_idx\n",
    "\n",
    "        # ---- NEW: apply train_fraction sub-sampling (deterministic) ----\n",
    "        self._eff_pos_idx, self._eff_neg_idx = self._subsample_indices(\n",
    "            self._pos_idx, self._neg_idx, self.train_fraction, self.fraction_seed\n",
    "        )\n",
    "        self._eff_total = len(self._eff_pos_idx) + len(self._eff_neg_idx)\n",
    "        if self._eff_total == 0:\n",
    "            raise RuntimeError(\"train_fraction produced an empty effective train set.\")\n",
    "\n",
    "        # sampler\n",
    "        if self.use_balanced_sampling:\n",
    "            self._train_sampler = BalancedBatchSampler(\n",
    "                self._eff_pos_idx, self._eff_neg_idx,\n",
    "                batch_size=self.batch_size, pos_fraction=self.target_pos_fraction\n",
    "            )\n",
    "        else:\n",
    "            self._train_sampler = None\n",
    "\n",
    "        # handy stats\n",
    "        self.window_seconds = self._infer_window_seconds()\n",
    "\n",
    "    def estimate_label_stats(self, sample: int = 200_000) -> Tuple[int,int]:\n",
    "        if hasattr(self, \"_pos_idx\") and hasattr(self, \"_neg_idx\") and (self._pos_idx or self._neg_idx):\n",
    "            pos = len(self._pos_idx)\n",
    "            total = len(self._pos_idx) + len(self._neg_idx)\n",
    "            if sample < total and total > 0:\n",
    "                frac = sample / total\n",
    "                pos = max(1, int(round(pos * frac))); total = sample\n",
    "            return pos, total\n",
    "        n = len(self.train_ds)\n",
    "        k = min(20_000, n)\n",
    "        idxs = random.sample(range(n), k=k)\n",
    "        pos = sum(int(self.train_ds[i][1]) for i in idxs)\n",
    "        return pos, k\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self._train_sampler is not None:\n",
    "            return DataLoader(self.train_ds, batch_sampler=self._train_sampler,\n",
    "                              num_workers=self.num_workers, pin_memory=self.pin_memory,\n",
    "                              persistent_workers=(self.num_workers > 0),\n",
    "                              prefetch_factor=2 if self.num_workers > 0 else None)\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, pin_memory=self.pin_memory,\n",
    "                          persistent_workers=(self.num_workers > 0),\n",
    "                          prefetch_factor=2 if self.num_workers > 0 else None)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=self.pin_memory,\n",
    "                          persistent_workers=(self.num_workers > 0),\n",
    "                          prefetch_factor=2 if self.num_workers > 0 else None)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=self.pin_memory,\n",
    "                          persistent_workers=(self.num_workers > 0),\n",
    "                          prefetch_factor=2 if self.num_workers > 0 else None)\n",
    "\n",
    "# ============================== Utility: run dirs & metadata ==============================\n",
    "\n",
    "def get_git_commit() -> str:\n",
    "    try:\n",
    "        return subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).decode().strip()\n",
    "    except Exception:\n",
    "        return \"n/a\"\n",
    "\n",
    "def system_info() -> dict:\n",
    "    return {\n",
    "        \"python\": platform.python_version(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"torch\": torch.__version__,\n",
    "        \"pytorch_lightning\": pl.__version__,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"cuda_device_count\": torch.cuda.device_count(),\n",
    "        \"cuda_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        \"git_commit\": get_git_commit(),\n",
    "        \"time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "\n",
    "# ============================== Train / Test (SCALING SWEEP) ==============================\n",
    "\n",
    "def main():\n",
    "    # ---- core hyper-params (unchanged defaults) ----\n",
    "    data_path   = os.getenv(\"DATA_PATH\", \"dataset\")\n",
    "    tmin, tmax  = -0.05, 1\n",
    "    epochs      = 10\n",
    "    batch_size  = 256\n",
    "    lr          = 2.5e-4\n",
    "    num_workers = 4\n",
    "    precision   = \"bf16-mixed\"\n",
    "    devices     = 1\n",
    "\n",
    "    # Sampling & priors\n",
    "    target_pos_fraction = 0.10   # balanced batch pos-rate\n",
    "    VAL_RUN  = ('0','12','Sherlock4','1')   # fixed for reproducibility\n",
    "    TEST_RUN = ('0','12','Sherlock5','1')\n",
    "\n",
    "    # Fractions to sweep\n",
    "    FRACTIONS = [0.05, 0.10, 0.20, 0.40, 0.60, 0.80, 1.00]\n",
    "\n",
    "    # ---- deterministic-ish setup ----\n",
    "    pl.seed_everything(42, workers=True)\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # ---- run root ----\n",
    "    run_root = Path(os.getenv(\"OUT_DIR\", \"runs\"))\n",
    "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    scaling_summary = []\n",
    "\n",
    "    for frac in FRACTIONS:\n",
    "        tag = f\"frac{int(round(frac*100)):02d}\"\n",
    "        run_dir = run_root / f\"watson_meg_{ts}_{tag}\"\n",
    "        ckpt_dir = run_dir / \"checkpoints\"\n",
    "        art_dir  = run_dir / \"artifacts\"\n",
    "        for d in (ckpt_dir, art_dir):\n",
    "            d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ---- data (per fraction) ----\n",
    "        dm = LibriBrainWordDataModule(\n",
    "            data_path, tmin, tmax, batch_size, num_workers,\n",
    "            standardize_train=True, target_pos_fraction=target_pos_fraction,\n",
    "            val_run_override=VAL_RUN, test_run_override=TEST_RUN,\n",
    "            apply_channel_mask=False, channel_indices=None,\n",
    "            use_balanced_sampling=True,\n",
    "            train_fraction=frac, fraction_seed=42,\n",
    "        )\n",
    "        dm.setup()\n",
    "\n",
    "        # Estimate base prevalence from TRAIN universe (not balanced, full train universe)\n",
    "        pos, total = dm.estimate_label_stats(sample=200_000)\n",
    "        base_rate = max(1, pos) / max(2, total)  # guard tiny denominators\n",
    "        print(f\"[Label stats] pos={pos} total={total}  π={base_rate:.6f}  sampler_pos={target_pos_fraction:.2f}  (train_fraction={frac:.2f})\")\n",
    "\n",
    "        # ---- compute hours accounting ----\n",
    "        # batches/epoch: from sampler when used, else ceil(len(train)/bs)\n",
    "        if dm._train_sampler is not None:\n",
    "            batches_per_epoch = len(dm._train_sampler)\n",
    "        else:\n",
    "            batches_per_epoch = math.ceil(len(dm.train_ds) / dm.batch_size)\n",
    "        samples_per_epoch = batches_per_epoch * dm.batch_size\n",
    "        window_seconds = getattr(dm, \"window_seconds\", float(tmax - tmin))\n",
    "        hours_per_epoch = (samples_per_epoch * window_seconds) / 3600.0\n",
    "        hours_total = hours_per_epoch * epochs\n",
    "\n",
    "        # ---- loggers (per fraction) ----\n",
    "        csv_logger = CSVLogger(save_dir=str(run_dir), name=\"pl_logs\", version=\"\")\n",
    "        neptune_logger = make_neptune_logger(run_name=f\"watson-meg-generalization-{tag}\")\n",
    "        loggers = [csv_logger] + ([neptune_logger] if neptune_logger else [])\n",
    "\n",
    "        # Neptune hyperparams\n",
    "        if neptune_logger:\n",
    "            neptune_logger.log_hyperparams({\n",
    "                \"data_path\": data_path, \"tmin\": tmin, \"tmax\": tmax,\n",
    "                \"batch_size\": batch_size, \"lr\": lr, \"precision\": precision,\n",
    "                \"base_rate\": base_rate, \"target_pos_fraction\": target_pos_fraction,\n",
    "                \"loss\": \"focal(alpha=0.95,gamma=2.0)+pairwise(lambda=1.0)\",\n",
    "                \"pooling\": \"temporal_attention\",\n",
    "                \"schedule\": \"warmup+cosine\",\n",
    "                \"val_run\": \"_\".join(VAL_RUN), \"test_run\": \"_\".join(TEST_RUN),\n",
    "                \"norm\": \"GroupNorm\", \"ema\": \"0.999\",\n",
    "                \"train_fraction\": frac,\n",
    "                \"window_seconds\": window_seconds,\n",
    "                \"batches_per_epoch\": batches_per_epoch,\n",
    "                \"samples_per_epoch\": samples_per_epoch,\n",
    "                \"hours_per_epoch\": hours_per_epoch,\n",
    "                \"hours_total\": hours_total,\n",
    "                \"prior_bias\": float(math.log((base_rate/(1-base_rate)) / (target_pos_fraction/(1-target_pos_fraction)))),\n",
    "            })\n",
    "\n",
    "        # persist run config & system info locally\n",
    "        (run_dir / \"config.json\").write_text(json.dumps({\n",
    "            \"data_path\": data_path, \"tmin\": tmin, \"tmax\": tmax,\n",
    "            \"batch_size\": batch_size, \"lr\": lr, \"precision\": precision,\n",
    "            \"epochs\": epochs, \"devices\": devices,\n",
    "            \"val_run\": VAL_RUN, \"test_run\": TEST_RUN,\n",
    "            \"target_pos_fraction\": target_pos_fraction,\n",
    "            \"train_fraction\": frac,\n",
    "            \"window_seconds\": window_seconds,\n",
    "            \"batches_per_epoch\": batches_per_epoch,\n",
    "            \"samples_per_epoch\": samples_per_epoch,\n",
    "            \"hours_per_epoch\": hours_per_epoch,\n",
    "            \"hours_total\": hours_total,\n",
    "        }, indent=2))\n",
    "        (run_dir / \"system_info.json\").write_text(json.dumps(system_info(), indent=2))\n",
    "\n",
    "        print(f\"[Scaling] {tag} -> batches/epoch={batches_per_epoch}  samples/epoch={samples_per_epoch}  \"\n",
    "              f\"window_s={window_seconds:.3f}  hours/epoch={hours_per_epoch:.3f}  hours_total={hours_total:.3f}\")\n",
    "\n",
    "        # ---- model ----\n",
    "        model = WatsonKeywordPL(\n",
    "            in_channels=getattr(dm, \"num_channels\", 306),\n",
    "            opt=OptimConfig(lr=lr, weight_decay=1e-4, warmup_epochs=2, cosine_after_warmup=True, noise_std=0.01),\n",
    "            loss_mode=\"focal_pairwise\", pairwise_lambda=1.0,\n",
    "            pi_train=target_pos_fraction, pi_target=base_rate\n",
    "        )\n",
    "\n",
    "        # ---- callbacks ----\n",
    "        ckpt_cb = ModelCheckpoint(monitor=\"val_auprc\", mode=\"max\", save_top_k=1,\n",
    "                                  filename=f\"best-val-auprc-ema-calib-{tag}\", dirpath=str(ckpt_dir))\n",
    "        callbacks = [\n",
    "            ckpt_cb,\n",
    "            EarlyStopping(monitor=\"val_auprc\", mode=\"max\", patience=3, min_delta=1e-3),\n",
    "            LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "        ]\n",
    "\n",
    "        # ---- trainer ----\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=epochs,\n",
    "            precision=precision,\n",
    "            devices=devices,\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            callbacks=callbacks,\n",
    "            logger=loggers,\n",
    "            log_every_n_steps=25,\n",
    "            gradient_clip_val=1.0,\n",
    "            detect_anomaly=False,\n",
    "            default_root_dir=str(run_dir),\n",
    "        )\n",
    "\n",
    "        # ---- train ----\n",
    "        t0 = time.time()\n",
    "        trainer.fit(model, datamodule=dm)\n",
    "        fit_secs = time.time() - t0\n",
    "\n",
    "        # save \"last\" checkpoint (state after training) locally\n",
    "        last_ckpt_path = ckpt_dir / f\"last_{tag}.ckpt\"\n",
    "        trainer.save_checkpoint(str(last_ckpt_path))\n",
    "\n",
    "        # optionally upload best ckpt to Neptune (still saved locally regardless)\n",
    "        if neptune_logger and ckpt_cb.best_model_path:\n",
    "            try:\n",
    "                # lightning neptune logger API varies; try both keys\n",
    "                exp = getattr(neptune_logger, \"experiment\", None) or getattr(neptune_logger, \"run\", None)\n",
    "                if exp is not None:\n",
    "                    exp[\"artifacts/checkpoints/best\"].upload(ckpt_cb.best_model_path)\n",
    "                print(f\"Neptune: uploaded best checkpoint -> {ckpt_cb.best_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Neptune: failed to upload checkpoint: {e}\")\n",
    "\n",
    "        # ---- test ----\n",
    "        t1 = time.time()\n",
    "        trainer.test(model, datamodule=dm)\n",
    "        test_secs = time.time() - t1\n",
    "\n",
    "        # ---- export test probabilities CSV ----\n",
    "        probs = getattr(model, \"last_test_probs\", None)\n",
    "        labels= getattr(model, \"last_test_labels\", None)\n",
    "        csv_path = art_dir / f\"test_probabilities_{tag}.csv\"\n",
    "        if probs is not None and labels is not None and probs.numel() == labels.numel():\n",
    "            import csv\n",
    "            with csv_path.open(\"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\"index\", \"prob\", \"label\"])  # prob is post-calibration (prior+temperature)\n",
    "                for i, (p, y) in enumerate(zip(probs.tolist(), labels.tolist())):\n",
    "                    w.writerow([i, float(p), int(y)])\n",
    "            print(f\"Saved test probabilities -> {csv_path}\")\n",
    "        else:\n",
    "            print(\"[WARN] Test probabilities unavailable; CSV not written.\")\n",
    "\n",
    "        # ---- write a compact metrics summary ----\n",
    "        summary = {\n",
    "            \"train_fraction\": frac,\n",
    "            \"fit_seconds\": round(fit_secs, 2),\n",
    "            \"test_seconds\": round(test_secs, 2),\n",
    "            \"best_ckpt\": ckpt_cb.best_model_path,\n",
    "            \"last_ckpt\": str(last_ckpt_path),\n",
    "            \"window_seconds\": window_seconds,\n",
    "            \"batches_per_epoch\": batches_per_epoch,\n",
    "            \"samples_per_epoch\": samples_per_epoch,\n",
    "            \"hours_per_epoch\": hours_per_epoch,\n",
    "            \"hours_total\": hours_total,\n",
    "        }\n",
    "        try:\n",
    "            final_metrics = {k: (v.item() if hasattr(v, 'item') else v)\n",
    "                             for k, v in trainer.callback_metrics.items()}\n",
    "            summary[\"final_metrics\"] = final_metrics\n",
    "        except Exception:\n",
    "            pass\n",
    "        (run_dir / \"metrics_summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "        print(\"\\n=== RUN ARTIFACTS ===\")\n",
    "        print(f\"Run dir: {run_dir}\")\n",
    "        print(f\" - Best checkpoint: {ckpt_cb.best_model_path or 'n/a'}\")\n",
    "        print(f\" - Last checkpoint: {last_ckpt_path}\")\n",
    "        print(f\" - CSV metrics (per-epoch): {Path(csv_logger.log_dir) / 'metrics.csv'}\")\n",
    "        print(f\" - Test probabilities: {csv_path if csv_path.exists() else 'n/a'}\\n\")\n",
    "\n",
    "        # Collect for a global summary\n",
    "        auprc = summary.get(\"final_metrics\", {}).get(\"test_auprc\", None)\n",
    "        auroc = summary.get(\"final_metrics\", {}).get(\"test_auroc\", None)\n",
    "        scaling_summary.append({\n",
    "            \"fraction\": frac,\n",
    "            \"hours_total\": hours_total,\n",
    "            \"hours_per_epoch\": hours_per_epoch,\n",
    "            \"batches_per_epoch\": batches_per_epoch,\n",
    "            \"samples_per_epoch\": samples_per_epoch,\n",
    "            \"test_auprc\": float(auprc) if auprc is not None else None,\n",
    "            \"test_auroc\": float(auroc) if auroc is not None else None,\n",
    "            \"run_dir\": str(run_dir),\n",
    "        })\n",
    "\n",
    "        # free memory between runs\n",
    "        del trainer, model, dm\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # write a top-level sweep summary next to the timestamped root of this sweep\n",
    "    sweep_dir = run_root / f\"watson_meg_{ts}_SWEEP_SUMMARY\"\n",
    "    sweep_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (sweep_dir / \"scaling_summary.json\").write_text(json.dumps(scaling_summary, indent=2))\n",
    "    # also CSV\n",
    "    try:\n",
    "        import csv\n",
    "        with (sweep_dir / \"scaling_summary.csv\").open(\"w\", newline=\"\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=list(scaling_summary[0].keys()))\n",
    "            w.writeheader(); w.writerows(scaling_summary)\n",
    "        print(f\"[Sweep] Wrote summary -> {sweep_dir / 'scaling_summary.csv'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Sweep] Could not write CSV summary: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
