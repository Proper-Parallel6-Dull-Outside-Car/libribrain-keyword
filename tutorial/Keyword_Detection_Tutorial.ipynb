{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MK9eTbr351A"
      },
      "source": [
        "![title](https://proper-parallel6-dull-outside-car.github.io/libribrain-keyword/tutorial/img/task-graphic.png)\n",
        "\n",
        "# üïµÔ∏è Tutorial: Neural Keyword Detection on Sherlock Holmes Stories (LibriBrain)\n",
        "\n",
        "> ‚ö†Ô∏è In Google Colab, make sure to switch to a `GPU` runtime via `Runtime -> Change runtime type`.\n",
        "\n",
        "This tutorial covers keyword detection from MEG data using the LibriBrain dataset, which covers **over 50 hours** of MEG data of a single participant listening to Sherlock Holmes audiobooks. It is inspired by (and, at times, re-uses small code snippets from) the existing LibriBrain tutorials found on the [official website](https://neural-processing-lab.github.io/2025-libribrain-competition/participate/).\n",
        "\n",
        "## Contents\n",
        "Here's what we'll do in the next hour:\n",
        "1. **Setup** ‚Äî install dependencies and set up configuration\n",
        "2. **Dataset Structure** ‚Äî have a look at the pre-processed MEG recordings and event files\n",
        "3. **Problem Formulation** ‚Äî understand the problem and associated challenges\n",
        "4. **Model Architecture** ‚Äî look at one possible solution approach,...\n",
        "5. **Training Strategy** ‚Äî ... and understand how to get the most out of it\n",
        "6. **Evaluation** ‚Äî then, evaluate on standard metrics - AUPRC and false alarms/hour at fixed recall\n",
        "\n",
        "Let's get started! üîé\n",
        "\n",
        "## I. Setup\n",
        "Run the cell below as-is. It will install all required dependencies and prepare the environment. Notice the lines at the bottom of the cell? Those install a modified variant of the `pnpl` library we have customized for neural keyword detection. The `pnpl` library makes our life much easier here - it automatically downloads the dataset into the correct folder structure, provides dataset classes and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvYY73Ld351E",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q mne_bids lightning torchmetrics scikit-learn plotly ipywidgets neptune\n",
        "\n",
        "BASE_PATH = \"./kws\"\n",
        "try:\n",
        "    import google.colab  # Colab runtime\n",
        "    IN_COLAB = True\n",
        "    BASE_PATH = \"/content\"\n",
        "    print(\"Running in Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running in Colab\")\n",
        "\n",
        "# Anonymous GitHub repo will be replaced after review process has concluded.\n",
        "!git clone https://github.com/Proper-Parallel6-Dull-Outside-Car/libribrain-keyword.git\n",
        "!cd libribrain-keyword/modified-pnpl/pnpl && pip install -q ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV-WKwer351G",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Configuration Parameters.\n",
        "\n",
        "Each parameter below affects the keyword detection pipeline we'll build.\n",
        "For now, just run this cell as-is - you can come back in the future and play around with it!\n",
        "\"\"\"\n",
        "\n",
        "CONFIG = {\n",
        "    # === DATASET CONFIGURATION ===\n",
        "    \"data_path\": f\"{BASE_PATH}/data/\",\n",
        "\n",
        "    # Target Keyword Selection\n",
        "    \"keyword\": \"watson\",              # Choose your target word (case-insensitive)\n",
        "                                      # Popular options: \"watson\", \"holmes\", \"sherlock\", \"the\"\n",
        "                                      # Tip: Common words have more examples but lower precision\n",
        "\n",
        "    # Word Filtering\n",
        "    \"min_word_len\": 3,                # Skip very short words (reduce noise)\n",
        "    \"max_word_len\": None,             # Optional: limit to shorter words for faster processing\n",
        "\n",
        "    # Temporal Window Configuration\n",
        "    \"tmin\": None,                     # Auto-computed: 0 - negative_buffer\n",
        "    \"tmax\": None,                     # Auto-computed: keyword_duration + positive_buffer\n",
        "    \"negative_buffer\": 0.10,          # Pre-onset context (100ms captures anticipatory activity)\n",
        "    \"positive_buffer\": 0.30,          # Post-offset context (300ms captures completion responses)\n",
        "                                      # Trade-off: More context vs. computational cost\n",
        "\n",
        "    # Signal Preprocessing\n",
        "    \"standardize\": True,              # Z-score normalization per channel (recommended)\n",
        "    \"clipping_boundary\": 10.0,        # Outlier clipping (prevents extreme values)\n",
        "\n",
        "    # === DATA SPLITS ===\n",
        "    # Strategy: Use minimal subset for fast experimentation\n",
        "    # For full performance, expand to more sessions/books\n",
        "    \"train_run_keys\": [(\"0\",\"1\",\"Sherlock1\",\"1\"), (\"0\",\"3\",\"Sherlock1\",\"1\"), (\"0\",\"5\",\"Sherlock1\",\"1\"), (\"0\",\"12\",\"Sherlock4\",\"1\"), (\"0\",\"12\",\"Sherlock6\",\"1\")],\n",
        "    \"val_partition\": \"validation\",     # Automatic keyword-aware validation selection\n",
        "    \"test_partition\": \"test\",         # Automatic keyword-aware test selection\n",
        "\n",
        "    # === DATALOADER SETTINGS ===\n",
        "    \"batch_size\": 64,                 # Balance: memory usage vs. gradient stability\n",
        "    \"num_workers\": 2 if IN_COLAB else 0,  # Parallel data loading (adjust for your system)\n",
        "\n",
        "    # === MODEL ARCHITECTURE ===\n",
        "    \"model_dim\": 128,                 # Hidden dimension size (trade-off: capacity vs. speed)\n",
        "    \"dropout\": 0.4,                   # Regularization strength (combat overfitting)\n",
        "    \"lstm_layers\": 2,                 # Temporal processing depth\n",
        "    \"bi_directional\": False,          # Bidirectional LSTM (doubles parameters)\n",
        "\n",
        "    # === OPTIMIZATION ===\n",
        "    \"learning_rate\": 1e-3,            # Step size (too high: instability, too low: slow convergence)\n",
        "    \"weight_decay\": 0.01,             # L2 regularization (prevent overfitting)\n",
        "    \"smoothing\": 0.1,                 # Label smoothing (makes model less confident)\n",
        "\n",
        "    # === TRAINING CONTROL ===\n",
        "    \"max_epochs\": 8,                  # Maximum training epochs\n",
        "    \"early_patience\": 6,              # Stop if validation doesn't improve\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded! Key settings:\")\n",
        "print(f\"üéØ Target keyword: '{CONFIG['keyword']}'\")\n",
        "print(f\"‚è±Ô∏è  Window: {CONFIG['negative_buffer']}s before ‚Üí {CONFIG['positive_buffer']}s after onset\")\n",
        "print(f\"üß† Model dimension: {CONFIG['model_dim']}\")\n",
        "print(f\"üìä Batch size: {CONFIG['batch_size']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaktXchP351G"
      },
      "source": [
        "## II. Dataset Structure\n",
        "\n",
        "Next, let's explore the dataset! LibriBrain uses two complementary file types that work together to provide both neural signals and their annotations, which we will use as labels:\n",
        "\n",
        "**üß† 1. HDF5 Files (.h5)**: The Neural Signal Container\n",
        "- **data**: MEG sensor readings (306 channels √ó timesteps)\n",
        "- **times**: Precise timestamps for each sample (250Hz sampling rate)\n",
        "- **Preprocessing**: Signals are cleaned, filtered, and standardised\n",
        "- **Size**: Large files (~100MB+) containing continuous recordings\n",
        "\n",
        "**üìù 2. Event Files (.tsv)**: The Annotation Layer  \n",
        "- **Timing**: Word and phoneme onset times aligned to MEG\n",
        "- **Labels**: Text content ('segment' column) for each time window\n",
        "- **Categories**: 'word', 'phoneme', and 'silence' event types\n",
        "- **Precision**: Millisecond-accurate alignment with neural data\n",
        "\n",
        "Naming and folder structure is based on the [BIDS](https://bids-specification.readthedocs.io/en/stable/) specification:\n",
        "- **Subject**: Single participant (always 'sub-0')\n",
        "- **Sessions**: Book chapters (1-12+ per story)\n",
        "- **Tasks**: Different Books (Sherlock1, Sherlock2,... up to Sherlock7)\n",
        "- **Runs**: Recording sessions per task\n",
        "- **Preprocessing**: The HDF5 file will also contain a \"preprocessing string\" at the end, indicating what processing has been done on the raw data.\n",
        "\n",
        "That means that you may find the two files\n",
        "- sub-0_ses-10_task-Sherlock1_run-1_proc-bads+headpos+sss+notch+bp+ds_meg.h5\n",
        "- sub-0_ses-10_task-Sherlock1_run-1_events.tsv\n",
        "\n",
        "in folders 'Sherlock1/derivatives/serialised/' and 'Sherlock1/derivatives/events/' respectively. Those two together represent a single session.\n",
        "\n",
        "#### Let's explore\n",
        "Below, we will examine these two example files. Here are some things to pay attention to:\n",
        "\n",
        "For the `pnpl` package:\n",
        "- How it automatically downloads both data and label given a single \"run key\"\n",
        "\n",
        "For the H5 file:\n",
        "- Matrix shape: 306 sensors √ó time samples\n",
        "- Sampling rate: 250Hz (4ms resolution)\n",
        "- Time-locked: Besides the raw data (as a `data` key), the file contains a `times` key that functions as a timestamp. It increases with 4ms per sample.\n",
        "\n",
        "For the events file:\n",
        "- Word-level timing in the 'segment' column\n",
        "- Different event types: word vs. phoneme vs. silence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrTb8u5b351H",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os, h5py, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from pnpl.datasets import LibriBrainWord\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Loading a single session to force download\n",
        "example_data = LibriBrainWord(\n",
        "  data_path=CONFIG['data_path'],\n",
        "  include_run_keys = [(\"0\",\"1\",\"Sherlock1\",\"1\")]\n",
        ")\n",
        "# Conditionally set num_workers to avoid multiprocessing issues (try increasing if performance is problematic)\n",
        "example_loader = DataLoader(example_data, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'])\n",
        "\n",
        "\n",
        "hdf5_file_path = f\"{CONFIG['data_path']}/Sherlock1/derivatives/serialised/sub-0_ses-1_task-Sherlock1_run-1_proc-bads+headpos+sss+notch+bp+ds_meg.h5\"\n",
        "events_path = f\"{CONFIG['data_path']}/Sherlock1/derivatives/events/sub-0_ses-1_task-Sherlock1_run-1_events.tsv\"\n",
        "\n",
        "# Quick look at H5 files (MEG data)\n",
        "with h5py.File(hdf5_file_path, 'r') as f:\n",
        "    data = f[\"data\"]\n",
        "    times = f[\"times\"]\n",
        "    sfreq = f.attrs[\"sample_frequency\"]\n",
        "    print(\"HDF5 datasets:\", list(f.keys()))\n",
        "    print(\"data shape (channels x time):\", data.shape)\n",
        "    print(\"sfreq:\", sfreq)\n",
        "    print(\"times[0:5] (s):\", times[:5])\n",
        "\n",
        "# Quick look at events (labels)\n",
        "df = pd.read_csv(events_path, sep='\\t')\n",
        "print(\"\\nEvents columns:\", list(df.columns))\n",
        "print(\"First rows:\\n\", df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVArMOcd351I"
      },
      "source": [
        "## III. Task Overview and Configuration\n",
        "\n",
        "Now that we understand our data, let's talk about the task. We'll try to detect a target keyword in continuous MEG using short windows around word onsets. Fundamentally, that is an _event-referenced binary classification problem_ - we're given a sample (`T` timepoints of MEG data across `306` channels) and are trying to predict a value between 0 and 1 representing the probability that our keyword occurs during that timeframe.\n",
        "\n",
        "> **How long is `T`?**\n",
        ">\n",
        "> The length of each sample equals the longest instance of the chosen keyword (meaning for a 1s word, each sample will be 250Hz * 1s = 250 samples). Optionally, you can provide a `negative_buffer` or `positive_buffer` (so the sample starts before/extends beyond the keyword duration.\n",
        "\n",
        "**There are two primary challenges making our lives harder:**\n",
        "\n",
        "### 1. Class Imbalance\n",
        "Unfortunately, there are _a lot_ of different words in the dataset. Even the most common word, \"the\" makes up just 5.5% of all words. Given that the distribution within the dataset is [Zipfian](https://en.wikipedia.org/wiki/Zipf%27s_law), most words are a lot rarer than that:\n",
        "\n",
        "![Word frequency chart](https://proper-parallel6-dull-outside-car.github.io/libribrain-keyword/tutorial/img/word_frequency_chart.png)\n",
        "The keyword we'll be looking at today is **\"Watson\"**, which appears just **608 times** in the entire dataset (**0.1189%**).\n",
        "\n",
        "### 2. Signal-to-Noise Ratio\n",
        "Compared to traditional audio, or even to invasive brain decoding, non-invasive brain data is _a lot_ more noisy. While we do get nice data from 306 [MEG](https://en.wikipedia.org/wiki/Magnetoencephalography) channels, all of these sensors were placed _outside_ the participant's head. Have a look at the visualisation below (which is adapted from the [2025 LibriBrain Competition Speech Detection Tutorial](https://neural-processing-lab.github.io/2025-libribrain-competition/links/speech-colab)). It's impossible to tell with the naked eye when _speech_ starts - let alone tell apart specific words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h_KMra0351I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_meg_and_labels(hdf5_file_path, tsv_file_path, start_time, end_time, title=None):\n",
        "    # ---- Load MEG (data: channels x samples; times: seconds) ----\n",
        "    with h5py.File(hdf5_file_path, \"r\") as f:\n",
        "        meg = f[\"data\"][:]    # (C, T)\n",
        "        times = f[\"times\"][:] # (T,)\n",
        "\n",
        "    if times.size < 2:\n",
        "        raise ValueError(\"Not enough time points in 'times' to determine a window.\")\n",
        "\n",
        "    # clamp window to available time range\n",
        "    t0 = max(float(times[0]), float(start_time))\n",
        "    t1 = min(float(times[-1]), float(end_time))\n",
        "    if not (t0 < t1):\n",
        "        raise ValueError(f\"Empty window after clamping: [{t0}, {t1}]\")\n",
        "\n",
        "    # slice by time using searchsorted (avoids sampling-frequency math)\n",
        "    i0 = int(np.searchsorted(times, t0, side=\"left\"))\n",
        "    i1 = int(np.searchsorted(times, t1, side=\"right\"))\n",
        "    seg = meg[:, i0:i1]\n",
        "    tseg = times[i0:i1]\n",
        "\n",
        "    # ---- Load and prep TSV ----\n",
        "    tsv = pd.read_csv(tsv_file_path, sep=\"\\t\")\n",
        "    # keep only rows with usable timing + kind\n",
        "    tsv = tsv.dropna(subset=[\"timemeg\", \"kind\"]).copy()\n",
        "    tsv[\"timemeg\"] = tsv[\"timemeg\"].astype(float)\n",
        "\n",
        "    # rows inside the window\n",
        "    win = tsv[(tsv[\"timemeg\"] >= t0) & (tsv[\"timemeg\"] <= t1)].copy()\n",
        "\n",
        "    # determine label at t0 from the last event before the window\n",
        "    prev = tsv[tsv[\"timemeg\"] < t0].tail(1)\n",
        "    start_label = int(prev[\"kind\"].isin([\"word\", \"phoneme\"]).iloc[0]) if not prev.empty else 0\n",
        "    end_label = start_label if win.empty else int(win[\"kind\"].iloc[-1] in (\"word\", \"phoneme\"))\n",
        "\n",
        "    # build a simple step series (0 = silence, 1 = speech event)\n",
        "    step_times = np.concatenate(([t0], win[\"timemeg\"].to_numpy(), [t1]))\n",
        "    step_vals = np.concatenate(([start_label],\n",
        "                                win[\"kind\"].isin([\"word\", \"phoneme\"]).astype(int).to_numpy(),\n",
        "                                [end_label]))\n",
        "\n",
        "    # ---- Plot ----\n",
        "    fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(14, 8),\n",
        "                                   gridspec_kw={\"height_ratios\": [2, 1]},\n",
        "                                   sharex=True)\n",
        "\n",
        "    ax0.plot(tseg, seg.T, alpha=0.5)\n",
        "    ax0.set_title(title or f\"MEG {t0:.2f}s‚Äì{t1:.2f}s\")\n",
        "    ax0.set_ylabel(\"Amplitude\")\n",
        "    ax0.grid(True, alpha=0.3)\n",
        "\n",
        "    ax1.plot(step_times, step_vals, drawstyle=\"steps-post\", linewidth=2)\n",
        "    # annotate word onsets only (kept simple)\n",
        "    for _, r in win.iterrows():\n",
        "        if r[\"kind\"] == \"word\":\n",
        "            ax1.text(float(r[\"timemeg\"]), 1.22, str(r.get(\"segment\", \"\")),\n",
        "                     fontsize=9, ha=\"center\", va=\"bottom\", rotation=0)\n",
        "\n",
        "    ax1.set_ylim(-0.2, 1.5)\n",
        "    ax1.set_xlim(t0, t1)\n",
        "    ax1.set_ylabel(\"Speech / Silence\")\n",
        "    ax1.set_xlabel(\"Time (s)\")\n",
        "    ax1.set_title(\"Labels\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "tsv_file_path  = f\"{CONFIG['data_path']}/Sherlock1/derivatives/events/sub-0_ses-1_task-Sherlock1_run-1_events.tsv\"\n",
        "hdf5_file_path = f\"{CONFIG['data_path']}/Sherlock1/derivatives/serialised/sub-0_ses-1_task-Sherlock1_run-1_proc-bads+headpos+sss+notch+bp+ds_meg.h5\"\n",
        "plot_meg_and_labels(hdf5_file_path, tsv_file_path, start_time=55, end_time=56,\n",
        "                    title=\"Transition silence ‚Üí speech\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VRkumsx351I"
      },
      "source": [
        "## IV. Problem Formulation and Challenges\n",
        "\n",
        "Now that we understand the challenges we're facing, let's start building a solution!\n",
        "\n",
        "### Step 1: Training data\n",
        "So far, we've only played around with a single session. The first step, then, is to acquire more data. As we may be somewhat compute-limited by the constraints of Google Colab, we might not want to work with _all 52 hours_ of the dataset. In the default configuration, we have pre-defined some sessions as a `train` set (see `train_run_keys` in your config above). The `LibriBrainWords` dataset class chooses the `validation` and `test` sets automatically using either the LibriBrain default split or, if the chosen keyword does not appear in them, the highest-prevalence session in the dataset. Our default keyword `Watson`, does not appear in the default split - that is why you can see the cell download all available label files. Due to this dynamic logic, we need the additional code below to check that we are not accidentally training on a session that was dynamically picked as validation/test set. Execute the cell below to download the data and set up the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GCq1nUs351I",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Build datasets and loaders (uses pnpl LibriBrainWord)\n",
        "from pnpl.datasets.libribrain2025.word_dataset import LibriBrainWord\n",
        "from pnpl.datasets.libribrain2025.base import LibriBrainBase\n",
        "from pnpl.datasets.libribrain2025.constants import RUN_KEYS, VALIDATION_RUN_KEYS, TEST_RUN_KEYS\n",
        "from torch.utils.data import DataLoader\n",
        "import os, pandas as pd\n",
        "\n",
        "# Train subset: user-chosen specific sessions to keep runtime light\n",
        "train_ds = LibriBrainWord(\n",
        "    data_path=CONFIG[\"data_path\"],\n",
        "    include_run_keys=CONFIG[\"train_run_keys\"],\n",
        "    keyword_detection=CONFIG[\"keyword\"],\n",
        "    min_word_length=CONFIG[\"min_word_len\"],\n",
        "    max_word_length=CONFIG[\"max_word_len\"],\n",
        "    tmin=CONFIG[\"tmin\"],\n",
        "    tmax=CONFIG[\"tmax\"],\n",
        "    negative_buffer=CONFIG[\"negative_buffer\"],\n",
        "    positive_buffer=CONFIG[\"positive_buffer\"],\n",
        "    standardize=CONFIG[\"standardize\"],\n",
        "    clipping_boundary=CONFIG[\"clipping_boundary\"],\n",
        "    preload_files=True,\n",
        ")\n",
        "\n",
        "# Explicitly choose validation/test runs by scanning events.tsv for the keyword\n",
        "\n",
        "def _events_path(data_path, rk):\n",
        "    s, se, t, r = rk\n",
        "    return os.path.join(data_path, t, \"derivatives\", \"events\", f\"sub-{s}_ses-{se}_task-{t}_run-{r}_events.tsv\")\n",
        "\n",
        "def _count_keyword(data_path, rk, keyword, min_len=None, max_len=None):\n",
        "    f = _events_path(data_path, rk)\n",
        "    try:\n",
        "        LibriBrainBase.ensure_file_download(f, data_path=data_path)\n",
        "    except Exception:\n",
        "        if not os.path.exists(f):\n",
        "            return 0, 0\n",
        "    try:\n",
        "        df = pd.read_csv(f, sep=\"\\t\")\n",
        "    except Exception:\n",
        "        return 0, 0\n",
        "    if \"kind\" in df.columns:\n",
        "        df = df[df[\"kind\"] == \"word\"].copy()\n",
        "    if \"segment\" not in df.columns:\n",
        "        return 0, 0\n",
        "    seg = df[\"segment\"].astype(str).str.strip()\n",
        "    if min_len and min_len > 1:\n",
        "        seg = seg[seg.str.len() >= min_len]\n",
        "    if max_len:\n",
        "        seg = seg[seg.str.len() <= max_len]\n",
        "    total = int(seg.shape[0])\n",
        "    pos = int(seg.str.lower().eq(str(keyword).lower()).sum()) if total > 0 else 0\n",
        "    return pos, total\n",
        "\n",
        "def _choose_val_test(data_path, keyword, min_len=None, max_len=None):\n",
        "    cands = [tuple(rk) for rk in RUN_KEYS]\n",
        "    scored = []  # (prev, pos, total, rk)\n",
        "    for rk in cands:\n",
        "        pos, total = _count_keyword(data_path, rk, keyword, min_len, max_len)\n",
        "        if total > 0 and pos > 0:\n",
        "            prev = pos / total\n",
        "            scored.append((prev, pos, total, rk))\n",
        "    if not scored:\n",
        "        return VALIDATION_RUN_KEYS[0], TEST_RUN_KEYS[0]\n",
        "    scored.sort(key=lambda x: (-x[0], -x[1]))\n",
        "    val_rk = scored[0][3]\n",
        "    pool = [s for s in scored if s[3] != val_rk]\n",
        "    if not pool:\n",
        "        return val_rk, scored[0][3]\n",
        "    pool.sort(key=lambda x: (0 if x[3] in set(TEST_RUN_KEYS) else 1, -x[0], -x[1]))\n",
        "    test_rk = pool[0][3]\n",
        "    return val_rk, test_rk\n",
        "\n",
        "val_rk, test_rk = _choose_val_test(\n",
        "    data_path=CONFIG[\"data_path\"],\n",
        "    keyword=CONFIG[\"keyword\"],\n",
        "    min_len=CONFIG[\"min_word_len\"],\n",
        "    max_len=CONFIG[\"max_word_len\"],\n",
        ")\n",
        "print(\"Chosen VAL:\", val_rk, \"| TEST:\", test_rk)\n",
        "\n",
        "val_ds = LibriBrainWord(\n",
        "    data_path=CONFIG[\"data_path\"],\n",
        "    include_run_keys=[val_rk],\n",
        "    keyword_detection=CONFIG[\"keyword\"],\n",
        "    min_word_length=CONFIG[\"min_word_len\"],\n",
        "    max_word_length=CONFIG[\"max_word_len\"],\n",
        "    tmin=CONFIG[\"tmin\"],\n",
        "    tmax=CONFIG[\"tmax\"],\n",
        "    negative_buffer=CONFIG[\"negative_buffer\"],\n",
        "    positive_buffer=CONFIG[\"positive_buffer\"],\n",
        "    standardize=CONFIG[\"standardize\"],\n",
        "    clipping_boundary=CONFIG[\"clipping_boundary\"],\n",
        "    preload_files=True,\n",
        ")\n",
        "\n",
        "test_ds = LibriBrainWord(\n",
        "    data_path=CONFIG[\"data_path\"],\n",
        "    include_run_keys=[test_rk],\n",
        "    keyword_detection=CONFIG[\"keyword\"],\n",
        "    min_word_length=CONFIG[\"min_word_len\"],\n",
        "    max_word_length=CONFIG[\"max_word_len\"],\n",
        "    tmin=CONFIG[\"tmin\"],\n",
        "    tmax=CONFIG[\"tmax\"],\n",
        "    negative_buffer=CONFIG[\"negative_buffer\"],\n",
        "    positive_buffer=CONFIG[\"positive_buffer\"],\n",
        "    standardize=CONFIG[\"standardize\"],\n",
        "    clipping_boundary=CONFIG[\"clipping_boundary\"],\n",
        "    preload_files=True,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=CONFIG[\"num_workers\"])\n",
        "val_loader = DataLoader(val_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "test_loader = DataLoader(test_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "\n",
        "print(\"Training samples: \",len(train_ds))\n",
        "print(\"Validation samples: \",len(val_ds))\n",
        "print(\"Test samples\", len(test_ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGelpDfl351J"
      },
      "source": [
        "### Step 1: Model Architecture\n",
        "While high quality training data is extremely important, it also matters what type of model you train with it. Our model addresses the challenges we identified through three key components:\n",
        "\n",
        "1. **Spatial-Temporal Processing** ‚Äî efficiently handle high-dimensional MEG data\n",
        "2. **Attention-Based Pooling** ‚Äî focus on discriminative time points within each window  \n",
        "3. **Adapted Training Strategy** ‚Äî specialised losses and sampling for rare positive examples\n",
        "\n",
        "#### Architecture Overview\n",
        "\n",
        "![Model Architecture Diagram](https://proper-parallel6-dull-outside-car.github.io/libribrain-keyword/tutorial/img/model-architecture.png)\n",
        "\n",
        "#### Component Details\n",
        "Let's walk through the things that make the model work!\n",
        "\n",
        "**üèóÔ∏è Convolutional Trunk**\n",
        "\n",
        "A lightweight Conv1D front-end projects the 306 MEG channels into a 128-dimensional feature space that already mixes spatial information across sensors. Stacked residual blocks plus a stride-2 downsampling expand the temporal receptive field and denoise/compress the sequence, leaving compact features that preserve event timing for the attention stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXpV9kxz351J"
      },
      "outputs": [],
      "source": [
        "# Demo: Convolutional Trunk -} 306‚Üí128 + residual blocks + downsampling (stride 2)\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, ch, k=3, dilation=1):\n",
        "        super().__init__()\n",
        "        p = (k - 1) // 2 * dilation\n",
        "        self.conv1 = nn.Conv1d(ch, ch, k, padding=p, dilation=dilation)\n",
        "        self.elu   = nn.ELU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(ch, ch, k, padding=p, dilation=dilation)\n",
        "        self.short = nn.Identity()\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='linear')\n",
        "        nn.init.zeros_(self.conv2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv1(x)\n",
        "        h = self.elu(h)\n",
        "        h = self.conv2(h)\n",
        "        return self.elu(h + self.short(x))\n",
        "\n",
        "class ConvTrunk(nn.Module):\n",
        "    def __init__(self, c_in=306, c_mid=128, k=7, n_blocks=2, stride=2):\n",
        "        super().__init__()\n",
        "        p = (k - 1) // 2\n",
        "        self.front = nn.Conv1d(c_in, c_mid, k, padding=p)\n",
        "        self.blocks = nn.Sequential(*[ResidualBlock(c_mid, k=3) for _ in range(n_blocks)])\n",
        "        self.down   = nn.Conv1d(c_mid, c_mid, 3, padding=1, stride=stride)\n",
        "        self.act    = nn.ELU(inplace=True)\n",
        "\n",
        "    def forward(self, x):  # (B, 306, T) -> (B, 128, T//2)\n",
        "        x = self.front(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.down(x)\n",
        "        return self.act(x)\n",
        "\n",
        "# Dummy MEG-like input: batch of 8, 306 chans, 250 timepoints (~1s@250Hz)\n",
        "B, C, T = 8, 306, 250\n",
        "x = torch.randn(B, C, T)\n",
        "trunk = ConvTrunk()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y = trunk(x)\n",
        "\n",
        "print(f\"Input shape:  {tuple(x.shape)}\")\n",
        "print(f\"Output shape: {tuple(y.shape)}  (306‚Üí128 channels; 250‚Üí{y.shape[-1]} time)\")\n",
        "\n",
        "# Quick visualization: channel-energy compression (sum of squares across time)\n",
        "x_energy = (x**2).sum(dim=-1).mean(dim=0).numpy()       # (306,)\n",
        "y_energy = (y**2).sum(dim=-1).mean(dim=0).numpy()       # (128,)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,3))\n",
        "ax.plot(np.log1p(np.sort(x_energy))[::-1], label=\"Input (306) energy\")\n",
        "ax.plot(np.log1p(np.sort(y_energy))[::-1], label=\"Trunk (128) energy\")\n",
        "ax.set_title(\"Convolutional trunk compresses channels while retaining structure\")\n",
        "ax.set_xlabel(\"Channel (sorted by energy)\")\n",
        "ax.set_ylabel(\"log(1+energy)\")\n",
        "ax.legend(); ax.grid(alpha=0.3); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg2LXYqE351J"
      },
      "source": [
        "**üß† Temporal Attention**\n",
        "\n",
        "A parallel attention head scores each time step and forms a softmax weight over the window, letting the model concentrate probability mass on brief, informative bursts (e.g., around keyword onsets) while down-weighting idle/noisy segments. This adaptive, interpretable pooling handles variable latency/duration better than mean or max, which are either too diffuse or too brittle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JYVkt7m351J"
      },
      "outputs": [],
      "source": [
        "# Demo: Temporal Attention - create a synthetic \"event\" so attention has something real to lock onto\n",
        "import torch, torch.nn as nn\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "class TemporalAttentionHead(nn.Module):\n",
        "    def __init__(self, c_in=128):\n",
        "        super().__init__()\n",
        "        self.logit_head = nn.Conv1d(c_in, 1, kernel_size=1)\n",
        "        self.attn_head  = nn.Conv1d(c_in, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, h):      # h: (B, C, T')\n",
        "        logit_t = self.logit_head(h)             # (B, 1, T')\n",
        "        attn_t  = self.attn_head(h)              # (B, 1, T')\n",
        "        attn_w  = torch.softmax(attn_t, -1)      # across time\n",
        "        pooled  = (attn_w * logit_t).sum(-1)     # (B, 1)\n",
        "        return pooled.squeeze(1), logit_t, attn_w\n",
        "\n",
        "# --- synth trunk activations with a localized event on a subset of channels\n",
        "B, C, Tprime = 1, 128, 160\n",
        "t0, sigma, amp, active_ch = 95, 6.0, 2.5, 16\n",
        "\n",
        "h = torch.randn(B, C, Tprime) * 0.4\n",
        "t = torch.arange(Tprime).float()\n",
        "event = torch.exp(-0.5 * ((t - t0) / sigma) ** 2)  # Gaussian bump\n",
        "sel = torch.randperm(C)[:active_ch]\n",
        "h[:, sel, :] += amp * event  # inject event pattern on a few channels\n",
        "\n",
        "head = TemporalAttentionHead(C)\n",
        "with torch.no_grad():\n",
        "    pooled, logit_t, attn_w = head(h)\n",
        "\n",
        "# Compare pooling strategies on the same per-time logits\n",
        "lt = logit_t[0, 0]                  # (T')\n",
        "aw = attn_w[0, 0]\n",
        "mean_pool = lt.mean()\n",
        "max_pool  = lt.max()\n",
        "attn_pool = (aw * lt).sum()\n",
        "\n",
        "# Effective support (how concentrated attention is): smaller = sharper\n",
        "eff_support = 1.0 / float((aw**2).sum())\n",
        "\n",
        "print(f\"Pooled logits ‚Äî mean: {mean_pool:.3f} | max: {max_pool:.3f} | attention: {attn_pool:.3f}\")\n",
        "print(f\"Attention effective support (1/sum(w^2)) = {eff_support:.1f} time-steps (lower = more focused)\")\n",
        "\n",
        "# Visualize: per-time logits + attention weights, with the event region shaded\n",
        "fig, ax = plt.subplots(figsize=(7.5,3.4))\n",
        "ax.plot(lt.numpy(), label=\"per-time logits\")\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(aw.numpy(), alpha=0.7, label=\"attention weights\")\n",
        "ax.axvspan(int(t0 - 3*sigma), int(t0 + 3*sigma), alpha=0.15, lw=0, label=\"event region\")\n",
        "ax.set_title(\"Temporal attention locks onto the event-like region\")\n",
        "ax.set_xlabel(\"Time index\")\n",
        "ax.set_ylabel(\"Logit\"); ax2.set_ylabel(\"Attention weight\")\n",
        "ax.grid(alpha=0.3)\n",
        "lines, labels = ax.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax.legend(lines+lines2, labels+labels2, loc=\"upper left\")\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylToqG1W351J"
      },
      "source": [
        "**‚öñÔ∏è Focal Loss (Œ±=0.95, Œ≥=2.0)**\n",
        "\n",
        "Focal loss rescales BCE by $(1-p_t)^\\gamma$ with a class prior $\\alpha$, so the ocean of easy negatives contributes almost nothing while positives and near-miss negatives dominate the gradient. Setting $\\alpha{=}0.95,\\ \\gamma{=}2.0$ matches the <1% base rate, preventing ‚Äúalways negative‚Äù collapse and improving ranking without aggressive oversampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_b1brXO351J"
      },
      "outputs": [],
      "source": [
        "# Demo: Focal loss vs BCE - visualize which examples contribute to the loss\n",
        "import torch, torch.nn.functional as F\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0); np.random.seed(0)\n",
        "\n",
        "def binary_focal_loss_with_logits(logits, targets, alpha=0.95, gamma=2.0, reduction='none'):\n",
        "    p = torch.sigmoid(logits)\n",
        "    ce = F.binary_cross_entropy_with_logits(logits, targets.float(), reduction='none')\n",
        "    pt = p*targets + (1-p)*(1-targets)\n",
        "    alpha_t = alpha*targets + (1-alpha)*(1-targets)\n",
        "    loss = alpha_t * (1-pt).clamp_min(0).pow(gamma) * ce\n",
        "    return loss if reduction=='none' else loss.mean()\n",
        "\n",
        "# Simulate a heavily imbalanced batch: ~0.8% positives\n",
        "N = 12000\n",
        "pos_n = max(1, int(0.008 * N))\n",
        "y = torch.zeros(N, dtype=torch.long)\n",
        "pos_idx = torch.randperm(N)[:pos_n]\n",
        "y[pos_idx] = 1\n",
        "\n",
        "# Build logits:\n",
        "# - many easy negatives (very < 0)\n",
        "# - some medium negatives\n",
        "# - a small slice of hard negatives (~ around 0)\n",
        "# - positives somewhat > 0 with noise\n",
        "logits = torch.randn(N) * 0.5 - 1.6\n",
        "neg_mask = (y == 0)\n",
        "neg_ids = neg_mask.nonzero(as_tuple=True)[0]\n",
        "hard_ids = neg_ids[torch.randperm(len(neg_ids))[:max(50, N//100)]]\n",
        "mid_ids  = neg_ids[torch.randperm(len(neg_ids))[:N//30]]\n",
        "logits[mid_ids] += 0.9\n",
        "logits[hard_ids] = torch.randn(len(hard_ids)) * 0.25  # centered near 0 (hard)\n",
        "logits[pos_idx]  = torch.randn(len(pos_idx)) * 0.5 + 1.2\n",
        "\n",
        "# Per-example losses\n",
        "bce_per   = F.binary_cross_entropy_with_logits(logits, y.float(), reduction='none').detach()\n",
        "focal_per = binary_focal_loss_with_logits(logits, y, alpha=0.95, gamma=2.0, reduction='none').detach()\n",
        "\n",
        "# Group examples\n",
        "def group_of(i):\n",
        "    if y[i] == 1: return \"positives\"\n",
        "    z = logits[i].item()\n",
        "    if z < -1.0: return \"easy neg\"\n",
        "    if -1.0 <= z < -0.3: return \"medium neg\"\n",
        "    if -0.3 <= z < 0.5:  return \"hard neg\"\n",
        "    return \"borderline neg\"\n",
        "\n",
        "groups = [\"positives\",\"hard neg\",\"borderline neg\",\"medium neg\",\"easy neg\"]\n",
        "bce_contrib = {g:0.0 for g in groups}\n",
        "foc_contrib = {g:0.0 for g in groups}\n",
        "\n",
        "for i in range(N):\n",
        "    g = group_of(i)\n",
        "    bce_contrib[g] += float(bce_per[i])\n",
        "    foc_contrib[g] += float(focal_per[i])\n",
        "\n",
        "bce_total  = sum(bce_contrib.values())\n",
        "foc_total  = sum(foc_contrib.values())\n",
        "bce_share  = [100.0 * bce_contrib[g] / bce_total for g in groups]\n",
        "foc_share  = [100.0 * foc_contrib[g] / foc_total for g in groups]\n",
        "\n",
        "print(\"BCE vs Focal ‚Äî share of total loss by group (%):\")\n",
        "for g, b, f in zip(groups, bce_share, foc_share):\n",
        "    print(f\"  {g:13s}  BCE {b:6.2f}%   |   Focal {f:6.2f}%\")\n",
        "\n",
        "# Plot: contribution shares side-by-side\n",
        "x = np.arange(len(groups))\n",
        "w = 0.38\n",
        "fig, ax = plt.subplots(figsize=(7.5,3.6))\n",
        "ax.bar(x - w/2, bce_share, width=w, label=\"BCE\")\n",
        "ax.bar(x + w/2, foc_share, width=w, label=\"Focal (Œ±=0.95, Œ≥=2)\")\n",
        "ax.set_xticks(x); ax.set_xticklabels(groups, rotation=15)\n",
        "ax.set_ylabel(\"Share of total loss (%)\")\n",
        "ax.set_title(\"Focal loss down-weights easy negatives, emphasizes positives & hard negatives\")\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "ax.legend(loc=\"upper right\")\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uidT0peA351K"
      },
      "source": [
        "**üéØ Pairwise Ranking Loss**\n",
        "A pairwise logistic term compares each positive to sampled in-batch negatives and penalizes inversions (when $s_{+}\\!\\le\\!s_{-}$), directly improving the **ordering** that drives precision‚Äìrecall. Complementing focal, it widens the margin to hard negatives and yields more stable low‚Äìfalse-alarm thresholds‚Äîthe regime that matters for KWS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gAtk9QX351K"
      },
      "outputs": [],
      "source": [
        "# Demo: Pairwise logistic ranking loss ‚Äî changes ordering ‚Üí PR improves\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "torch.manual_seed(0); np.random.seed(0)\n",
        "\n",
        "def pairwise_logistic_loss(scores: torch.Tensor, targets: torch.Tensor, num_neg: int = 24):\n",
        "    \"\"\"\n",
        "    scores:  (N,) float tensor (can require_grad)\n",
        "    targets: (N,) {0,1} long tensor\n",
        "    Loss = mean(log(1 + exp(-(s_pos - s_neg)))) over sampled pos/neg pairs.\n",
        "    \"\"\"\n",
        "    pos = (targets == 1).nonzero(as_tuple=True)[0]\n",
        "    neg = (targets == 0).nonzero(as_tuple=True)[0]\n",
        "    if len(pos) == 0 or len(neg) == 0:\n",
        "        return scores.new_tensor(0.0), scores.new_tensor([0.0])\n",
        "\n",
        "    pairs_pos, pairs_neg = [], []\n",
        "    for p in pos:\n",
        "        sel = neg[torch.randint(0, len(neg), (min(num_neg, len(neg)),))]\n",
        "        pairs_pos.append(p.repeat(len(sel)))\n",
        "        pairs_neg.append(sel)\n",
        "    pos_idx = torch.cat(pairs_pos); neg_idx = torch.cat(pairs_neg)\n",
        "    margins = scores[pos_idx] - scores[neg_idx]\n",
        "    loss = torch.log1p(torch.exp(-margins)).mean()\n",
        "    return loss, margins\n",
        "\n",
        "# --- 1) Synthetic batch with overlap (ranking can improve)\n",
        "N, pos_n = 1500, 24\n",
        "y = torch.zeros(N, dtype=torch.long)\n",
        "pos_idx = torch.randperm(N)[:pos_n]; y[pos_idx] = 1\n",
        "\n",
        "scores0 = (torch.randn(N) * 0.8 - 0.1).to(torch.float32)\n",
        "scores0[pos_idx] += 0.4  # slight lift for positives\n",
        "\n",
        "# Metrics BEFORE\n",
        "loss0, margins0 = pairwise_logistic_loss(scores0.detach(), y, num_neg=24)\n",
        "ap0 = average_precision_score(y.numpy(), scores0.detach().numpy())\n",
        "\n",
        "# --- 2) Optimize scores directly (Adam)\n",
        "scores = torch.nn.Parameter(scores0.detach().clone())\n",
        "opt = torch.optim.Adam([scores], lr=0.02)\n",
        "K = 50\n",
        "for _ in range(K):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    loss, _ = pairwise_logistic_loss(scores, y, num_neg=24)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "scores1 = scores.detach()\n",
        "loss1, margins1 = pairwise_logistic_loss(scores1, y, num_neg=24)\n",
        "ap1 = average_precision_score(y.numpy(), scores1.numpy())\n",
        "\n",
        "print(f\"Pairwise loss  ‚Äî before: {float(loss0):.4f} | after: {float(loss1):.4f}\")\n",
        "print(f\"Avg margin s+‚àís‚àí ‚Äî before: {float(margins0.mean()):.3f} | after: {float(margins1.mean()):.3f}\")\n",
        "print(f\"AUPRC (AP)     ‚Äî before: {ap0:.3f} | after: {ap1:.3f}\")\n",
        "\n",
        "# --- 3) Visuals: PR curves + score distributions\n",
        "prec0, rec0, _ = precision_recall_curve(y.numpy(), scores0.numpy())\n",
        "prec1, rec1, _ = precision_recall_curve(y.numpy(), scores1.numpy())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7.2,4.0))\n",
        "ax.plot(rec0, prec0, label=f\"Before (AP={ap0:.3f})\")\n",
        "ax.plot(rec1, prec1, label=f\"After {K} steps (AP={ap1:.3f})\")\n",
        "ax.set_title(\"Pairwise ranking improves ordering ‚Üí better PR\")\n",
        "ax.set_xlabel(\"Recall\"); ax.set_ylabel(\"Precision\")\n",
        "ax.grid(alpha=0.3); ax.legend(loc=\"lower left\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,3.6), sharey=True)\n",
        "ax[0].hist(scores0[y==0].numpy(), bins=40, alpha=0.85, label=\"neg\")\n",
        "ax[0].hist(scores0[y==1].numpy(), bins=20, alpha=0.85, label=\"pos\")\n",
        "ax[0].set_title(\"Scores before\"); ax[0].legend()\n",
        "\n",
        "ax[1].hist(scores1[y==0].numpy(), bins=40, alpha=0.85, label=\"neg\")\n",
        "ax[1].hist(scores1[y==1].numpy(), bins=20, alpha=0.85, label=\"pos\")\n",
        "ax[1].set_title(\"Scores after\"); ax[1].legend()\n",
        "for a in ax: a.grid(alpha=0.3); a.set_xlabel(\"score\")\n",
        "ax[0].set_ylabel(\"count\")\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vab1AqCP351K"
      },
      "source": [
        "**üìä Balanced Sampling**\n",
        "\n",
        "We build training batches with a target ~10% positive rate‚Äîby pulling in all/most positives and subsampling negatives‚Äîso gradients aren‚Äôt starved by all-negative minibatches and the model learns rare-event features faster. Crucially, evaluation stays on the natural class prior, so AUPRC and false-alarms-per-hour reflect real-world conditions while training remains stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGHl6nnJ351K",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Ultra-fast one-cell demo: natural vs balanced composition\n",
        "# - Exact if train_ds exposes cached labels (labels/y/targets)\n",
        "# - Otherwise uses a tiny random sample to estimate natural prevalence\n",
        "# - Balanced: uses your global 'sampler' for 5 batches if present; else simulates ~10% positives\n",
        "\n",
        "import time, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "assert 'train_ds' in globals(), \"Missing train_ds. Run your dataset setup first.\"\n",
        "\n",
        "B = int(CONFIG.get(\"batch_size\", 64))\n",
        "TARGET_POS = 0.10\n",
        "NUM_ITERS = 5  # tiny, to keep it snappy\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "# ---------- Try to get labels without touching MEG arrays ----------\n",
        "def _get_label_array_if_any(ds):\n",
        "    for name in (\"labels\", \"y\", \"targets\", \"labels_np\", \"label_array\"):\n",
        "        if hasattr(ds, name):\n",
        "            arr = getattr(ds, name)\n",
        "            try:\n",
        "                return np.asarray(arr, dtype=np.int64)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "labels_arr = _get_label_array_if_any(train_ds)\n",
        "N = len(train_ds)\n",
        "\n",
        "# ---------- Natural prevalence ----------\n",
        "if labels_arr is not None and len(labels_arr) == N:\n",
        "    # Fast path: exact, zero window loads\n",
        "    nat_pos = int((labels_arr == 1).sum()); nat_tot = N\n",
        "    nat_frac = nat_pos / max(1, nat_tot)\n",
        "    nat_mode = \"exact (cached labels)\"\n",
        "else:\n",
        "    # Fast estimate: small random sample (min 512 or 0.2% of N)\n",
        "    sample_n = int(max(512, 0.002 * N))\n",
        "    sample_idx = rng.choice(np.arange(N), size=min(sample_n, N), replace=False)\n",
        "    pos_hits = 0\n",
        "    t0 = time.time()\n",
        "    for j in sample_idx:\n",
        "        yj = train_ds[j][1]  # accesses only a tiny subset\n",
        "        pos_hits += int(yj)\n",
        "    took = time.time() - t0\n",
        "    nat_frac = pos_hits / max(1, len(sample_idx))\n",
        "    nat_pos = int(round(nat_frac * N)); nat_tot = N\n",
        "    nat_mode = f\"estimated from {len(sample_idx)} samples ({took:.2f}s)\"\n",
        "\n",
        "# ---------- Balanced observation ----------\n",
        "obs_pos = obs_tot = 0\n",
        "start = time.time()\n",
        "if 'sampler' in globals():\n",
        "    # Light touch: just a few batches\n",
        "    it = iter(sampler)\n",
        "    for _ in range(NUM_ITERS):\n",
        "        try:\n",
        "            batch_indices = next(it)\n",
        "        except StopIteration:\n",
        "            break\n",
        "        # Only read labels; keep count small for speed\n",
        "        ys = [int(train_ds[j][1]) for j in batch_indices]\n",
        "        obs_pos += sum(ys); obs_tot += len(ys)\n",
        "    bal_mode = f\"observed via global sampler ({NUM_ITERS} batches)\"\n",
        "else:\n",
        "    # No sampler? Simulate balanced batches ~10% positives (no dataset access)\n",
        "    for _ in range(NUM_ITERS):\n",
        "        k_pos = int(round(TARGET_POS * B))\n",
        "        k_neg = max(0, B - k_pos)\n",
        "        obs_pos += k_pos; obs_tot += (k_pos + k_neg)\n",
        "    bal_mode = f\"simulated at TARGET_POS={TARGET_POS:.2f}\"\n",
        "\n",
        "elapsed = time.time() - start\n",
        "obs_frac = (obs_pos / obs_tot) if obs_tot else TARGET_POS\n",
        "\n",
        "# ---------- Plot ----------\n",
        "labels_txt = [\"Natural\", \"Balanced\"]\n",
        "pos_fracs = [nat_frac, obs_frac]\n",
        "neg_fracs = [1 - nat_frac, 1 - obs_frac]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.bar(labels_txt, neg_fracs, label=\"negatives\", color=\"#c7d4e8\")\n",
        "ax.bar(labels_txt, pos_fracs, bottom=neg_fracs, label=\"positives\", color=\"#4c72b0\")\n",
        "ax.set_ylabel(\"Fraction in batch\")\n",
        "ax.set_title(\"Class composition: natural vs balanced\")\n",
        "ax.legend(loc=\"upper right\")\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "print(f\"Natural pos frac: {nat_frac:.5f}  | Balanced pos frac (observed): {obs_frac:.2f}\")\n",
        "print(f\"Counts ‚Äî Train: N‚âà{nat_tot:,} (pos‚âà{nat_pos:,}, neg‚âà{nat_tot - nat_pos:,})\")\n",
        "print(f\"[natural] mode: {nat_mode}\")\n",
        "print(f\"[balanced] mode: {bal_mode}  | elapsed={elapsed:.3f}s  | batch_size={B}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dh4ltHj351K"
      },
      "source": [
        "Now, let's **train the model**! We define the model architecture, sampler, loss, Pytorch Lighning module (for convenience). We then start the actual training run. Depending on your GPU, this may take **~30 minutes.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RsU5P5F351K",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import math, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader, Dataset, BatchSampler\n",
        "\n",
        "# NEW: metrics/figures deps\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lightning\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
        "\n",
        "# ----------------------------- Model blocks -----------------------------\n",
        "class ResNetBlock1D(nn.Module):\n",
        "    def __init__(self, channels: int = 128):\n",
        "        super().__init__()\n",
        "        same_supported = 'same' in nn.Conv1d.__init__.__code__.co_varnames\n",
        "        pad3 = 'same' if same_supported else 1\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ELU(), nn.Conv1d(channels, channels, 3, 1, pad3),\n",
        "            nn.ELU(), nn.Conv1d(channels, channels, 1, 1, 0),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.net(x)\n",
        "\n",
        "class SpeechDetectionNet(nn.Module):\n",
        "    def __init__(self, in_channels: int = 306, lse_temperature: float = 0.5):\n",
        "        super().__init__()\n",
        "        same_supported = 'same' in nn.Conv1d.__init__.__code__.co_varnames\n",
        "        pad7 = 'same' if same_supported else 3\n",
        "        self.trunk = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, 128, 7, 1, pad7),\n",
        "            ResNetBlock1D(128),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(128, 128, 50, 25, 0),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(128, 128, 7, 1, pad7),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.head = nn.Sequential(nn.Conv1d(128, 512, 4, 1, 0), nn.ReLU(), nn.Dropout(0.5))\n",
        "        self.logits_t = nn.Conv1d(512, 1, 1, 1, 0)\n",
        "        self.attn_t   = nn.Conv1d(512, 1, 1, 1, 0)\n",
        "    def forward(self, x):\n",
        "        h = self.head(self.trunk(x))\n",
        "        logit_t = self.logits_t(h)\n",
        "        attn = torch.softmax(self.attn_t(h), dim=-1)\n",
        "        return (logit_t * attn).sum(dim=-1).squeeze(1)\n",
        "\n",
        "@dataclass\n",
        "class OptimConfig:\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    max_time_shift: int = 4\n",
        "    noise_std: float = 0.01\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha: float = 0.95, gamma: float = 2.0):\n",
        "        super().__init__(); self.alpha=float(alpha); self.gamma=float(gamma)\n",
        "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        ce = nn.functional.binary_cross_entropy_with_logits(logits, targets.float(), reduction='none')\n",
        "        p = torch.sigmoid(logits); pt = torch.where(targets == 1, p, 1 - p)\n",
        "        alpha_t = torch.where(targets == 1, logits.new_tensor(self.alpha), logits.new_tensor(1 - self.alpha))\n",
        "        return (alpha_t * (1 - pt).pow(self.gamma) * ce).mean()\n",
        "\n",
        "def pairwise_logistic_loss(logits: torch.Tensor, labels: torch.Tensor, max_pairs: int = 4096) -> torch.Tensor:\n",
        "    pos_idx = (labels == 1).nonzero(as_tuple=False).view(-1)\n",
        "    neg_idx = (labels == 0).nonzero(as_tuple=False).view(-1)\n",
        "    if pos_idx.numel() == 0 or neg_idx.numel() == 0: return logits.new_zeros(())\n",
        "    num_pairs = min(max_pairs, int(pos_idx.numel()) * int(neg_idx.numel()))\n",
        "    pi = pos_idx[torch.randint(0, pos_idx.numel(), (num_pairs,), device=logits.device)]\n",
        "    ni = neg_idx[torch.randint(0, neg_idx.numel(), (num_pairs,), device=logits.device)]\n",
        "    return torch.nn.functional.softplus(-(logits[pi] - logits[ni])).mean()\n",
        "\n",
        "class BalancedBatchSampler(BatchSampler):\n",
        "    def __init__(self, pos_idx, neg_idx, batch_size: int, pos_fraction: float = 0.1):\n",
        "        assert 0.0 < pos_fraction < 1.0 and len(pos_idx) > 0\n",
        "        self.p_idx, self.n_idx = list(pos_idx), list(neg_idx)\n",
        "        self.batch_size = batch_size\n",
        "        self.n_pos = max(1, int(round(batch_size * pos_fraction)))\n",
        "        self.n_neg = batch_size - self.n_pos\n",
        "        total = len(self.p_idx) + len(self.n_idx)\n",
        "        self._epoch_len = max(1, total // batch_size)\n",
        "    def __iter__(self):\n",
        "        p, n = self.p_idx[:], self.n_idx[:]\n",
        "        random.shuffle(p); random.shuffle(n); pi = ni = 0\n",
        "        while True:\n",
        "            if pi + self.n_pos > len(p): random.shuffle(p); pi = 0\n",
        "            if ni + self.n_neg > len(n): random.shuffle(n); ni = 0\n",
        "            batch = p[pi:pi+self.n_pos] + n[ni:ni+self.n_neg]\n",
        "            pi += self.n_pos; ni += self.n_neg\n",
        "            random.shuffle(batch); yield batch\n",
        "    def __len__(self): return self._epoch_len\n",
        "\n",
        "# --------------------- Lightning module with rich metrics ---------------------\n",
        "class KeywordDetectorPL(pl.LightningModule):\n",
        "    def __init__(self, in_channels: int = 306, opt: OptimConfig = OptimConfig(), pairwise_lambda: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = SpeechDetectionNet(in_channels)\n",
        "        self.criterion = FocalLoss(alpha=0.95, gamma=2.0)\n",
        "        self.pairwise_lambda = float(pairwise_lambda)\n",
        "\n",
        "        self._val_probs, self._val_targets = [], []\n",
        "        self._test_probs, self._test_targets = [], []\n",
        "\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "    def _augment(self, x):\n",
        "        if not self.training: return x\n",
        "        smax = self.hparams.opt.max_time_shift\n",
        "        if smax and smax > 0:\n",
        "            shifts = torch.randint(-smax, smax + 1, (x.size(0),), device=x.device)\n",
        "            for i, sh in enumerate(shifts):\n",
        "                if int(sh) != 0: x[i] = torch.roll(x[i], int(sh), dims=-1)\n",
        "        sigma = self.hparams.opt.noise_std\n",
        "        return x + torch.randn_like(x) * sigma if (sigma and sigma > 0) else x\n",
        "\n",
        "    # -- steps --\n",
        "    def training_step(self, batch, _):\n",
        "        x, y = batch\n",
        "        logits = self(self._augment(x))\n",
        "        focal = self.criterion(logits.float(), y.float())\n",
        "        pairwise = pairwise_logistic_loss(logits.detach(), y)\n",
        "        loss = focal + self.pairwise_lambda * pairwise\n",
        "        self.log_dict(\n",
        "            {\"train/loss\": loss, \"train/focal\": focal, \"train/pairwise\": pairwise},\n",
        "            on_step=False, on_epoch=True, prog_bar=False\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, _):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.functional.binary_cross_entropy_with_logits(logits.float(), y.float())\n",
        "        probs = torch.sigmoid(logits.detach()).float().view(-1).cpu()\n",
        "        self._val_probs.append(probs)\n",
        "        self._val_targets.append(y.detach().float().view(-1).cpu())\n",
        "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, _):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.functional.binary_cross_entropy_with_logits(logits.float(), y.float())\n",
        "        probs = torch.sigmoid(logits.detach()).float().view(-1).cpu()\n",
        "        self._test_probs.append(probs)\n",
        "        self._test_targets.append(y.detach().float().view(-1).cpu())\n",
        "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    # -- epoch hooks --\n",
        "    def on_validation_epoch_start(self):\n",
        "        self._val_probs, self._val_targets = [], []\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if len(self._val_probs) == 0: return\n",
        "        probs = torch.cat(self._val_probs).numpy()\n",
        "        y = torch.cat(self._val_targets).numpy().astype(np.int64)\n",
        "        metrics, figs = self._compute_all_metrics(y, probs)\n",
        "        for k, v in metrics.items():\n",
        "            if np.isnan(v): continue\n",
        "            self.log(f\"val/{k}\", float(v), prog_bar=(k in [\"auprc\",\"roc_auc\",\"best_f1\",\"uplift\"]))\n",
        "        self._log_figs_tensorboard(figs, split=\"val\")\n",
        "\n",
        "    def on_test_epoch_start(self):\n",
        "        self._test_probs, self._test_targets = [], []\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        if len(self._test_probs) == 0: return\n",
        "        probs = torch.cat(self._test_probs).numpy()\n",
        "        y = torch.cat(self._test_targets).numpy().astype(np.int64)\n",
        "        metrics, figs = self._compute_all_metrics(y, probs)\n",
        "        for k, v in metrics.items():\n",
        "            if np.isnan(v): continue\n",
        "            self.log(f\"test/{k}\", float(v), prog_bar=(k in [\"auprc\",\"roc_auc\",\"best_f1\",\"uplift\"]))\n",
        "        self._log_figs_tensorboard(figs, split=\"test\")\n",
        "\n",
        "    # -- optim --\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.opt.lr, weight_decay=self.hparams.opt.weight_decay)\n",
        "\n",
        "    # ------------------------ metric helpers ------------------------\n",
        "    @staticmethod\n",
        "    def _precision_recall_ap(y_true: np.ndarray, y_score: np.ndarray):\n",
        "        order = np.argsort(-y_score, kind=\"mergesort\")\n",
        "        y_sorted = y_true[order]\n",
        "        tp = np.cumsum(y_sorted)\n",
        "        fp = np.cumsum(1 - y_sorted)\n",
        "        P = tp[-1] if tp.size else 0\n",
        "\n",
        "        if P == 0:\n",
        "            recall = np.array([0.0, 1.0])\n",
        "            precision = np.array([1.0, 0.0])\n",
        "            ap = np.nan\n",
        "            return precision, recall, ap\n",
        "\n",
        "        recall = tp / P\n",
        "        precision = tp / np.maximum(tp + fp, 1)\n",
        "\n",
        "        recall = np.concatenate(([0.0], recall))\n",
        "        precision = np.concatenate(([1.0], precision))\n",
        "\n",
        "        for i in range(precision.size - 2, -1, -1):\n",
        "            precision[i] = max(precision[i], precision[i + 1])\n",
        "        ap = np.sum((recall[1:] - recall[:-1]) * precision[1:])\n",
        "        return precision, recall, float(ap)\n",
        "\n",
        "    @staticmethod\n",
        "    def _roc_auc_pairwise(y_true: np.ndarray, y_score: np.ndarray, max_pairs: int = 2_000_000):\n",
        "        P = int(y_true.sum()); N = int((1 - y_true).sum())\n",
        "        if P == 0 or N == 0: return np.nan\n",
        "        pos = y_score[y_true == 1]\n",
        "        neg = y_score[y_true == 0]\n",
        "        total = P * N\n",
        "        if total <= max_pairs:\n",
        "            gt = (pos[:, None] > neg[None, :]).mean()\n",
        "            eq = (pos[:, None] == neg[None, :]).mean()\n",
        "            return float(gt + 0.5 * eq)\n",
        "        rng = np.random.default_rng(42)\n",
        "        pi = rng.integers(0, P, size=max_pairs)\n",
        "        ni = rng.integers(0, N, size=max_pairs)\n",
        "        ps = pos[pi]; ns = neg[ni]\n",
        "        gt = (ps > ns).mean(); eq = (ps == ns).mean()\n",
        "        return float(gt + 0.5 * eq)\n",
        "\n",
        "    @staticmethod\n",
        "    def _threshold_metrics(y_true: np.ndarray, y_score: np.ndarray, thresh: float):\n",
        "        yhat = (y_score >= thresh).astype(np.int64)\n",
        "        tp = int(np.sum((yhat == 1) & (y_true == 1)))\n",
        "        fp = int(np.sum((yhat == 1) & (y_true == 0)))\n",
        "        fn = int(np.sum((yhat == 0) & (y_true == 1)))\n",
        "        tn = int(np.sum((yhat == 0) & (y_true == 0)))\n",
        "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
        "        return dict(tp=tp, fp=fp, fn=fn, tn=tn, precision=prec, recall=rec, f1=f1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _best_f1(y_true: np.ndarray, y_score: np.ndarray):\n",
        "        if y_true.sum() == 0 or (1 - y_true).sum() == 0:\n",
        "            return dict(threshold=np.nan, precision=np.nan, recall=np.nan, f1=np.nan)\n",
        "        qs = np.unique(np.percentile(y_score, np.linspace(0, 100, 201)))\n",
        "        best = {\"f1\": -1.0, \"threshold\": 0.5, \"precision\": 0.0, \"recall\": 0.0}\n",
        "        for t in qs:\n",
        "            m = KeywordDetectorPL._threshold_metrics(y_true, y_score, float(t))\n",
        "            if m[\"f1\"] > best[\"f1\"]:\n",
        "                best.update(m); best[\"threshold\"] = float(t)\n",
        "        return best\n",
        "\n",
        "    def _compute_all_metrics(self, y_true: np.ndarray, y_score: np.ndarray):\n",
        "        prevalence = float(y_true.mean()) if y_true.size else np.nan\n",
        "        brier = float(np.mean((y_score - y_true) ** 2)) if y_true.size else np.nan\n",
        "        precision, recall, ap = self._precision_recall_ap(y_true, y_score)\n",
        "        roc_auc = self._roc_auc_pairwise(y_true, y_score)\n",
        "\n",
        "        at_05 = self._threshold_metrics(y_true, y_score, 0.5)\n",
        "        best = self._best_f1(y_true, y_score)\n",
        "\n",
        "        uplift = (ap / prevalence) if (prevalence and prevalence > 0 and ap == ap) else np.nan\n",
        "\n",
        "        metrics = {\n",
        "            \"auprc\": ap if ap == ap else np.nan,\n",
        "            \"roc_auc\": roc_auc,\n",
        "            \"brier\": brier,\n",
        "            \"prevalence\": prevalence,\n",
        "            \"uplift\": uplift,\n",
        "            \"prec_at_0.5\": at_05[\"precision\"],\n",
        "            \"recall_at_0.5\": at_05[\"recall\"],\n",
        "            \"f1_at_0.5\": at_05[\"f1\"],\n",
        "            \"best_f1\": best[\"f1\"],\n",
        "            \"best_f1_threshold\": best[\"threshold\"],\n",
        "            \"best_f1_precision\": best[\"precision\"],\n",
        "            \"best_f1_recall\": best[\"recall\"],\n",
        "            \"n_samples\": float(y_true.size),\n",
        "            \"n_pos\": float(y_true.sum()),\n",
        "            \"n_neg\": float((1 - y_true).sum()),\n",
        "        }\n",
        "\n",
        "        figs = {}\n",
        "        # PR\n",
        "        fig_pr = plt.figure()\n",
        "        plt.plot(recall, precision)\n",
        "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
        "        title = f\"PR (AP={metrics['auprc']:.4f})\" if metrics[\"auprc\"] == metrics[\"auprc\"] else \"PR\"\n",
        "        plt.title(title)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        figs[\"pr\"] = fig_pr\n",
        "\n",
        "        # ROC (if valid)\n",
        "        if not np.isnan(roc_auc):\n",
        "            thr = np.unique(np.percentile(y_score, np.linspace(0, 100, 201)))\n",
        "            tprs, fprs = [], []\n",
        "            P = max(1, int(y_true.sum())); N = max(1, int((1 - y_true).sum()))\n",
        "            for t in thr:\n",
        "                yhat = (y_score >= t).astype(np.int64)\n",
        "                tp = np.sum((yhat == 1) & (y_true == 1))\n",
        "                fp = np.sum((yhat == 1) & (y_true == 0))\n",
        "                tprs.append(tp / P); fprs.append(fp / N)\n",
        "            fig_roc = plt.figure()\n",
        "            plt.plot(fprs, tprs)\n",
        "            plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)\n",
        "            plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
        "            plt.title(f\"ROC (AUC={roc_auc:.4f})\")\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            figs[\"roc\"] = fig_roc\n",
        "\n",
        "        return metrics, figs\n",
        "\n",
        "    def _log_figs_tensorboard(self, figs: dict, split: str):\n",
        "        writers = []\n",
        "        try:\n",
        "            candidates = []\n",
        "            if isinstance(self.logger, TensorBoardLogger):\n",
        "                candidates = [self.logger]\n",
        "            elif hasattr(self.trainer, \"loggers\") and self.trainer.loggers:\n",
        "                candidates = [lg for lg in self.trainer.loggers if isinstance(lg, TensorBoardLogger)]\n",
        "            elif self.logger is not None:\n",
        "                candidates = [self.logger] if isinstance(self.logger, TensorBoardLogger) else []\n",
        "            for lg in candidates:\n",
        "                writers.append(lg.experiment)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        if not writers:\n",
        "            for f in figs.values(): plt.close(f)\n",
        "            return\n",
        "\n",
        "        global_step = int(self.current_epoch)\n",
        "        for name, fig in figs.items():\n",
        "            for w in writers:\n",
        "                try:\n",
        "                    w.add_figure(f\"{name}/{split}\", fig, global_step=global_step, close=True)\n",
        "                except Exception:\n",
        "                    try:\n",
        "                        w.add_figure(f\"{name}/{split}\", fig, global_step=global_step)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "            plt.close(fig)\n",
        "\n",
        "# ------------------------ Sampler + DataLoaders ------------------------\n",
        "# Assumes train_ds/val_ds/test_ds (and val_loader/test_loader) are defined elsewhere.\n",
        "pos_idx, neg_idx = [], []\n",
        "for i in range(len(train_ds)):\n",
        "    _, y = train_ds[i]\n",
        "    (pos_idx if int(y)==1 else neg_idx).append(i)\n",
        "print(f\"Found {len(pos_idx)} positives / {len(pos_idx)+len(neg_idx)} total in train\")\n",
        "\n",
        "sampler = BalancedBatchSampler(pos_idx, neg_idx, batch_size=CONFIG[\"batch_size\"], pos_fraction=0.10)\n",
        "train_loader_bal = DataLoader(train_ds, batch_sampler=sampler, num_workers=CONFIG[\"num_workers\"])\n",
        "\n",
        "# ------------------------------ Training --------------------------------\n",
        "print(\"üöÄ Starting training...\")\n",
        "\n",
        "# CSV logger always; add TB in Colab for curves\n",
        "csv_logger = CSVLogger(save_dir=f\"{BASE_PATH}/lightning_logs\", name=\"kws\", version=None)\n",
        "logger = csv_logger\n",
        "if IN_COLAB:\n",
        "    tb_logger = TensorBoardLogger(save_dir=f\"{BASE_PATH}/lightning_logs\", name=\"tb\", version=None)\n",
        "    logger = [csv_logger, tb_logger]\n",
        "\n",
        "# TensorBoard in Colab (optional)\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        %load_ext tensorboard\n",
        "        %tensorboard --logdir {BASE_PATH}/lightning_logs\n",
        "        print(\"üìà TensorBoard launched! Check the output above.\")\n",
        "    except Exception:\n",
        "        print(\"üìä TensorBoard setup failed, but training completed successfully.\")\n",
        "\n",
        "model = KeywordDetectorPL(\n",
        "    in_channels=train_ds[0][0].shape[0],\n",
        "    opt=OptimConfig(\n",
        "        lr=1e-4,\n",
        "        weight_decay=1e-4,\n",
        "        max_time_shift=4,\n",
        "        noise_std=0.01\n",
        "    )\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    devices=\"auto\",\n",
        "    max_epochs=CONFIG[\"max_epochs\"],\n",
        "    callbacks=[EarlyStopping(monitor=\"val/loss\", mode=\"min\", patience=CONFIG[\"early_patience\"], verbose=True)],\n",
        "    logger=logger,\n",
        "    log_every_n_steps=50,\n",
        "    check_val_every_n_epoch=1\n",
        ")\n",
        "\n",
        "pl.seed_everything(42)\n",
        "\n",
        "print(f\"üìä Training on {len(train_ds)} samples with balanced sampling\")\n",
        "print(f\"‚úÖ Validation on {len(val_ds)} samples\")\n",
        "print(f\"üî¨ Testing on {len(test_ds)} samples\")\n",
        "\n",
        "trainer.fit(model, train_loader_bal, val_loader)\n",
        "\n",
        "print(\"\\nüéØ Final evaluation on test set:\")\n",
        "trainer.test(model, dataloaders=test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwuxEGZ351L"
      },
      "source": [
        "## VI. Evaluation\n",
        "\n",
        "Evaluating keyword detection models requires careful metric selection and interpretation. Traditional metrics can be misleading on imbalanced data‚Äîbelow are the ones that matter and how to interpret them for our keyword.\n",
        "\n",
        "### Why Standard Metrics Fail\n",
        "\n",
        "Accuracy is often misleading on imbalanced data (always predicting \"no\" will be correct over 99% of the time).\n",
        "\n",
        "F1, on the other hand, can be dominated by precision when positive prevalence is very low.\n",
        "\n",
        "### Metrics That Actually Matter\n",
        "For the Neural Keyword Spotting task, we standardize two key evaluation dimensions:\n",
        "\n",
        "**Threshold-free Metrics**\n",
        "AUPRC (area under precision‚Äìrecall):\n",
        "- Baseline equals positive class prevalence. For Watson, p‚âà0.001 (0.1%).\n",
        "- Aim for values clearly above 0.001; improvements of 2‚Äì10√ó over chance are meaningful.\n",
        "\n",
        "Precision‚ÄìRecall trade-off:\n",
        "- Precision: fraction of predicted keywords that are correct (controls false alarms)\n",
        "- Recall: fraction of true keywords detected\n",
        "\n",
        "AUROC (secondary):\n",
        "- Useful for architecture comparison, but optimistic under heavy imbalance.\n",
        "\n",
        "\n",
        "**User-facing Deployment Metrics**\n",
        "False alarms per hour (FA/h):\n",
        "- Target <10 FA/h. Compute as (FP / total_seconds) √ó 3600.\n",
        "\n",
        "Operating point selection:\n",
        "- Choose threshold on validation to meet FA/h or precision targets; report test results at that threshold.\n",
        "\n",
        "### Broad Performance Interpretation\n",
        "- Chance: Prevalence (% of words that are our keyword)\n",
        "- 2-5x Chance: modest improvement\n",
        "- 2-5x Chance: reasonable for this task\n",
        "- .>10x Chance: strong for this dataset\n",
        "\n",
        "Let's see how we stack up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AsS-ma8351L",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# --- Metrics & curves + structured logs & nicer visuals ---\n",
        "import json\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve, roc_curve, auc, confusion_matrix,\n",
        "    precision_score, recall_score, f1_score, accuracy_score\n",
        ")\n",
        "import numpy as np, torch\n",
        "\n",
        "# Run preds\n",
        "model.eval()\n",
        "device = next(model.parameters()).device\n",
        "all_probs, all_true = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        probs = torch.sigmoid(logits).detach().cpu().float().view(-1)\n",
        "        all_probs.append(probs)\n",
        "        all_true.append(yb.cpu().int().view(-1))\n",
        "\n",
        "y_prob = torch.cat(all_probs).numpy().astype(float)\n",
        "y_true = torch.cat(all_true).numpy().astype(int)\n",
        "\n",
        "\n",
        "# PR / ROC\n",
        "prec, rec, thr = precision_recall_curve(y_true, y_prob)\n",
        "pr_auc = auc(rec, prec)\n",
        "\n",
        "try:\n",
        "    fpr, tpr, roc_thr = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "except Exception:\n",
        "    fpr, tpr, roc_thr = None, None, None\n",
        "    roc_auc = float('nan')\n",
        "\n",
        "prevalence = float(y_true.mean())\n",
        "uplift = (pr_auc / prevalence) if prevalence > 0 else float('nan')\n",
        "\n",
        "# Operating point @ threshold 0.5\n",
        "tau = 0.5\n",
        "y_pred = (y_prob >= tau).astype(int)\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Rates @ 0.5\n",
        "prec05 = precision_score(y_true, y_pred, zero_division=0)\n",
        "rec05  = recall_score(y_true, y_pred, zero_division=0)\n",
        "f105   = f1_score(y_true, y_pred, zero_division=0)\n",
        "acc05  = accuracy_score(y_true, y_pred)\n",
        "spec05 = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
        "fpr05  = 1 - spec05\n",
        "tpr05  = rec05\n",
        "\n",
        "# --- Human-readable summary ---\n",
        "print(\"\\n=== Test Summary ===\")\n",
        "print(f\"Samples                      : {len(y_true):,} \"\n",
        "      f\"(pos={int(y_true.sum()):,}, neg={int((1-y_true).sum()):,})\")\n",
        "print(f\"Prevalence (chance AUPRC)    : {prevalence:.6f}\")\n",
        "print(f\"AUPRC                        : {pr_auc:.4f}  | uplift vs. chance: {uplift:.2f}√ó\")\n",
        "print(f\"AUROC                        : {roc_auc:.4f}\" if not np.isnan(roc_auc) else \"AUROC                        : n/a\")\n",
        "print(f\"Threshold œÑ                  : {tau:.2f}\")\n",
        "print(f\"Precision@œÑ                  : {prec05:.4f}\")\n",
        "print(f\"Recall@œÑ (TPR)               : {rec05:.4f}\")\n",
        "print(f\"Specificity@œÑ (TNR)          : {spec05:.4f}\")\n",
        "print(f\"FPR@œÑ                        : {fpr05:.4f}\")\n",
        "print(f\"F1@œÑ                         : {f105:.4f}\")\n",
        "print(f\"Accuracy@œÑ                   : {acc05:.4f}\")\n",
        "\n",
        "# --- Curves with operating point marked ---\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4.2))\n",
        "\n",
        "# PR\n",
        "ax[0].plot(rec, prec, lw=1.8, label=f'PR curve (AUC={pr_auc:.3f})')\n",
        "ax[0].axhline(y=prevalence, color='r', linestyle='--', alpha=0.7, label=f'Chance ({prevalence:.3f})')\n",
        "ax[0].scatter(rec05, prec05, s=40, marker='o', edgecolor='k', zorder=5,\n",
        "              label=f'@ œÑ={tau:.2f}  (P={prec05:.2f}, R={rec05:.2f})')\n",
        "ax[0].set_xlabel(\"Recall\")\n",
        "ax[0].set_ylabel(\"Precision\")\n",
        "ax[0].set_title(\"Precision‚ÄìRecall\")\n",
        "ax[0].legend()\n",
        "ax[0].grid(True, alpha=0.3)\n",
        "\n",
        "# ROC (if available)\n",
        "if fpr is not None:\n",
        "    ax[1].plot(fpr, tpr, lw=1.8, label=f'ROC curve (AUC={roc_auc:.3f})')\n",
        "    ax[1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Chance (0.5)')\n",
        "    ax[1].scatter(fpr05, tpr05, s=40, marker='o', edgecolor='k', zorder=5,\n",
        "                  label=f'@ œÑ={tau:.2f}  (FPR={fpr05:.2f}, TPR={tpr05:.2f})')\n",
        "    ax[1].set_xlabel(\"False Positive Rate\")\n",
        "    ax[1].set_ylabel(\"True Positive Rate\")\n",
        "    ax[1].set_title(\"ROC\")\n",
        "    ax[1].legend()\n",
        "    ax[1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Colorful + annotated confusion matrix (counts + row-normalized %) ---\n",
        "def plot_confusion_matrix_annotated(cm, class_names=(\"Negative\", \"Positive\"), cmap=\"viridis\"):\n",
        "    cm = cm.astype(np.float64)\n",
        "    # Row-normalized for percentages\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    cm_norm = np.divide(cm, row_sums, out=np.zeros_like(cm), where=row_sums != 0)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5.5, 4.6))\n",
        "    im = ax.imshow(cm_norm, interpolation='nearest', cmap=cmap)\n",
        "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.ax.set_ylabel('Row-normalized %', rotation=270, va='bottom')\n",
        "\n",
        "    ax.set_xticks([0, 1], labels=[f\"Pred {n}\" for n in class_names])\n",
        "    ax.set_yticks([0, 1], labels=[f\"True {n}\" for n in class_names])\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    ax.set_title(\"Confusion Matrix (counts + row %)\")\n",
        "    ax.grid(False)\n",
        "\n",
        "    # Annotate each cell with \"xx% (count)\"\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            pct = f\"{(cm_norm[i, j] * 100):.1f}%\"\n",
        "            cnt = f\"{int(cm[i, j]):,}\"\n",
        "            text = f\"{pct}\\n({cnt})\"\n",
        "            ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=11,\n",
        "                    color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n",
        "print(f\"True Negatives: {tn}, False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}, True Positives: {tp}\")\n",
        "\n",
        "plot_confusion_matrix_annotated(cm, class_names=(\"Negative\", \"Positive\"), cmap=\"viridis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_cpFrGR351L"
      },
      "source": [
        "Even at an AUPRC of 0.01 the model is, frankly, unusable in practice - the tiny uplift over chance in the PR curve‚Äôs top-left makes that obvious. Still, it‚Äôs **significantly above chance**, which matters: it signals there‚Äôs real information in the MEG that a better model/training regime could exploit.\n",
        "\n",
        "To make this more actionable, we complement AUPRC with **false alarms per hour (FA/h) at fixed recall**. On the next chart, you want your model to be on the top-left. We report FA/h at a few recall targets and compare against two baselines: a **random (permuted-scores)** chance model and the **always-negative** trivial model. This turns ‚Äúa small AUPRC‚Äù into an operational question: *at recall 0.2/0.4/0.6, how many false alarms per hour would you actually get‚Äîand is your performance better than nothing?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ3WJmkx351L"
      },
      "outputs": [],
      "source": [
        "# === False Alarms per Hour (FA/h) @ fixed recall ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "\n",
        "# y_true: 0/1 labels; y_prob: model probabilities for the same windows\n",
        "y_true_np = np.asarray(y_true).astype(int)\n",
        "y_prob_np = np.asarray(y_prob).astype(float)\n",
        "N = len(y_true_np)\n",
        "\n",
        "# -------------------- Infer evaluation coverage hours from CONFIG / dataset --------------------\n",
        "def infer_window_seconds(CONFIG, ds=None):\n",
        "    # 1) If dataset exposes exact window span, prefer that\n",
        "    for attr_pair in [\n",
        "        (\"window_seconds\", None),\n",
        "        (\"tmin\", \"tmax\"),            # many datasets expose tmin/tmax\n",
        "        (\"window_tmin\", \"window_tmax\"),\n",
        "    ]:\n",
        "        a, b = attr_pair\n",
        "        if ds is not None and hasattr(ds, a) and (b is None or hasattr(ds, b)):\n",
        "            if b is None:\n",
        "                return float(getattr(ds, a))\n",
        "            return float(getattr(ds, b) - getattr(ds, a))\n",
        "    # 2) Otherwise: derive from CONFIG buffers (keyword duration varies, but buffers dominate)\n",
        "    neg = float(CONFIG.get(\"negative_buffer\", 0.0))\n",
        "    pos = float(CONFIG.get(\"positive_buffer\", 0.0))\n",
        "    # If tmin/tmax explicitly set in CONFIG, use that\n",
        "    tmin = CONFIG.get(\"tmin\", None)\n",
        "    tmax = CONFIG.get(\"tmax\", None)\n",
        "    if tmin is not None and tmax is not None:\n",
        "        return float(tmax - tmin)\n",
        "    # Default: buffers only (conservative)\n",
        "    return neg + pos\n",
        "\n",
        "def infer_stride_seconds(CONFIG, ds=None, window_seconds=None):\n",
        "    # Prefer dataset-provided stride/hop attributes if present\n",
        "    for name in [\"stride_seconds\", \"hop_seconds\", \"hop_s\", \"step_seconds\"]:\n",
        "        if ds is not None and hasattr(ds, name):\n",
        "            return float(getattr(ds, name))\n",
        "    # If CONFIG ever includes stride_seconds, use it\n",
        "    if \"stride_seconds\" in CONFIG:\n",
        "        return float(CONFIG[\"stride_seconds\"])\n",
        "    # Otherwise assume 50% overlap (robust default for tutorials)\n",
        "    return (window_seconds or infer_window_seconds(CONFIG, ds)) / 2.0\n",
        "\n",
        "def infer_hours_total(CONFIG, ds=None, y_len=None):\n",
        "    # If dataset carries explicit coverage (best case), use it\n",
        "    for name in [\"coverage_seconds\", \"total_seconds\", \"eval_seconds\"]:\n",
        "        if ds is not None and hasattr(ds, name):\n",
        "            return float(getattr(ds, name)) / 3600.0, f\"{name} from dataset\"\n",
        "    # If dataset has an index with per-window [t0,t1], compute the union duration\n",
        "    idx = None\n",
        "    for name in [\"index_df\", \"windows_df\", \"index\"]:\n",
        "        if ds is not None and hasattr(ds, name):\n",
        "            idx = getattr(ds, name)\n",
        "            break\n",
        "    if idx is not None:\n",
        "        # Try the most likely column name pairs\n",
        "        for lcol, rcol in [(\"t0\", \"t1\"), (\"start_s\", \"end_s\"), (\"left_s\", \"right_s\")]:\n",
        "            if lcol in idx and rcol in idx:\n",
        "                intervals = np.array(idx[[lcol, rcol]], dtype=float)\n",
        "                # union of intervals\n",
        "                order = np.argsort(intervals[:,0])\n",
        "                intervals = intervals[order]\n",
        "                total = 0.0\n",
        "                cur_l, cur_r = intervals[0]\n",
        "                for l, r in intervals[1:]:\n",
        "                    if l <= cur_r:\n",
        "                        cur_r = max(cur_r, r)\n",
        "                    else:\n",
        "                        total += (cur_r - cur_l)\n",
        "                        cur_l, cur_r = l, r\n",
        "                total += (cur_r - cur_l)\n",
        "                return total / 3600.0, \"union of window intervals from dataset index\"\n",
        "    # Otherwise: estimate from window + stride implied by CONFIG\n",
        "    ws = infer_window_seconds(CONFIG, ds)\n",
        "    st = infer_stride_seconds(CONFIG, ds, ws)\n",
        "    seconds = max(0.0, (int(y_len or 0) - 1) * st + ws)\n",
        "    return seconds / 3600.0, \"estimated from CONFIG (window & 50% overlap)\"\n",
        "\n",
        "# Try to find a test dataset object if it's around\n",
        "_test_ds = None\n",
        "for cand in [\"test_ds\", \"test_dataset\", \"test_loader\"]:\n",
        "    if cand in globals():\n",
        "        obj = globals()[cand]\n",
        "        if hasattr(obj, \"dataset\"):   # DataLoader\n",
        "            _test_ds = obj.dataset\n",
        "            break\n",
        "        else:\n",
        "            _test_ds = obj\n",
        "            break\n",
        "\n",
        "hours_total, hours_source = infer_hours_total(CONFIG, ds=_test_ds, y_len=N)\n",
        "window_seconds = infer_window_seconds(CONFIG, ds=_test_ds)\n",
        "stride_seconds = infer_stride_seconds(CONFIG, ds=_test_ds, window_seconds=window_seconds)\n",
        "print(f\"[coverage] hours_total={hours_total:.3f}h  (source: {hours_source})  \"\n",
        "      f\"| window={window_seconds:.3f}s  stride‚âà{stride_seconds:.3f}s\")\n",
        "\n",
        "# -------------------- Core helpers --------------------\n",
        "def _metrics_at_threshold(y_true, y_prob, tau, hours_total):\n",
        "    y_pred = (y_prob >= tau).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    prec   = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    fah    = fp / max(hours_total, 1e-12)\n",
        "    return dict(threshold=float(tau), tp=int(tp), fp=int(fp), fn=int(fn), tn=int(tn),\n",
        "                recall=recall, precision=prec, fa_per_hour=fah)\n",
        "\n",
        "def _threshold_for_recall(y_true, y_prob, recall_target):\n",
        "    # Map recall target ‚Üí threshold via PR curve\n",
        "    prec, rec, thr = precision_recall_curve(y_true, y_prob)\n",
        "    if len(thr) == 0:\n",
        "        return 0.5\n",
        "    rec_seg = rec[1:]               # align to thr\n",
        "    rec_inc = rec_seg[::-1]\n",
        "    thr_inc = thr[::-1]\n",
        "    r = float(np.clip(recall_target, rec_inc.min(), rec_inc.max()))\n",
        "    return float(np.interp(r, rec_inc, thr_inc))\n",
        "\n",
        "def _fa_curve(y_true, y_prob, hours_total):\n",
        "    _, rec, thr = precision_recall_curve(y_true, y_prob)\n",
        "    if len(thr) == 0:\n",
        "        return np.array([0.0]), np.array([0.0]), np.array([0.5])\n",
        "    all_thr = np.unique(np.concatenate([[-np.inf], thr, [np.inf]]))\n",
        "    recalls, fahs = [], []\n",
        "    for t in all_thr:\n",
        "        m = _metrics_at_threshold(y_true, y_prob, t, hours_total)\n",
        "        recalls.append(m[\"recall\"])\n",
        "        fahs.append(m[\"fa_per_hour\"])\n",
        "    return np.array(recalls), np.array(fahs), all_thr\n",
        "\n",
        "# -------------------- Baselines --------------------\n",
        "rng = np.random.default_rng(0)\n",
        "y_prob_perm = rng.permutation(y_prob_np)  # random (same distribution; broken ordering)\n",
        "always_neg_point = dict(recall=0.0, fa_per_hour=0.0)\n",
        "\n",
        "# -------------------- FA/h at fixed recall targets --------------------\n",
        "recall_targets = [0.20, 0.40, 0.60]\n",
        "\n",
        "rows = []\n",
        "rows_baseline = []\n",
        "for r in recall_targets:\n",
        "    tau = _threshold_for_recall(y_true_np, y_prob_np, r)\n",
        "    rows.append({\"recall_target\": r, **_metrics_at_threshold(y_true_np, y_prob_np, tau, hours_total)})\n",
        "\n",
        "    tau_b = _threshold_for_recall(y_true_np, y_prob_perm, r)\n",
        "    rows_baseline.append({\"recall_target\": r, **_metrics_at_threshold(y_true_np, y_prob_perm, tau_b, hours_total)})\n",
        "\n",
        "def _row_fmt(r):\n",
        "    return (f\"r*={r['recall_target']:.2f} | œÑ={r['threshold']:.3f} | \"\n",
        "            f\"FA/h={r['fa_per_hour']:.2f} | P={r['precision']:.3f} | R={r['recall']:.3f}  \"\n",
        "            f\"| TP={r['tp']}, FP={r['fp']}, FN={r['fn']}, TN={r['tn']}\")\n",
        "\n",
        "print(\"\\n=== FA/h at fixed recall (model) ===\")\n",
        "for r in rows: print(_row_fmt(r))\n",
        "print(\"\\n--- Baseline: random (permuted scores) ---\")\n",
        "for r in rows_baseline: print(_row_fmt(r))\n",
        "print(\"\\n--- Trivial baseline: always negative ---\")\n",
        "print(\"r*=0.00 | œÑ=n/a  | FA/h=0.00 | P=0.000 | R=0.000  | TP=0, FP=0, FN=pos, TN=neg\")\n",
        "\n",
        "# -------------------- Plot: Recall vs FA/h (axes flipped) --------------------\n",
        "rec_m, fah_m, _ = _fa_curve(y_true_np, y_prob_np, hours_total)\n",
        "rec_b, fah_b, _ = _fa_curve(y_true_np, y_prob_perm, hours_total)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7.6, 4.4))\n",
        "ax.plot(fah_m, rec_m, label=\"Model\")\n",
        "ax.plot(fah_b, rec_b, label=\"Random baseline (permuted)\")\n",
        "ax.scatter(always_neg_point[\"fa_per_hour\"], always_neg_point[\"recall\"], marker=\"x\", s=60, label=\"Always negative\")\n",
        "\n",
        "for r in rows:\n",
        "    ax.scatter(r[\"fa_per_hour\"], r[\"recall\"], s=36)\n",
        "    ax.annotate(f\"r*={r['recall_target']:.2f}\\nœÑ={r['threshold']:.2f}\",\n",
        "                (r[\"fa_per_hour\"], r[\"recall\"]), textcoords=\"offset points\", xytext=(6,6))\n",
        "\n",
        "ax.set_xlabel(\"False alarms per hour (FA/h)\")\n",
        "ax.set_ylabel(\"Recall\")\n",
        "ax.set_title(\"Recall vs False Alarms per Hour\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMdrrYv9351M"
      },
      "source": [
        "## VII. Next Steps\n",
        "\n",
        "Congratulations! You've successfully built and trained a Neural Keyword Spotting model! This final section guides you through advanced experiments and research directions to push performance further.\n",
        "\n",
        "### üéØ Immediate Experiments\n",
        "\n",
        "Here are some things you can try out right now, in this Notebook:\n",
        "\n",
        "**Keyword Selection**\n",
        "```python\n",
        "# Try different keywords with varying properties:\n",
        "common_words = [\"the\", \"and\", \"of\", \"to\"]        # High frequency, low precision\n",
        "medium_words = [\"holmes\", \"watson\", \"sherlock\"]   # Medium frequency, higher precision  \n",
        "rare_words = [\"magnifying\", \"deduction\", \"pipe\"]  # Low frequency, potentially high precision\n",
        "```\n",
        "*Research question*: How does keyword frequency affect detection difficulty?\n",
        "\n",
        "**Sample Length Optimisation**\n",
        "```python\n",
        "# Experiment with different context windows:\n",
        "short_context = {\"negative_buffer\": 0.05, \"positive_buffer\": 0.15}  # 200ms total\n",
        "medium_context = {\"negative_buffer\": 0.10, \"positive_buffer\": 0.30} # 400ms total\n",
        "long_context = {\"negative_buffer\": 0.20, \"positive_buffer\": 0.50}   # 700ms total\n",
        "```\n",
        "*Research question*: What's the optimal temporal context for keyword detection?\n",
        "\n",
        "**Architecture Modifications**\n",
        "```python\n",
        "# Test different model configurations:\n",
        "configs = [\n",
        "    {\"model_dim\": 64, \"dropout\": 0.3},   # Smaller model\n",
        "    {\"model_dim\": 256, \"dropout\": 0.5},  # Larger model\n",
        "]\n",
        "```\n",
        "*Research question*: Which architectural choices matter most for performance?\n",
        "\n",
        "### üß† Further Research Directions\n",
        "\n",
        "If you prefer more comprehensive research directions, these might be interesting:\n",
        "\n",
        "**1. Multi-keyword Detection**\n",
        "- *Challenge*: Multi-label classification with extreme imbalance\n",
        "- *Benefit*: More practical for real BCI applications\n",
        "\n",
        "**2. Cross-session Generalisation**\n",
        "- *Challenge*: Neural patterns drift over time and sessions\n",
        "- *Metric*: Performance degradation over time\n",
        "\n",
        "**3. Real-time Implementation**\n",
        "- *Challenge*: Causal processing, latency constraints\n",
        "- *Metric*: Detection latency vs. accuracy trade-off\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "We hope you enjoyed the tutorial! If you have feedback or comments, please let us know under [REDACTED]. We believe that KWS has the potential to be the first practically useful application of non-invasive speech-BCIs. Even a 1-Bit channel might improve patients' quality of life drastically - so we wish you the best of luck in exploring it! üß†‚ú®\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
